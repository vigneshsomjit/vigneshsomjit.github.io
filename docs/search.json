[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "\nVignesh Somjit\n",
    "section": "",
    "text": "Vignesh Somjit\n\n\nI am a Research Professional at the University of Chicago Booth School of Business, where I work with Professor Richard Hornbeck. Before joining Booth, I received my B.A. in Economics (with honors) and B.A. in Mathematics from Boston University in 2025. I maintain a research blog, where I write about topics in math, econometrics, statistics, and machine learning."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "Vignesh Somjit",
    "section": "About Me",
    "text": "About Me\nI am a Research Professional at the University of Chicago Booth School of Business, where I work with Professor Richard Hornbeck. I received my B.A. in Economics and B.A. in Mathematics from Boston University in 2025."
  },
  {
    "objectID": "blog/blog-index.html",
    "href": "blog/blog-index.html",
    "title": "Vignesh Somjit",
    "section": "",
    "text": "Basic Concepts in Linear Algebra\n\n\n\n\n\n\nLinear Algebra\n\n\n\nDiscussion of fundamental concepts in linear algebra, including vector spaces, linear combination and independence, basis vectors, and subspaces.\n\n\n\n\n\nSep 20, 2025\n\n\n\n\n\n\n\nRandom Sampling and Model-Based Inference\n\n\n\n\n\n\nMathematical Statistics\n\n\n\nBig picture intuition for statistical inference under the random sampling framework.\n\n\n\n\n\nSep 20, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/regressions.html",
    "href": "blog/regressions.html",
    "title": "An Applied Econometrics Rationale for Linear Regression",
    "section": "",
    "text": "Regression Approximates Conditional Averages\nWe are interested in predicting the value of a random variable \\(Y_i\\), called the outcome, given some realized value of a random vector \\(X_i\\) of covariates. As a starting point, we let our predictions be a function of \\(X_i\\) and want our predictions to minimize the mean squared error (MSE) objective function\n\\[\n\\mathbb{E} [Y_i - f(X_i)]^2.\n\\] With the law of iterated expectation and some calculus, we can show that for every possible realized value \\(X_i = x\\), the optimal function that solves the minimum mean squared error problem is the conditional expectation function (CEF)\n\\[\n\\mu(x) = \\mathbb{E}[Y_i | X_i = x].\n\\]\nIn most empirical cases, however, the CEF of interest has no analytic form. Thus, it is more practical to model the relationship between the outcome and covariates using a simpler function. One option is to find a linear approximation of the CEF, which takes the form\n\\[\n\\ell(x) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_k x_k = x^T\\beta.\n\\]\nNote that the word “linear” here refers to the simplifying assumption that the function is a linear combination of the covariates. Since we want our predictions to minimize the MSE function, the best linear predictor is found by setting the coefficients as\n\\[\n\\beta^\\star = \\underset{\\beta}{\\arg\\min} \\, \\mathbb{E} [Y_i - (X_i^T \\beta)]^2 = \\mathbb{E}[X_i X_i^T]^{-1}\\mathbb{E}[X_iY_i].\n\\] Thus, the linear regression model that describes the relationship between \\(Y_i\\) and \\(X_i\\) is given by \\[\nY_i = \\underbrace{X_i^T \\beta^\\star}_{\\text{best linear predictor}} + \\underbrace{\\epsilon_i}_{\\text{residual}},\n\\] where \\(\\epsilon_i\\) captures the difference between the realized outcome and the best linear predictor.\nTo summarize, the conditional expectation function is the optimal predictor of an outcome given covariates, in the sense that it minimizes the mean squared prediction error. Since the CEF is typically a complex function, linear regression approximates it by selecting the linear combination of covariates that minimizes the mean squared error. The resulting linear predictor provides a simple model of the relationship between the outcome and the covariates.\n\n\nApproximate and Exact Conditional Averages\n\nset.seed(123)\n\n# Sample size \nn &lt;- 1000 \n\n# Covariates\nsex &lt;- rbinom(n, 1, 0.5)\nrace &lt;- rbinom(n, 1, 0.3)                   \n\n# Non-linear data generating process; no interactions\noutcome1 &lt;- 5 + 2^sex + 3^race \n\n# Non-linear data generating process; interactions \noutcome2 &lt;- 5 + 2^sex + 3^race + 1.5^(sex*race)\n\n# Consolidate in a dataset \ndf &lt;- data.frame(\n  outcome1 = outcome1,\n  outcome2 = outcome2,\n  sex = sex,\n  race = race\n)"
  },
  {
    "objectID": "blog/random-sampling.html",
    "href": "blog/random-sampling.html",
    "title": "Random Sampling and Model-Based Inference",
    "section": "",
    "text": "Observational data is inherently random: if we were to measure the same variables repeatedly, we would almost certainly get different values each time. A classical explanation for the source of this randomness is the model-based (or sampling-based) perspective. In this abstraction, we assume there exists an underlying superpopulation or data generating process (DGP): a fixed but unknown probability distribution \\(F\\) that can, in principle, generate infinitely many observations. Randomness then arises because we only observe finitely many realizations from \\(F\\) — never the entire distribution. Within this framework, we define statistical inference as the process of using the observed random data to estimate features of \\(F\\) and quantify the uncertainty in those estimates.\nNotice that the exposition above implicitly assumes that there is a single underlying distribution \\(F\\) from which all observed data are drawn. To make this mathematically precise, we need to introduce the random sampling assumption. This provides the framework to connect the observed data to the DGP, and thereby lays the foundation for statistical inference."
  },
  {
    "objectID": "blog/random-sampling.html#mathematical-formalization-of-observational-datasets",
    "href": "blog/random-sampling.html#mathematical-formalization-of-observational-datasets",
    "title": "Random Sampling Framework",
    "section": "",
    "text": "Cross-sectional datasets consist of several observations of a collection of variables for a given point in time and can be denoted as\n\\[\n\\{x_{1i}, x_{2i}, \\ldots, x_{Ki}\\}_{i=1}^n,\n\\]\nwhere \\(x_{ki}\\) is the value of the \\(k\\)-th variable for the \\(i\\)-th observation. A typical source of cross-sectional data in economics is through surveys like the Current Population Survey (CPS) or the American Community Survey (ACS).\nWe are interested in making statements about the underlying process that generates the dataset. In statistics, we do this by mathematically formalizing the way the data was generated. Specifically, we view each observational vector \\[\n\\boldsymbol{x}_i = (x_{1i}, \\ldots, x_{Ki})' \\in \\mathbb{R}^k \\quad \\text{for } i = 1, \\ldots, n,\n\\] as a realization of the random vector \\[\n\\boldsymbol X_i = (X_{1i}, \\ldots, X_{K_i})' \\in \\mathbb{R}^k \\quad \\text{for } i = 1, \\ldots, n,\n\\]\nwith some associated probability distribution. For example, we can think of the CPS dataset as consisting of random vectors\n\\[\n\\boldsymbol X_i = (sex_i, age_i, educ_i, wage_i)' \\quad \\text{for } i = 1, \\ldots, n.\n\\] For some specific individual \\(i\\) in the CPS dataset, we then observe the vector of realized values1\n\\[\n\\boldsymbol x_i = (1, 25, 16, 25000)'.\n\\] Intuitively, the distinction between the random vector \\(\\boldsymbol X_i\\) and the realization \\(\\boldsymbol x_i\\) is that the former represents the \\(i\\)-th observation before viewing the data (unknown and random) and the latter represents the \\(i\\)-th observation after viewing the data (specific known value)."
  },
  {
    "objectID": "blog/random-sampling.html#random-sampling-assumption",
    "href": "blog/random-sampling.html#random-sampling-assumption",
    "title": "Random Sampling Framework",
    "section": "Random Sampling Assumption",
    "text": "Random Sampling Assumption\nThe assumption of random sampling is a characterization of the probability distribution of the random vectors \\(\\boldsymbol X_1, \\ldots, \\boldsymbol X_n\\).\n\nRandom Sampling Assumption. The random vectors \\(\\boldsymbol X_1, \\ldots , \\boldsymbol X_n\\) are mutually independent and have the same probability distribution \\(F(X_1, \\ldots, X_K)\\). Equivalently, we say that the random vectors \\(\\boldsymbol X_1, \\ldots , \\boldsymbol X_n\\) are independent and identically distributed (iid) with distribution \\(F\\).\n\nThe random sampling framework is one potential way to characterize the dependence structure across the observed data points. It is popular because (i) it is often reasonable when working with cross-sectional datasets2, and (ii) it is the backbone of several statistical theorems and methods.3\n\nAlternative Sampling Assumptions"
  },
  {
    "objectID": "blog/random-sampling.html#footnotes",
    "href": "blog/random-sampling.html#footnotes",
    "title": "Random Sampling and Model-Based Inference",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nHere, the value of \\(1\\) refers to the individual being a male and \\(16\\) means \\(16\\) years of education.↩︎\nFor example, if we collected data on a random subset of individuals from a large common population (e.g. the USA), it is reasonable to assume that the characteristics of one individual are independent of another individual and that they all come from the same population distribution.↩︎\nCrucial theorems in asymptotic statistical theory, like the Law of Large Numbers and the Central Limit Theorem, require the random sampling assumption to hold.↩︎\nTechnically, under iid sampling, the empirical distribution converges to the true distribution as the sample size tends to infinity — a result known as the Glivenko–Cantelli theorem. Nevertheless, in any finite sample, uncertainty remains, and with it the need for statistical inference.↩︎\nTo quote my Mathematical Statistics Professor Daniel Weiner: “Do to the sample to get your estimator, as you would do to your population to get your estimand.”↩︎\nTo be more precise, \\(\\hat\\theta\\) is unbiased for \\(\\theta\\) if \\(\\mathbb{E}[\\hat\\theta]=\\theta\\) for all \\(F \\in \\mathcal{F}\\), where \\(\\mathcal{F}\\) is a class of distributions.↩︎"
  },
  {
    "objectID": "blog/random-sampling.html#parameters-and-statistics",
    "href": "blog/random-sampling.html#parameters-and-statistics",
    "title": "Random Sampling Framework",
    "section": "Parameters and Statistics",
    "text": "Parameters and Statistics"
  },
  {
    "objectID": "blog/random-sampling.html#sampling-distribution",
    "href": "blog/random-sampling.html#sampling-distribution",
    "title": "Random Sampling Framework",
    "section": "Sampling Distribution",
    "text": "Sampling Distribution"
  },
  {
    "objectID": "blog/random-sampling.html#estimands-estimators-and-sampling-distribution",
    "href": "blog/random-sampling.html#estimands-estimators-and-sampling-distribution",
    "title": "Random Sampling Framework",
    "section": "Estimands, Estimators, and Sampling Distribution",
    "text": "Estimands, Estimators, and Sampling Distribution\nAn estimand \\(\\theta\\) is a function of the data generating process \\(F\\). For example, the population mean of the \\(k\\)-th variable is an estimand defined as\n\\[\n\\mu_k = \\mathbb{E}_F[ X_k].\n\\] However, we do not know the data generating process \\(F\\) and therefore cannot calculate any estimand. Instead, we make certain simplifying assumptions about the general structure of \\(F\\) and then use the observed data to guess the value of the estimand. More formally, an estimator is a function that maps the sample to an estimand under the assumptions we make about \\(F\\). For example, the sample mean of the \\(k\\)-th variable is an estimator for the population mean:\n\\[\n\\hat{\\mu}_k = \\frac{1}{n} \\sum_{i=1}^n x_{ki}.\n\\]"
  },
  {
    "objectID": "blog/random-sampling.html#estimands-and-estimators",
    "href": "blog/random-sampling.html#estimands-and-estimators",
    "title": "Model-Based Inference Under Random Sampling",
    "section": "Estimands and Estimators",
    "text": "Estimands and Estimators\nAn estimand \\(\\theta\\) is a function of the data generating process \\(F\\). For example, the the population mean of a random variable \\(X\\) \\[\n\\mu = \\mathbb{E}_F[X]\n\\] is an estimand. However, we do not know the data generating process \\(F\\) and therefore cannot calculate any estimand. Instead, we use the observed data to guess the value of the estimand. An estimator \\(\\hat \\theta\\) is a function of the sample that provides a “good guess” for \\(\\theta\\).\nIn statistics, there are several estimation methods that provide systematic ways (i.e. rules) to construct estimators. One common method is the analog principle (or plug-in principle). The idea is to construct the estimator by replacing the population quantities in the estimand with their sample analogs.4 Thus, the sample mean is the analog estimator for the population mean:\n\\[\n\\hat{\\mu} = \\frac{1}{n} \\sum_{i=1}^n X_i.\n\\]\n\nSampling Distribution\nHow do we define a good guess? How do we choose between two different estimators?\nwe make certain simplifying assumptions about the general structure of \\(F\\) and then\nWe will be able to quantify the uncertainty of the guess if we make stronger starting assumptions about the data generating process."
  },
  {
    "objectID": "blog/random-sampling.html#random-sampling",
    "href": "blog/random-sampling.html#random-sampling",
    "title": "Random Sampling Framework",
    "section": "Random Sampling",
    "text": "Random Sampling\nThe mathematical formalization of the dataset now allows us to make a connection between the observed data and the underlying process that generates it. Specifically, we establish this connection by characterizing the probability distribution of each random vector \\(\\boldsymbol X_1, \\ldots, \\boldsymbol X_n\\).\nThe simplest characterization is the random sampling framework. Here, we assume that the random vectors \\(\\boldsymbol X_1, \\ldots , \\boldsymbol X_n\\) are independent and identically distributed (iid) with some common but unknown distribution \\(F(X_1, \\ldots, X_K)\\). Notice that \\(F\\) is exactly the underlying process we want to learn about, and is thus aptly called the data generating process (DGP). We also sometimes refer to \\(F\\) as the population. This captures the idea that the dataset is a random subset of some infinitely large population.\nIt’s useful to explicitly clarify the notation being used above. The random variables \\(X_1, \\ldots, X_K\\) are mathematical objects used to represent the generic or population-level variable associated with the DGP. For example, the random variable \\(wage\\) represents the generic variable for wage in the population. The random vector \\(\\boldsymbol X_i = (X_{i1}, \\ldots, X_{iK})\\) are mathematical objects used to represent the sample-level variable associated with specific units. For example, the random variable \\(wage_i\\) represents the wage of individual \\(i\\) before the data is observed. Lastly, the realizations \\(\\boldsymbol{x}_i = (x_{i1}, \\ldots, x_{iK})\\) are the actual observed values of the sample-level random variables. For example, the value \\(wage_i = 25000\\) is the observed wage of individual \\(i\\) after the data is observed.\n\nAlternative Sampling Assumptions\nThe random sampling assumption is one potential way to characterize the dependence structure across the observed data points. It is popular because (i) it is often reasonable when working with cross-sectional datasets2, and (ii) it is the backbone of several statistical theorems and methods.3"
  },
  {
    "objectID": "blog/random-sampling.html#random-samples",
    "href": "blog/random-sampling.html#random-samples",
    "title": "Random Sampling Framework",
    "section": "Random Samples",
    "text": "Random Samples\nThe random sampling assumption is one way to characterize the probability distribution of the random vectors \\(\\boldsymbol X_1, \\ldots, \\boldsymbol X_n\\). The definition below helps formalize the idea.\n\nDefinition. The random vectors \\(\\{\\boldsymbol X_1, \\ldots , \\boldsymbol X_n\\}\\) are called a random sample from the distribution \\(F\\) if they are mutually independent and have the same probability distribution \\(F\\). Equivalently, we say that the random vectors \\(\\{\\boldsymbol X_1, \\ldots , \\boldsymbol X_n\\}\\) are independent and identically distributed (iid) with distribution \\(F\\).\n\nThe random sampling framework asserts that our observed dataset is a random sample from some common, but unknown distribution \\(F\\). The distribution \\(F\\) is often called the data generating process or the population.2\n\nAlternative Sampling Assumptions\nThe random sampling assumption is one potential way to characterize the dependence structure across the observed data points. It is popular because (i) it is often reasonable when working with cross-sectional datasets[^4], and (ii) it is the backbone of several statistical theorems and methods.3"
  },
  {
    "objectID": "blog/random-sampling.html#random-sampling-framework",
    "href": "blog/random-sampling.html#random-sampling-framework",
    "title": "Random Sampling and Model-Based Inference",
    "section": "Random Sampling Framework",
    "text": "Random Sampling Framework\nThe discussion going forward will primarily focus on cross-sectional datasets. These consist of several observations of a collection of variables for a given point in time and can be denoted as\n\\[\n\\{x_{i1}, x_{i2} \\ldots, x_{iK}\\}_{i=1}^n,\n\\]\nwhere \\(x_{ik}\\) is the value of the \\(k\\)-th variable for the \\(i\\)-th unit. A typical source of cross-sectional data in economics is through surveys like the Current Population Survey (CPS) or the American Community Survey (ACS).\n\nMathematical Formalization of the Dataset\nSince observational data is random, it is natural to we view the observed data vector \\[\n\\boldsymbol{x}_i = (x_{i1}, \\ldots, x_{iK})' \\in \\mathbb{R}^K \\quad \\text{for } i = 1, \\ldots, n,\n\\] as a realization of the random vector \\[\n\\boldsymbol X_i = (X_{i1}, \\ldots, X_{iK})' \\in \\mathbb{R}^K \\quad \\text{for } i = 1, \\ldots, n,\n\\]\nwith some associated probability distribution. For example, we can think of the CPS dataset as consisting of random vectors\n\\[\n\\boldsymbol X_i = (sex_i, age_i, educ_i, wage_i)' \\quad \\text{for } i = 1, \\ldots, n.\n\\] For some specific individual \\(i\\) in the CPS dataset, we then observe the vector of realized values1\n\\[\n\\boldsymbol x_i = (1, 25, 16, 25000)'.\n\\] Intuitively, the distinction between the random vector \\(\\boldsymbol X_i\\) and the realization \\(\\boldsymbol x_i\\) is that the former represents the \\(i\\)-th observation before viewing the data (unknown and random) and the latter represents the \\(i\\)-th observation after viewing the data (specific known value).\n\n\nRandom Sampling\nWe have now represented the dataset as a collection of random vectors \\(\\boldsymbol X_1, \\ldots, \\boldsymbol X_n\\), each with some probability distribution \\(F_1 \\ldots, F_n\\). However, the connection between the underlying DGP and the observed data remains unclear. Specifically, we need a simplifying assumption that will allow us to connect the distributions of each \\(\\boldsymbol X_i\\) to the underlying DGP in a straightforward manner. The simplest framework is that of random sampling. Here, we assume that the random vectors \\(\\boldsymbol X_1, \\ldots , \\boldsymbol X_n\\) are independent and identically distributed (iid) with some common but unknown distribution \\(F\\) on \\(\\mathbb{R}^K\\). Thus, under this assumption, the data generating process is precisely the distribution that governs each individual random vector \\(\\boldsymbol X_i\\).\n\nAlternative Assumptions\nThe random sampling assumption is one potential way to characterize the dependence structure across the observed data points. It is popular because (i) it is often reasonable when working with cross-sectional datasets2, and (ii) it is the backbone of several statistical theorems and methods.3 However, it does not necessarily have to hold. For example, we often work with data where the units are connected via some underlying factor (location, industry, etc.). In such cases, the assumption of independence across individual units is violated. An alternative approach in such cases is to instead assume mutual independence across clusters of units. Another example of a violation to the independence assumption is time-series data, where the individual unit is indexed by time. Here, consecutive observations are usually correlated and independence is instead formulated in terms of stationarity and other concepts outside the scope of this post.\n\n\n\nNotation and Example\nThe discussion so far has purely been conceptual, and so it is useful to consider a concrete example. Before doing so, however, let’s clarify some notation used in the random sampling framework. We denote the population-level random variables as \\(X_1, \\ldots, X_K\\). The data generating process \\(F\\) is the joint distribution of these random variables. For example, \\(X_1\\) could denote the generic random variable for wage in the entire US population and we are interested in making inferences about its distribution. We denote the sample-level random variables as \\(X_{i1}, \\ldots, X_{iK}\\). These represent the random variables associated with specific units in the sample. Continuing the example, \\(X_{i1}\\) denotes the random variable representing the wage of individual \\(i\\) in the sample. Finally, we denote the realized observations as \\(X_{i1}=x_{i1}, \\ldots, X_{iK}=x_{iK}\\). These are the actual values we observe. So, \\(X_{i1} = 25,000\\) means that the observed wage of individual \\(i\\) is \\(25,000\\).\nNow suppose our data generating process consists of one random variable with an exponential distribution with scale parameter \\(\\beta = 1\\): \\[\nX \\sim \\text{Exp}(1) \\quad \\text{with density } f(x) = e^{-x} \\text{ for } x \\geq 0.\n\\]\nIf we assume our dataset is a random sample of size 30 from the above DGP, then \\[\nX_{i} \\sim \\text{Exp}(1) \\, \\, \\text{ for } i = 1, \\ldots, 30 \\quad  \\text{and} \\quad  X_i \\perp X_j  \\, \\, \\text{ for } i \\neq j.\n\\]\nThe figure below plots the empirical kernel density of such a random sample (black line) and the true density of the exponential distribution (red line). In practice, we do not know the true DGP and need to guess its characteristics using the observed data. Randomness in the finite observed data makes this a non-trivial task — as illustrated by the discrepancy between the empirical and true densities in the figure below.4\n\n\nCode\n# Simulate IID sample of 30 obs from exp(1)\nset.seed(123)\nn &lt;- 30\nx &lt;- rexp(n, rate = 1)\n\n# Empirical Density \ndens &lt;- density(x)\n\n# True Exponential Density\nxs &lt;- seq(0, max(x), length.out = 200)\nys &lt;- dexp(xs, rate = 1)\n\n# Plot\nplot(dens, main = \"\", xlab = \"Observed Data\", ylab = \"Density\", xlim = c(0, 5), ylim = c(0, max(c(dens$y, ys))))\ncurve(dexp(x, rate = 1), from = min(x), add = TRUE, col = \"red\", lwd = 2)"
  },
  {
    "objectID": "blog/random-sampling.html#mathematical-formalization-of-observational-data",
    "href": "blog/random-sampling.html#mathematical-formalization-of-observational-data",
    "title": "Random Sampling Framework",
    "section": "Mathematical Formalization of Observational Data",
    "text": "Mathematical Formalization of Observational Data\nTo keep things concrete, the rest of this post will focus on cross-sectional datasets. These consist of several observations of a collection of variables for a given point in time and can be denoted as\n\\[\n\\{x_{i1}, x_{i2} \\ldots, x_{iK}\\}_{i=1}^n,\n\\]\nwhere \\(x_{ik}\\) is the value of the \\(k\\)-th variable for the \\(i\\)-th unit. A typical source of cross-sectional data in economics is through surveys like the Current Population Survey (CPS) or the American Community Survey (ACS).\nLet us now mathematically formalize the connection between the observed dataset and the underlying DGP \\(F\\). Specifically, we view each observational vector \\[\n\\boldsymbol{x}_i = (x_{i1}, \\ldots, x_{iK})' \\in \\mathbb{R}^K \\quad \\text{for } i = 1, \\ldots, n,\n\\] as a realization of the random vector \\[\n\\boldsymbol X_i = (X_{i1}, \\ldots, X_{iK})' \\in \\mathbb{R}^K \\quad \\text{for } i = 1, \\ldots, n,\n\\]\nwith some associated probability distribution. For example, we can think of the CPS dataset as consisting of random vectors\n\\[\n\\boldsymbol X_i = (sex_i, age_i, educ_i, wage_i)' \\quad \\text{for } i = 1, \\ldots, n.\n\\] For some specific individual \\(i\\) in the CPS dataset, we then observe the vector of realized values1\n\\[\n\\boldsymbol x_i = (1, 25, 16, 25000)'.\n\\] Intuitively, the distinction between the random vector \\(\\boldsymbol X_i\\) and the realization \\(\\boldsymbol x_i\\) is that the former represents the \\(i\\)-th observation before viewing the data (unknown and random) and the latter represents the \\(i\\)-th observation after viewing the data (specific known value)."
  },
  {
    "objectID": "blog/random-sampling.html#radnom-sampling",
    "href": "blog/random-sampling.html#radnom-sampling",
    "title": "Random Sampling Framework",
    "section": "Radnom Sampling",
    "text": "Radnom Sampling\nThe random sampling assumption is one way to characterize the probability distribution of the random vectors \\(\\boldsymbol X_1, \\ldots, \\boldsymbol X_n\\). Specifically, it asserts that the random vectors \\(\\boldsymbol X_1, \\ldots , \\boldsymbol X_n\\) are independent and identically distributed (iid) with some common but unknown distribution \\(F(X_1, \\ldots, X_K)\\). The distribution \\(F\\) is often called the data generating process (DGP) because if we knew the distribution, then we could reproduce the dataset by drawing \\(n\\) independent samples from \\(F\\). We also refer to \\(F\\) as the population. This captures the intuition that the dataset is a random subset of some infinitely large population.\nIt’s easy to get confused about the notation and terminology here, so let’s clarify.\nIn this setting, making statements about the underlying process that generates the dataset is now equivalent to learning about the distribution \\(F\\) using the sample. This idea is the basis of statistical inference and is briefly explored in the next section.\n\nAlternative Sampling Assumptions\nThe random sampling assumption is one potential way to characterize the dependence structure across the observed data points. It is popular because (i) it is often reasonable when working with cross-sectional datasets[^4], and (ii) it is the backbone of several statistical theorems and methods.2"
  },
  {
    "objectID": "blog/random-sampling.html#what-is-statistical-inference",
    "href": "blog/random-sampling.html#what-is-statistical-inference",
    "title": "Random Sampling and Model-Based Inference",
    "section": "",
    "text": "Observational data is inherently random: if we were to measure the same variables repeatedly, we would almost certainly get different values each time. A classical explanation for the source of this randomness is the model-based (or sampling-based) perspective. In this abstraction, we assume there exists an underlying superpopulation or data generating process (DGP): a fixed but unknown probability distribution \\(F\\) that can, in principle, generate infinitely many observations. Randomness then arises because we only observe finitely many realizations from \\(F\\) — never the entire distribution. Within this framework, we define statistical inference as the process of using the observed random data to estimate features of \\(F\\) and quantify the uncertainty in those estimates.\nNotice that the exposition above implicitly assumes that there is a single underlying distribution \\(F\\) from which all observed data are drawn. To make this mathematically precise, we need to introduce the random sampling assumption. This provides the framework to connect the observed data to the DGP, and thereby lays the foundation for statistical inference."
  },
  {
    "objectID": "blog/random-sampling.html#random-sampling-framework-for-cross-sectional-data",
    "href": "blog/random-sampling.html#random-sampling-framework-for-cross-sectional-data",
    "title": "Random Sampling and Model-Based Inference",
    "section": "Random Sampling Framework for Cross-Sectional Data",
    "text": "Random Sampling Framework for Cross-Sectional Data\n\nMathematical Formalization of the Dataset\nSince observational data is random, it is natural to we view the observed data vectors \\[\n\\boldsymbol{x}_i = (x_{i1}, \\ldots, x_{iK})' \\in \\mathbb{R}^K \\quad \\text{for } i = 1, \\ldots, n,\n\\] as a realization of the random vector \\[\n\\boldsymbol X_i = (X_{i1}, \\ldots, X_{iK})' \\in \\mathbb{R}^K \\quad \\text{for } i = 1, \\ldots, n,\n\\]\nwith some associated probability distribution. For example, we can think of the CPS dataset as consisting of random vectors\n\\[\n\\boldsymbol X_i = (sex_i, age_i, educ_i, wage_i)' \\quad \\text{for } i = 1, \\ldots, n.\n\\] For some specific individual \\(i\\) in the CPS dataset, we then observe the vector of realized values1\n\\[\n\\boldsymbol x_i = (1, 25, 16, 25000)'.\n\\] Intuitively, the distinction between the random vector \\(\\boldsymbol X_i\\) and the realization \\(\\boldsymbol x_i\\) is that the former represents the \\(i\\)-th observation before viewing the data (unknown and random) and the latter represents the \\(i\\)-th observation after viewing the data (specific known value).\n\n\nRandom Sampling\nWe have now represented the dataset as a collection of random vectors \\(\\boldsymbol X_1, \\ldots, \\boldsymbol X_n\\), each with some probability distribution \\(F_1 \\ldots, F_n\\). As it stands right now, the connection between the underlying DGP and the observed data is still unclear. Specifically, we need a simplifying assumption that will allow us to connect the distributions of each \\(\\boldsymbol X_i\\) to the underlying DGP in a straightforward manner.\nThe simplest framework is that of random sampling. Here, we assume that the random vectors \\(\\boldsymbol X_1, \\ldots , \\boldsymbol X_n\\) are independent and identically distributed (iid) with some common but unknown distribution \\(F\\) on \\(\\mathbb{R}^K\\). Under this assumption, the data generating process \\(F\\) is precisely the distribution that governs each individual random vector \\(\\boldsymbol X_i\\).\n\n\nAlternative Sampling Assumptions\nThe random sampling assumption is one potential way to characterize the dependence structure across the observed data points. It is popular because (i) it is often reasonable when working with cross-sectional datasets2, and (ii) it is the backbone of several statistical theorems and methods.3"
  },
  {
    "objectID": "blog/random-sampling.html#notation",
    "href": "blog/random-sampling.html#notation",
    "title": "Random Sampling and Model-Based Inference",
    "section": "Notation",
    "text": "Notation\nIt’s useful to explicitly clarify the notation being used above. The random variables \\(X_1, \\ldots, X_K\\) are mathematical objects used to represent the generic or population-level variable associated with the DGP. For example, the random variable \\(wage\\) represents the generic variable for wage in the population. The random vector \\(\\boldsymbol X_i = (X_{i1}, \\ldots, X_{iK})\\) are mathematical objects used to represent the sample-level variable associated with specific units. For example, the random variable \\(wage_i\\) represents the wage of individual \\(i\\) before the data is observed. Lastly, the realizations \\(\\boldsymbol{x}_i = (x_{i1}, \\ldots, x_{iK})\\) are the actual observed values of the sample-level random variables. For example, the value \\(wage_i = 25000\\) is the observed wage of individual \\(i\\) after the data is observed."
  },
  {
    "objectID": "blog/random-sampling.html#point-estimation",
    "href": "blog/random-sampling.html#point-estimation",
    "title": "Random Sampling and Model-Based Inference",
    "section": "Point Estimation",
    "text": "Point Estimation\nPoint estimation is the first step of statistical inference, and involves constructing a “good guess” for a feature of the unknown data generating process. To be more precise, this feature is called an estimand \\(\\theta\\) and is defined as a function of the data generating process \\(F\\):\n\\[\n\\theta = \\theta(F).\n\\]\nAn example of an estimand is the the population mean of a random variable \\(X\\): \\[\n\\mu = \\mathbb{E}_F[X].\n\\]\nSince the DGP is unknown, the best we can do is use the observed data to guess the value of the estimand. An estimator \\(\\hat \\theta\\) is a function of the sample that is intended to provide a guess of the estimand:\n\\[\n\\hat{\\theta} = \\hat{\\theta}(\\boldsymbol X_1, \\ldots, \\boldsymbol X_n).\n\\]\nWhen the estimator is evaluated at a specific realization of the sample, we obtain an estimate \\(\\hat\\theta(\\boldsymbol x_1, \\ldots, \\boldsymbol x_n)\\) of the estimand. It’s worth emphasizing that the estimand is a fixed but unknown number, the estimator is a random variable, and the estimate is a fixed and known number.\nIn statistics, there are several estimation principles that provide systematic ways (i.e. rules) to construct estimators. One common method is the analog principle (or plug-in principle). The idea is to construct the estimator by replacing the population quantities in the estimand with their sample analogs.5 Thus, the analog estimator for the population mean is the sample mean, defined as\n\\[\n\\hat{\\mu} = \\frac{1}{n} \\sum_{i=1}^n \\boldsymbol X_i.\n\\]\n\nEstimator Properties\nHow do we know if an estimator is any good? To answer this question, statisticians study desirable properties that an estimator should ideally satisfy. A full treatment of estimator properties is typically the focus of a mathematical statistics course, but it is still valuable to briefly highlight some fundamental properties here.\nThe error of an estimator is defined as the difference between the estimate and the estimand:\n\\[\ne(\\boldsymbol x_1, \\ldots, \\boldsymbol x_n) = \\hat\\theta(\\boldsymbol x_1, \\ldots, \\boldsymbol x_n) - \\theta.\n\\] The bias of an estimator is the average error of the estimator across all possible samples of size \\(n\\) from the DGP:\n\\[\nB(\\hat\\theta) = \\mathbb{E}_F[\\hat{\\theta}] - \\theta\n\\] Intuitively, the bias captures the systematic error of the estimator: if the bias is positive, the estimator tends to overestimate the estimand, and if the bias is negative, the estimator tends to underestimate the estimand. We say an estimator \\(\\hat{\\theta}\\) is unbiased for \\(\\theta\\) if its bias is zero:6\n\\[\n\\mathbb{E}_F[\\hat{\\theta}] - \\theta = 0.\n\\] Thus, the errors of an unbiased estimator are purely due to randomness in the data. As it turns out, the sample mean is an unbiased estimator of the population mean under the random sampling assumption:\n\\[\n\\mathbb{E}_F[\\hat{\\mu}] = \\frac{1}{n} \\sum_{i=1}^n \\mathbb{E}_F[\\boldsymbol X_i] = \\frac{1}{n} \\sum_{i=1}^n \\mu = \\mu.\n\\] The first equality follows from the linearity of expectations, the second equality follows from the random sampling assumption, and the third equality is a simplification.\nWhile bias quantifies how far the estimator’s average is from the estimand, the variance (or sampling variance) measures how much the estimator varies across repeated samples of size \\(n\\):\n\\[\nVar(\\hat\\theta) = \\mathbb{E}_F[(\\hat\\theta - \\mathbb{E}_F[\\hat\\theta])^2].\n\\] The variance of the sample mean under the random sampling assumption is given by\n\\[\n\\operatorname {Var} \\left[\\hat\\mu\\right] = \\frac{1}{n^2}\\operatorname{Var} \\left[ \\sum_{i=1}^n \\boldsymbol X_i\\right] = \\frac{1}{n^2} \\sum_{i=1}^n \\operatorname{Var}[\\boldsymbol X_i] = \\frac{1}{n^2} \\sum_{i=1}^n \\sigma^2 = \\frac{\\sigma^2}{n},\n\\] where \\(\\sigma^2\\) is the population variance \\(\\operatorname{Var}[X]\\). The first equality uses the properties of variance. The second equality follows from the fact that the independence of each \\(\\boldsymbol X_i\\) means they are uncorrelated, and so the variance of their sum equals the sum of their variance. The third equality uses the fact that each \\(\\boldsymbol X_i\\) are drawn from an identical distribution and so have the same variance \\(\\sigma^2\\). The fourth equality is an algebraic simplification."
  },
  {
    "objectID": "blog/random-sampling.html#sampling-distribution-and-the-utility-of-statistical-models",
    "href": "blog/random-sampling.html#sampling-distribution-and-the-utility-of-statistical-models",
    "title": "Model-Based Inference Under Random Sampling",
    "section": "Sampling Distribution and the Utility of Statistical Models",
    "text": "Sampling Distribution and the Utility of Statistical Models\nHow do we define a good guess? How do we choose between two different estimators?\nwe make certain simplifying assumptions about the general structure of \\(F\\) and then\nWe will be able to quantify the uncertainty of the guess if we make stronger starting assumptions about the data generating process."
  },
  {
    "objectID": "blog/random-sampling.html#sampling-distribution-and-the-necessity-for-statistical-models",
    "href": "blog/random-sampling.html#sampling-distribution-and-the-necessity-for-statistical-models",
    "title": "Model-Based Inference Under Random Sampling",
    "section": "Sampling Distribution and the Necessity for Statistical Models",
    "text": "Sampling Distribution and the Necessity for Statistical Models\nHow do we define a good guess? How do we choose between two different estimators?\nwe make certain simplifying assumptions about the general structure of \\(F\\) and then\nWe will be able to quantify the uncertainty of the guess if we make stronger starting assumptions about the data generating process."
  },
  {
    "objectID": "blog/random-sampling.html#necessity-for-statistical-models",
    "href": "blog/random-sampling.html#necessity-for-statistical-models",
    "title": "Random Sampling and Model-Based Inference",
    "section": "Necessity for Statistical Models",
    "text": "Necessity for Statistical Models\n\nSampling Distribution\nwe make certain simplifying assumptions about the general structure of \\(F\\) and then\nWe will be able to quantify the uncertainty of the guess if we make stronger starting assumptions about the data generating process.\n\n\nInference Is Within Statistical Models\nThe discussion in this sectiono is summarized by the figure below.\n\n\n\n\n\nflowchart LR\n  classDef box fill:#f8f9fa,stroke:#444,stroke-width:1px,rx:10,ry:10;\n\n  DGP[\"Data Generating Process\"]:::box\n  Data[\"Observed Data\"]:::box\n  Model[\"Statistical Model\"]:::box\n\n  %% Main flows\n  DGP -- \"Randomness\" --&gt; Data\n  DGP -. \"Assumptions\" .-&gt; Model\n  Model -- \"Probability\" --&gt; Data\n  Data -- \"Inference\" --&gt; Model\n  Model -. \"Approximate Reality\" .-&gt; DGP"
  },
  {
    "objectID": "blog/random-sampling.html#the-necessity-for-statistical-models",
    "href": "blog/random-sampling.html#the-necessity-for-statistical-models",
    "title": "Random Sampling and Model-Based Inference",
    "section": "The Necessity for Statistical Models",
    "text": "The Necessity for Statistical Models\nThe sampling distribution of an estimator is the probability distribution that describes how the estimator’s estimates vary across all possible samples of size \\(n\\) drawn from the DGP. Intuitively, it characterizes the behavior of the estimator under repeated sampling. Under the random sampling assumption, the sampling distribution is completely determined by the DGP \\(F\\), the sample size \\(n\\), and the functional form of the estimator \\(\\hat\\theta\\).\nLet’s revisit the example of the sample mean estimator \\(\\hat\\mu\\) for the population mean \\(\\mu\\). We have already established some features of the sampling distribution of \\(\\hat\\mu\\) despite knowing nothing about \\(F\\). Particularly, the mean of \\(\\hat\\mu\\) is \\(\\mu\\) and its variance is \\(\\sigma^2/n\\). However, to say more about the distribution of \\(\\hat\\mu\\) — like its shape — we need to make assumptions about the DGP.\nA statistical model is a set of assumptions about the general structure of the data generating process \\(F\\). Put differently, we can think of a statistical model as a family of possible distributions that \\(F\\) could belong to. To illustrate the added value of statistical models, suppose our sample \\(\\boldsymbol X_1, \\ldots, \\boldsymbol X_n\\) is drawn iid from \\(\\mathcal{N}(\\mu, \\sigma^2)\\). Since\n\\[\n\\hat\\mu = \\frac{1}{n} \\sum_{i=1}^n \\boldsymbol X_i,\n\\]\nis a linear combination of normally distributed random variables, it is also normally distributed. Moreover, we have previously established that \\(\\mathbb{E}_F[\\hat\\mu] = \\mu\\) and \\(\\operatorname{Var}[\\hat\\mu] = \\sigma^2/n\\) for any DGP \\(F\\). Thus, assumption of a normal DGP allows us to completely characterize the sampling distribution of \\(\\hat\\mu\\) as \\(\\mathcal{N}(\\mu, \\sigma^2/n)\\). This is powerful because we can use this sampling distribution to quantify the uncertainty in our estimates by constructing confidence intervals and conducting hypothesis tests.\n\nConstructing Confidence Intervals for \\(\\hat\\mu\\)"
  },
  {
    "objectID": "blog/random-sampling.html#conclusion",
    "href": "blog/random-sampling.html#conclusion",
    "title": "Random Sampling and Model-Based Inference",
    "section": "Conclusion",
    "text": "Conclusion\n\n\n\n\n\nflowchart LR\n  classDef box fill:#f8f9fa,stroke:#444,stroke-width:1px,rx:10,ry:10;\n\n  DGP[\"Data Generating Process\"]:::box\n  Data[\"Observed Data\"]:::box\n  Model[\"Statistical Model\"]:::box\n\n  %% Main flows\n  DGP -- \"Random Sampling\" --&gt; Data\n  DGP -. \"Assumptions\" .-&gt; Model\n  Model -- \"Probability\" --&gt; Data\n  Data -- \"Inference\" --&gt; Model\n  Model -. \"Approximate Reality\" .-&gt; DGP\n\n\n\n\n\n\nIt is important to note that statistical inference is valid only if the assumptions of statistical model hold."
  },
  {
    "objectID": "blog/random-sampling.html#the-random-sampling-framework",
    "href": "blog/random-sampling.html#the-random-sampling-framework",
    "title": "Random Sampling and Model-Based Inference",
    "section": "The Random Sampling Framework",
    "text": "The Random Sampling Framework\nThe discussion going forward will primarily focus on cross-sectional datasets. These consist of several observations of a collection of variables for a given point in time and can be denoted as\n\\[\n\\{x_{i1}, x_{i2} \\ldots, x_{iK}\\}_{i=1}^n,\n\\]\nwhere \\(x_{ik}\\) is the value of the \\(k\\)-th variable for the \\(i\\)-th unit. A typical source of cross-sectional data in economics is through surveys like the Current Population Survey (CPS) or the American Community Survey (ACS).\n\nMathematical Formalization of the Dataset\nSince observational data is random, it is natural to we view the observed data vector \\[\n\\boldsymbol{x}_i = (x_{i1}, \\ldots, x_{iK})' \\in \\mathbb{R}^K \\quad \\text{for } i = 1, \\ldots, n,\n\\] as a realization of the random vector \\[\n\\boldsymbol X_i = (X_{i1}, \\ldots, X_{iK})' \\in \\mathbb{R}^K \\quad \\text{for } i = 1, \\ldots, n,\n\\]\nwith some associated probability distribution. For example, we can think of the CPS dataset as consisting of random vectors\n\\[\n\\boldsymbol X_i = (sex_i, age_i, educ_i, wage_i)' \\quad \\text{for } i = 1, \\ldots, n.\n\\] For some specific individual \\(i\\) in the CPS dataset, we then observe the vector of realized values1\n\\[\n\\boldsymbol x_i = (1, 25, 16, 25000)'.\n\\] Intuitively, the distinction between the random vector \\(\\boldsymbol X_i\\) and the realization \\(\\boldsymbol x_i\\) is that the former represents the \\(i\\)-th observation before viewing the data (unknown and random) and the latter represents the \\(i\\)-th observation after viewing the data (specific known value).\n\n\nRandom Sampling\nWe have now represented the dataset as a collection of random vectors \\(\\boldsymbol X_1, \\ldots, \\boldsymbol X_n\\), each with some probability distribution \\(F_1 \\ldots, F_n\\). However, the connection between the underlying DGP and the observed data remains unclear. Specifically, we need a simplifying assumption that will allow us to connect the distributions of each \\(\\boldsymbol X_i\\) to the underlying DGP in a straightforward manner. The simplest framework is that of random sampling. Here, we assume that the random vectors \\(\\boldsymbol X_1, \\ldots , \\boldsymbol X_n\\) are independent and identically distributed (iid) with some common but unknown distribution \\(F\\) on \\(\\mathbb{R}^K\\). Thus, under this assumption, the data generating process is precisely the distribution that governs each individual random vector \\(\\boldsymbol X_i\\).\n\nAlternative Assumptions\nThe random sampling assumption is one potential way to characterize the dependence structure across the observed data points. It is popular because (i) it is often reasonable when working with cross-sectional datasets2, and (ii) it is the backbone of several statistical theorems and methods.3 However, it does not necessarily have to hold. For example, we often work with data where the units are connected via some underlying factor (location, industry, etc.). In such cases, the assumption of independence across individual units is violated. An alternative approach in such cases is to instead assume mutual independence across clusters of units. Another example of a violation to the independence assumption is time-series data, where the individual unit is indexed by time. Here, consecutive observations are usually correlated and independence is instead formulated in terms of stationarity and other concepts outside the scope of this post.\n\n\n\nNotation and Example\nThe discussion so far has purely been conceptual, and so it is useful to consider a concrete example. Before doing so, however, let’s clarify some notation used in the random sampling framework. We denote the population-level random variables as \\(X_1, \\ldots, X_K\\). The data generating process \\(F\\) is the joint distribution of these random variables. For example, \\(X_1\\) could denote the generic random variable for wage in the entire US population and we are interested in making inferences about its distribution. We denote the sample-level random variables as \\(X_{i1}, \\ldots, X_{iK}\\). These represent the random variables associated with specific units in the sample. Continuing the example, \\(X_{i1}\\) denotes the random variable representing the wage of individual \\(i\\) in the sample. Finally, we denote the realized observations as \\(X_{i1}=x_{i1}, \\ldots, X_{iK}=x_{iK}\\). These are the actual values we observe. So, \\(X_{i1} = 25,000\\) means that the observed wage of individual \\(i\\) is \\(25,000\\).\nNow suppose our data generating process consists of one random variable with an exponential distribution with scale parameter \\(\\beta = 1\\): \\[\nX \\sim \\text{Exp}(1) \\quad \\text{with density } f(x) = e^{-x} \\text{ for } x \\geq 0.\n\\]\nIf we assume our dataset is a random sample of size 30 from the above DGP, then \\[\nX_{i} \\sim \\text{Exp}(1) \\, \\, \\text{ for } i = 1, \\ldots, 30 \\quad  \\text{and} \\quad  X_i \\perp X_j  \\, \\, \\text{ for } i \\neq j.\n\\]\nThe figure below plots the empirical kernel density of such a random sample (black line) and the true density of the exponential distribution (red line). In practice, we do not know the true DGP and need to guess its characteristics using the observed data. Randomness in the finite observed data makes this a non-trivial task — as illustrated by the discrepancy between the empirical and true densities in the figure below.4\n\n\nCode\n# Simulate IID sample of 30 obs from exp(1)\nset.seed(123)\nn &lt;- 30\nx &lt;- rexp(n, rate = 1)\n\n# Empirical Density \ndens &lt;- density(x)\n\n# True Exponential Density\nxs &lt;- seq(0, max(x), length.out = 200)\nys &lt;- dexp(xs, rate = 1)\n\n# Plot\nplot(dens, main = \"\", xlab = \"Observed Data\", ylab = \"Density\", xlim = c(0, 5), ylim = c(0, max(c(dens$y, ys))))\ncurve(dexp(x, rate = 1), from = min(x), add = TRUE, col = \"red\", lwd = 2)"
  },
  {
    "objectID": "blog/random-sampling.html#introduction",
    "href": "blog/random-sampling.html#introduction",
    "title": "Random Sampling and Model-Based Inference",
    "section": "",
    "text": "This post is an attempt to provide a big-picture understanding of what statistical inference means. The motivation for this post stems from the following Reddit post that I came across."
  },
  {
    "objectID": "blog/linear-algebra-1.html",
    "href": "blog/linear-algebra-1.html",
    "title": "Basic Concepts in Linear Algebra",
    "section": "",
    "text": "In mathematics, an algebraic structure is an abstraction consisting of (i) a set of elements, (ii) operations that manipulate those elements, and (iii) axioms that the operations must satisfy. The power of this abstraction is that once the core properties of the structure are formalized in general, they can be applied to any specific system — mathematical or real-world — that shares the same structure. For example, the field \\(F\\) is an algebraic structure consisting of elements called scalars with operations of addition and multiplication that satisfy six axioms. An ubiquitous field is the set of real numbers \\(\\mathbb{R}\\).\nLinear algebra is the study of vector spaces \\(V\\), which is an algebraic structure defined in the context of a field. The elements in a vector space are called vectors. For any two vectors \\(\\boldsymbol u,\\boldsymbol v \\in V\\), the operation of vector addition creates a third vector \\(\\boldsymbol u + \\boldsymbol v \\in V\\); this is known as closure under vector addition. For any scalar \\(c \\in F\\) and vector \\(\\boldsymbol u \\in V\\), the operation of scalar multiplication creates another vector \\(c \\boldsymbol u \\in V\\); this is known as closure under scalar multiplication. The 8 axioms that govern these two operations are listed here.\nAny sets of elements equipped with vector addition and scalar multiplication that satisfy the closure property and the 8 axioms is considered a vector space. Of particular interest are \\(n\\)-tuples of the form\n\\[\n\\boldsymbol u = (u_1, u_2, \\ldots, u_n),\n\\]\nwhere the components \\(u_1, \\ldots, u_n\\) are scalars from a field \\(F\\). The set of all such \\(n\\)-tuples is denoted by \\(F^n\\).1 For example, \\(\\mathbb{R}^3\\) is the set of all 3-tuples of real numbers. Here, vector addition is defined as the component-wise operation:\n\\[\n\\begin{aligned}\n\\boldsymbol u = (u_1, u_2, &\\ldots, u_n) \\in F^n \\quad\\text{and}\\quad \\boldsymbol v = (v_1, v_2, \\ldots, v_n) \\in F^n \\\\\n\\\\\n\\boldsymbol u + \\boldsymbol v &= (u_1 + v_1, u_2 + v_2, \\ldots, u_n + v_n) \\in F^n.\n\\end{aligned}\n\\]\nSimilarly, scalar multiplication is defined as the component-wise operation:\n\\[\n\\begin{aligned}\nc \\in F \\quad&\\text{and}\\quad \\boldsymbol u = (u_1,u_2, \\ldots, u_n) \\in F^n \\\\\n\\\\\nc \\boldsymbol u &= (c u_1, c u_2, \\ldots, c u_n) \\in F^n.\n\\end{aligned}\n\\]\nA natural generalization of \\(n\\)-tuples is the \\(m \\times n\\) array called the matrix:\n\\[\nA = \\begin{pmatrix}\na_{11} & a_{12} & \\ldots & a_{1n} \\\\\na_{21} & a_{22} & \\ldots & a_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{m1} & a_{m2} & \\ldots & a_{mn}\n\\end{pmatrix}.\n\\]\nThe set of all \\(m \\times n\\) matrices with components in a field \\(F\\) is denoted by \\(F^{m \\times n}\\). Vector addition and scalar multiplication are defined analogously to the component-wise operations for \\(n\\)-tuples. Specifically, for any two matrices \\(A, B \\in F^{m \\times n}\\), vector addition creates a third matrix \\(A + B \\in F^{m \\times n}\\) whose components are given by\n\\[\n(A + B)_{ij} = A_{ij} + B_{ij}.\n\\]\nFor any scalar \\(c \\in F\\), scalar multiplication creates another matrix \\(cA \\in F^{m \\times n}\\) where\n\\[\nc A_{ij} = c (A_{ij}).\n\\]"
  },
  {
    "objectID": "blog/linear-algebra-1.html#vector-spaces-vectors-and-matrices",
    "href": "blog/linear-algebra-1.html#vector-spaces-vectors-and-matrices",
    "title": "Basic Concepts in Linear Algebra",
    "section": "",
    "text": "In mathematics, an algebraic structure is an abstraction consisting of (i) a set of elements, (ii) operations that manipulate those elements, and (iii) axioms that the operations must satisfy. The power of this abstraction is that once the core properties of the structure are formalized in general, they can be applied to any specific system — mathematical or real-world — that shares the same structure. For example, the field \\(F\\) is an algebraic structure consisting of elements called scalars with operations of addition and multiplication that satisfy six axioms. An ubiquitous field is the set of real numbers \\(\\mathbb{R}\\).\nLinear algebra is the study of vector spaces \\(V\\), which is an algebraic structure defined in the context of a field. The elements in a vector space are called vectors. For any two vectors \\(\\boldsymbol u,\\boldsymbol v \\in V\\), the operation of vector addition creates a third vector \\(\\boldsymbol u + \\boldsymbol v \\in V\\); this is known as closure under vector addition. For any scalar \\(c \\in F\\) and vector \\(\\boldsymbol u \\in V\\), the operation of scalar multiplication creates another vector \\(c \\boldsymbol u \\in V\\); this is known as closure under scalar multiplication. The 8 axioms that govern these two operations are listed here.\nAny sets of elements equipped with vector addition and scalar multiplication that satisfy the closure property and the 8 axioms is considered a vector space. Of particular interest are \\(n\\)-tuples of the form\n\\[\n\\boldsymbol u = (u_1, u_2, \\ldots, u_n),\n\\]\nwhere the components \\(u_1, \\ldots, u_n\\) are scalars from a field \\(F\\). The set of all such \\(n\\)-tuples is denoted by \\(F^n\\).1 For example, \\(\\mathbb{R}^3\\) is the set of all 3-tuples of real numbers. Here, vector addition is defined as the component-wise operation:\n\\[\n\\begin{aligned}\n\\boldsymbol u = (u_1, u_2, &\\ldots, u_n) \\in F^n \\quad\\text{and}\\quad \\boldsymbol v = (v_1, v_2, \\ldots, v_n) \\in F^n \\\\\n\\\\\n\\boldsymbol u + \\boldsymbol v &= (u_1 + v_1, u_2 + v_2, \\ldots, u_n + v_n) \\in F^n.\n\\end{aligned}\n\\]\nSimilarly, scalar multiplication is defined as the component-wise operation:\n\\[\n\\begin{aligned}\nc \\in F \\quad&\\text{and}\\quad \\boldsymbol u = (u_1,u_2, \\ldots, u_n) \\in F^n \\\\\n\\\\\nc \\boldsymbol u &= (c u_1, c u_2, \\ldots, c u_n) \\in F^n.\n\\end{aligned}\n\\]\nA natural generalization of \\(n\\)-tuples is the \\(m \\times n\\) array called the matrix:\n\\[\nA = \\begin{pmatrix}\na_{11} & a_{12} & \\ldots & a_{1n} \\\\\na_{21} & a_{22} & \\ldots & a_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{m1} & a_{m2} & \\ldots & a_{mn}\n\\end{pmatrix}.\n\\]\nThe set of all \\(m \\times n\\) matrices with components in a field \\(F\\) is denoted by \\(F^{m \\times n}\\). Vector addition and scalar multiplication are defined analogously to the component-wise operations for \\(n\\)-tuples. Specifically, for any two matrices \\(A, B \\in F^{m \\times n}\\), vector addition creates a third matrix \\(A + B \\in F^{m \\times n}\\) whose components are given by\n\\[\n(A + B)_{ij} = A_{ij} + B_{ij}.\n\\]\nFor any scalar \\(c \\in F\\), scalar multiplication creates another matrix \\(cA \\in F^{m \\times n}\\) where\n\\[\nc A_{ij} = c (A_{ij}).\n\\]"
  },
  {
    "objectID": "blog/linear-algebra-1.html#subspaces",
    "href": "blog/linear-algebra-1.html#subspaces",
    "title": "Basic Concepts in Linear Algebra",
    "section": "Subspaces",
    "text": "Subspaces\nIt is often useful to consider a lower-dimension vector space that still preserves the properties of the original vector space. Formally, for a vector space \\(V\\), we define the subspace \\(W\\) to be any vector space that consists of a nonempty subset of the vectors in \\(V\\) endowed with the same operations of vector addition and scalar multiplication defined on \\(V\\).\nThis definition can seem abstract at first, and so it’s valuable to consider a concrete example. Recall that the vector space \\(\\mathbb{R}^3\\) is the set of all 3-tuples of real numbers. This is visualized by the faint blue lattice in Figure 1. Now, consider following subset of \\(\\mathbb{R}^3\\)\n\\[\nW = \\{(x,y,0): x, y \\in \\mathbb{R}\\} \\subseteq {\\mathbb{R^3}},\n\\]\nwhich is visualized as the turquoise \\(x-y\\) plane in the figure below.\n\n\nCode\nL &lt;- 6       # half-size of the cube window\nstep &lt;- 1    # spacing of lattice points\n\n# Lattice points: all 3D triples on a grid\ng &lt;- seq(-L, L, by = step)\npts &lt;- expand.grid(x = g, y = g, z = g)\n\n# Plane through the origin (z=0)\nZ &lt;- matrix(0, length(g), length(g))\n\n# Define cube edges\nedges &lt;- list(\n  rbind(c(-L,-L,-L), c( L,-L,-L)),  rbind(c(-L, L,-L), c( L, L,-L)),\n  rbind(c(-L,-L, L), c( L,-L, L)),  rbind(c(-L, L, L), c( L, L, L)),\n  rbind(c(-L,-L,-L), c(-L, L,-L)),  rbind(c( L,-L,-L), c( L, L,-L)),\n  rbind(c(-L,-L, L), c(-L, L, L)),  rbind(c( L,-L, L), c( L, L, L)),\n  rbind(c(-L,-L,-L), c(-L,-L, L)),  rbind(c( L,-L,-L), c( L,-L, L)),\n  rbind(c(-L, L,-L), c(-L, L, L)),  rbind(c( L, L,-L), c( L, L, L))\n)\n\np &lt;- plot_ly()\n\n# (1) Lattice points (all possible 3-tuples)\np &lt;- add_markers(\n  p, data = pts, x = ~x, y = ~y, z = ~z,\n  opacity = 0.12, marker = list(size = 2),\n  showlegend = FALSE, hoverinfo = \"skip\"\n)\n\n# (2) Plane through the origin\np &lt;- add_surface(\n  p, x = g, y = g, z = Z,\n  opacity = 0.35, showscale = FALSE\n)\n\n# (3) Axes\np &lt;- add_trace(p, type = \"scatter3d\", mode = \"lines\",\n               x = c(-L, L), y = c(0, 0), z = c(0, 0),\n               line = list(width = 6), hoverinfo = \"skip\")\np &lt;- add_trace(p, type = \"scatter3d\", mode = \"lines\",\n               x = c(0, 0), y = c(-L, L), z = c(0, 0),\n               line = list(width = 6), hoverinfo = \"skip\")\np &lt;- add_trace(p, type = \"scatter3d\", mode = \"lines\",\n               x = c(0, 0), y = c(0, 0), z = c(-L, L),\n               line = list(width = 6), hoverinfo = \"skip\")\n\n# (4) Wireframe cube (wideframe)\nfor (e in edges) {\n  p &lt;- add_trace(\n    p, type = \"scatter3d\", mode = \"lines\",\n    x = e[,1], y = e[,2], z = e[,3],\n    line = list(width = 4, color = \"orange\"),\n    showlegend = FALSE, hoverinfo = \"skip\"\n  )\n}\n\n# Layout\np &lt;- layout(\n  p,\n  scene = list(\n    aspectmode = \"cube\",\n    xaxis = list(title = \"x\", range = c(-L, L), showgrid = FALSE, zeroline = FALSE),\n    yaxis = list(title = \"y\", range = c(-L, L), showgrid = FALSE, zeroline = FALSE),\n    zaxis = list(title = \"z\", range = c(-L, L), showgrid = FALSE, zeroline = FALSE),\n    bgcolor = \"white\"\n  ),\n  showlegend = FALSE\n)\n\np\n\n\n\n\n\n\n\n\nFigure 1: A 2D plane through the origin is a subspace of R³.\n\n\n\n\nThe subset \\(W\\) is a subspace of \\(\\mathbb{R}^3\\). To see this, note that for any \\(u = (u_1, u_2, 0) \\in W\\), \\(v = (v_1, v_2, 0) \\in W\\), and \\(c \\in \\mathbb{R}\\), the component-wise definitions of vector addition and scalar multiplication for \\(\\mathbb{R}^3\\) are closed in \\(W\\)\n\\[\n\\begin{aligned}\nu + v = &(u_1 + v_1, u_2 + v_2, 0) \\in W \\\\\n\\\\\nc u &= (c u_1, c u_2, 0) \\in W,\n\\end{aligned}\n\\] and thus \\(W\\) is a valid vector space.\nNotice that the origin \\((0,0,0)\\) — the additive identity for \\(\\mathbb{R}^3\\) — is contained in \\(W\\). This is not a coincidence: every subspace of \\(\\mathbb{R}^3\\) must contain the origin. To see this, consider the \\(x-y\\) plane shifted up by one unit:\n\\[\nW' = \\{(x,y,1): x, y \\in \\mathbb{R}\\} \\subseteq {\\mathbb{R^3}}.\n\\]\nThis set does not include the origin, and it fails to be a subspace because the operations of vector addition and scalar multiplication are not closed in \\(W'\\). Specifically, for any \\(u = (u_1, u_2, 1) \\in W'\\), \\(v = (v_1, v_2, 1) \\in W'\\), and \\(c \\in \\{\\mathbb{R} / 1\\}\\), we have that\n\\[\n\\begin{aligned}\nu + v = &(u_1 + v_1, u_2 + v_2, 2) \\notin W' \\\\\n\\\\\ncu &= (c u_1, c u_2, c) \\notin W',\n\\end{aligned}\n\\] and so \\(W'\\) is not a valid vector space. Importantly, this is a general property not limited to \\(\\mathbb{R}^3\\): any subspace must contain the additive identity (also called the zero vector) of the parent vector space."
  },
  {
    "objectID": "blog/linear-algebra-1.html#linear-combinations-basis-vectors-and-span",
    "href": "blog/linear-algebra-1.html#linear-combinations-basis-vectors-and-span",
    "title": "Basic Concepts in Linear Algebra",
    "section": "Linear Combinations, Basis Vectors and Span",
    "text": "Linear Combinations, Basis Vectors and Span\nA vector space \\(V\\) contains infinitely many vectors. We are interested in constructing a smaller, finite set of vectors that captures the entire structure in a much more tractable manner. For example, consider the vector space \\(\\mathbb{R}^3\\) in Figure 1. It seems like any point on the blue lattice structure can be described by how far it extends along the \\(x\\), \\(y\\), and \\(z\\) axes. This section formalizes this idea.\nLet \\(\\mathcal{A} = \\{\\boldsymbol{u}_1, \\ldots, \\boldsymbol{u}_n\\}\\) be some finite subset of vectors in \\(V\\). One way to formalize the idea of \\(\\mathcal{A}\\) capturing the entire structure of \\(V\\) is if any vector \\(\\boldsymbol{v} \\in V\\) can be expressed as a linear combination of the vectors in \\(\\mathcal{A}\\):\n\\[\n\\boldsymbol v = c_1 \\boldsymbol u_1 + c_2 \\boldsymbol u_2 + \\ldots + c_n \\boldsymbol u_n = \\sum_{i=1}^n c_i \\boldsymbol u_i \\in V,\n\\]\nwhere \\(c_1, \\ldots, c_n \\in F\\). If this is the case, then we say that the set \\(\\mathcal{A}\\) spans the vector space \\(V\\). An equivalent way to say this is that the set of all possible linear combinations of the vectors in \\(\\mathcal{A}\\) — the span of \\(\\mathcal{A}\\) — is equal to \\(V\\).\nWe are also interested in efficiency. That is to say, we want \\(\\mathcal{A}\\) to be as small as possible while still spanning \\(V\\). We call a set of vectors linearly dependent if one of the vectors can be expressed as a linear combination of the others. More formally, the vectors \\(\\boldsymbol{u}_1, \\ldots, \\boldsymbol{u}_n\\) in \\(\\mathcal{A}\\) would be linearly dependent if\n\\[\nc_1 \\boldsymbol u_1 + c_2 \\boldsymbol u_2 + \\ldots + c_n \\boldsymbol u_n = 0,\n\\]\nand not all of the \\(c_i\\) are zero. Thus, we would like to remove any linearly dependent vectors from \\(\\mathcal{A}\\): such vectors are going to be redundant since they will be a part of the span (i.e. the set of all possible linear combinations) of the other vectors in \\(\\mathcal{A}\\). Formally, we say that the vectors \\(\\boldsymbol{u}_1, \\ldots, \\boldsymbol{u}_n\\) in \\(\\mathcal{A}\\) are linearly independent if\n\\[\nc_1 \\boldsymbol u_1 + c_2 \\boldsymbol u_2 + \\ldots + c_n \\boldsymbol u_n = 0,\n\\]\nonly if \\(c_1 = c_2 = \\ldots = c_n = 0\\)."
  },
  {
    "objectID": "blog/linear-algebra-1.html#footnotes",
    "href": "blog/linear-algebra-1.html#footnotes",
    "title": "Basic Concepts in Linear Algebra",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThese tuples are usually called vectors in most applied settings, but to avoid confusion with the more general definition of vectors in a vector space, I refer to them as \\(n\\)-tuples.↩︎"
  },
  {
    "objectID": "blog/linear-algebra-1.html#linear-combinations-linear-independence-spans-and-basis-vectors",
    "href": "blog/linear-algebra-1.html#linear-combinations-linear-independence-spans-and-basis-vectors",
    "title": "Basic Concepts in Linear Algebra",
    "section": "Linear Combinations, Linear Independence, Spans, and Basis Vectors",
    "text": "Linear Combinations, Linear Independence, Spans, and Basis Vectors\nA vector space \\(V\\) contains infinitely many vectors. We are interested in constructing a smaller, finite set of vectors that captures the entire structure in a much more tractable manner. For example, consider the vector space \\(\\mathbb{R}^3\\) in Figure 1. It seems like any point on the blue lattice structure can be described by how far it extends along the \\(x\\), \\(y\\), and \\(z\\) axes. This section formalizes this idea.\nLet \\(\\mathcal{A} = \\{\\boldsymbol{u}_1, \\ldots, \\boldsymbol{u}_n\\}\\) be some finite subset of vectors in \\(V\\). One way to formalize the idea of \\(\\mathcal{A}\\) capturing the entire structure of \\(V\\) is if any vector \\(\\boldsymbol{v} \\in V\\) can be expressed as a linear combination of the vectors in \\(\\mathcal{A}\\):\n\\[\n\\boldsymbol v = c_1 \\boldsymbol u_1 + c_2 \\boldsymbol u_2 + \\ldots + c_n \\boldsymbol u_n = \\sum_{i=1}^n c_i \\boldsymbol u_i \\in V,\n\\]\nwhere \\(c_1, \\ldots, c_n \\in F\\). If this is the case, then we say that the set \\(\\mathcal{A}\\) spans the vector space \\(V\\). An equivalent way to say this is that the set of all possible linear combinations of the vectors in \\(\\mathcal{A}\\) — the span of \\(\\mathcal{A}\\) — is equal to \\(V\\).\nWe are also interested in efficiency. That is to say, we want \\(\\mathcal{A}\\) to be as small as possible while still spanning \\(V\\). We call a set of vectors linearly dependent if one of the vectors can be expressed as a linear combination of the others. More formally, the vectors \\(\\boldsymbol{u}_1, \\ldots, \\boldsymbol{u}_n\\) in \\(\\mathcal{A}\\) would be linearly dependent if\n\\[\nc_1 \\boldsymbol u_1 + c_2 \\boldsymbol u_2 + \\ldots + c_n \\boldsymbol u_n = 0,\n\\]\nand not all of the \\(c_i\\) are zero. Thus, we would like to remove any linearly dependent vectors from \\(\\mathcal{A}\\): such vectors are going to be redundant since they will be a part of the span (i.e. the set of all possible linear combinations) of the other vectors in \\(\\mathcal{A}\\). Formally, we say that the vectors \\(\\boldsymbol{u}_1, \\ldots, \\boldsymbol{u}_n\\) in \\(\\mathcal{A}\\) are linearly independent if\n\\[\nc_1 \\boldsymbol u_1 + c_2 \\boldsymbol u_2 + \\ldots + c_n \\boldsymbol u_n = 0,\n\\]\nonly if \\(c_1 = c_2 = \\ldots = c_n = 0\\)."
  },
  {
    "objectID": "blog/linear-algebra-1.html#linear-combinations-spans-and-basis-vectors",
    "href": "blog/linear-algebra-1.html#linear-combinations-spans-and-basis-vectors",
    "title": "Basic Concepts in Linear Algebra",
    "section": "Linear Combinations, Spans, and Basis Vectors",
    "text": "Linear Combinations, Spans, and Basis Vectors\nA vector space \\(V\\) contains infinitely many vectors. We are interested in constructing a smaller, finite set of vectors that captures the entire structure in a much more tractable manner. For example, consider the vector space \\(\\mathbb{R}^3\\) in Figure 1. It seems like any point on the blue lattice structure can be described by how far it extends along the \\(x\\), \\(y\\), and \\(z\\) axes. This section formalizes this idea.\nLet \\(\\mathcal{A} = \\{\\boldsymbol{u}_1, \\ldots, \\boldsymbol{u}_n\\}\\) be some finite subset of vectors in \\(V\\). One way to formalize the idea of \\(\\mathcal{A}\\) capturing the entire structure of \\(V\\) is if any vector \\(\\boldsymbol{v} \\in V\\) can be expressed as a linear combination of the vectors in \\(\\mathcal{A}\\):\n\\[\n\\boldsymbol v = c_1 \\boldsymbol u_1 + c_2 \\boldsymbol u_2 + \\ldots + c_n \\boldsymbol u_n = \\sum_{i=1}^n c_i \\boldsymbol u_i \\in V,\n\\]\nwhere \\(c_1, \\ldots, c_n \\in F\\). If this is the case, then we say that the set \\(\\mathcal{A}\\) spans the vector space \\(V\\). An equivalent way to say this is that the set of all possible linear combinations of the vectors in \\(\\mathcal{A}\\) — the span of \\(\\mathcal{A}\\) — is equal to \\(V\\).\nWe are also interested in efficiency. That is to say, we want \\(\\mathcal{A}\\) to be as small as possible while still spanning \\(V\\). We call a set of vectors linearly dependent if one of the vectors can be expressed as a linear combination of the others. More formally, the vectors \\(\\boldsymbol{u}_1, \\ldots, \\boldsymbol{u}_n\\) in \\(\\mathcal{A}\\) would be linearly dependent if\n\\[\nc_1 \\boldsymbol u_1 + c_2 \\boldsymbol u_2 + \\ldots + c_n \\boldsymbol u_n = 0,\n\\]\nand not all of the \\(c_i\\) are zero. Thus, we would like to remove any linearly dependent vectors from \\(\\mathcal{A}\\): such vectors are going to be redundant since they will be a part of the span (i.e. the set of all possible linear combinations) of the other vectors in \\(\\mathcal{A}\\). Formally, we say that the vectors \\(\\boldsymbol{u}_1, \\ldots, \\boldsymbol{u}_n\\) in \\(\\mathcal{A}\\) are linearly independent if\n\\[\nc_1 \\boldsymbol u_1 + c_2 \\boldsymbol u_2 + \\ldots + c_n \\boldsymbol u_n = 0,\n\\]\nonly if \\(c_1 = c_2 = \\ldots = c_n = 0\\).\nTaken together, these two ideas — spanning and linear independence — gives us exactly what we were looking for: a minimal yet complete description of the vector space \\(V\\). More formally, we call a set of vectors a basis for the vector space \\(V\\) if it is a linearly independent subset that spans \\(V\\)."
  },
  {
    "objectID": "blog/linear-algebra-1.html#linear-combinations-spans-and-basis",
    "href": "blog/linear-algebra-1.html#linear-combinations-spans-and-basis",
    "title": "Basic Concepts in Linear Algebra",
    "section": "Linear Combinations, Spans, and Basis",
    "text": "Linear Combinations, Spans, and Basis\nA vector space \\(V\\) contains infinitely many vectors. We are interested in constructing a smaller, finite set of vectors that captures the entire structure in a much more tractable manner. For example, consider the vector space \\(\\mathbb{R}^3\\) in Figure 1. It seems like any point on the blue lattice structure can be described by how far it extends along the \\(x\\), \\(y\\), and \\(z\\) axes. This section formalizes this idea.\nLet \\(\\mathcal{A} = \\{\\boldsymbol{u}_1, \\ldots, \\boldsymbol{u}_n\\}\\) be some finite subset of vectors in \\(V\\). One way to formalize the idea of \\(\\mathcal{A}\\) capturing the entire structure of \\(V\\) is if any vector \\(\\boldsymbol{v} \\in V\\) can be expressed as a linear combination of the vectors in \\(\\mathcal{A}\\):\n\\[\n\\boldsymbol v = c_1 \\boldsymbol u_1 + c_2 \\boldsymbol u_2 + \\ldots + c_n \\boldsymbol u_n = \\sum_{i=1}^n c_i \\boldsymbol u_i \\in V,\n\\]\nwhere \\(c_1, \\ldots, c_n \\in F\\). If this is the case, then we say that the set \\(\\mathcal{A}\\) spans the vector space \\(V\\). An equivalent way to say this is that the set of all possible linear combinations of the vectors in \\(\\mathcal{A}\\) — the span of \\(\\mathcal{A}\\) — is equal to \\(V\\).\nWe are also interested in efficiency. That is to say, we want \\(\\mathcal{A}\\) to be as small as possible while still spanning \\(V\\). We call a set of vectors linearly dependent if one of the vectors can be expressed as a linear combination of the others. More formally, the vectors \\(\\boldsymbol{u}_1, \\ldots, \\boldsymbol{u}_n\\) in \\(\\mathcal{A}\\) would be linearly dependent if\n\\[\nc_1 \\boldsymbol u_1 + c_2 \\boldsymbol u_2 + \\ldots + c_n \\boldsymbol u_n = 0,\n\\]\nand not all of the \\(c_i\\) are zero. Thus, we would like to remove any linearly dependent vectors from \\(\\mathcal{A}\\): such vectors are going to be redundant since they will be a part of the span (i.e. the set of all possible linear combinations) of the other vectors in \\(\\mathcal{A}\\). Formally, we say that the vectors \\(\\boldsymbol{u}_1, \\ldots, \\boldsymbol{u}_n\\) in \\(\\mathcal{A}\\) are linearly independent if\n\\[\nc_1 \\boldsymbol u_1 + c_2 \\boldsymbol u_2 + \\ldots + c_n \\boldsymbol u_n = 0,\n\\]\nonly if \\(c_1 = c_2 = \\ldots = c_n = 0\\).\nTaken together, these two ideas — spanning and linear independence — gives us exactly what we were looking for: a minimal yet complete description of the vector space \\(V\\). More formally, we call a set of vectors a basis for the vector space \\(V\\) if it is a linearly independent subset that spans \\(V\\)."
  },
  {
    "objectID": "blog/linear-algebra-1.html#linear-combinations-linear-independence-and-basis",
    "href": "blog/linear-algebra-1.html#linear-combinations-linear-independence-and-basis",
    "title": "Basic Concepts in Linear Algebra",
    "section": "Linear Combinations, Linear Independence, and Basis",
    "text": "Linear Combinations, Linear Independence, and Basis\nA vector space \\(V\\) contains infinitely many vectors. We are interested in constructing a smaller, finite set of vectors that captures the entire structure in a much more tractable manner. For example, consider the vector space \\(\\mathbb{R}^3\\) in Figure 1. It seems like any point on the blue lattice structure can be described by how far it extends along the \\(x\\), \\(y\\), and \\(z\\) axes. This section formalizes this idea.\nLet \\(\\mathcal{A} = \\{\\boldsymbol{u}_1, \\ldots, \\boldsymbol{u}_n\\}\\) be some finite subset of vectors in \\(V\\). One way to formalize the idea of \\(\\mathcal{A}\\) capturing the entire structure of \\(V\\) is if any vector \\(\\boldsymbol{v} \\in V\\) can be expressed as a linear combination of the vectors in \\(\\mathcal{A}\\):\n\\[\n\\boldsymbol v = c_1 \\boldsymbol u_1 + c_2 \\boldsymbol u_2 + \\ldots + c_n \\boldsymbol u_n = \\sum_{i=1}^n c_i \\boldsymbol u_i \\in V,\n\\]\nwhere \\(c_1, \\ldots, c_n \\in F\\). If this is the case, then we say that the set \\(\\mathcal{A}\\) spans the vector space \\(V\\). An equivalent way to say this is that the set of all possible linear combinations of the vectors in \\(\\mathcal{A}\\) — the span of \\(\\mathcal{A}\\) — is equal to \\(V\\).\nWe are also interested in efficiency. That is to say, we want \\(\\mathcal{A}\\) to be as small as possible while still spanning \\(V\\). We call a set of vectors linearly dependent if one of the vectors can be expressed as a linear combination of the others. More formally, the vectors \\(\\boldsymbol{u}_1, \\ldots, \\boldsymbol{u}_n\\) in \\(\\mathcal{A}\\) would be linearly dependent if\n\\[\nc_1 \\boldsymbol u_1 + c_2 \\boldsymbol u_2 + \\ldots + c_n \\boldsymbol u_n = 0,\n\\]\nand not all of the \\(c_i\\) are zero. Thus, we would like to remove any linearly dependent vectors from \\(\\mathcal{A}\\): such vectors are going to be redundant since they will be a part of the span (i.e. the set of all possible linear combinations) of the other vectors in \\(\\mathcal{A}\\). Formally, we say that the vectors \\(\\boldsymbol{u}_1, \\ldots, \\boldsymbol{u}_n\\) in \\(\\mathcal{A}\\) are linearly independent if\n\\[\nc_1 \\boldsymbol u_1 + c_2 \\boldsymbol u_2 + \\ldots + c_n \\boldsymbol u_n = 0,\n\\]\nonly if \\(c_1 = c_2 = \\ldots = c_n = 0\\).\nTaken together, these two ideas — spanning and linear independence — gives us exactly what we were looking for: a minimal yet complete description of the vector space \\(V\\). More formally, we call a set of vectors a basis for the vector space \\(V\\) if it is a linearly independent subset that spans \\(V\\)."
  },
  {
    "objectID": "blog/linear-algebra-1.html#linear-combination-linear-independence-and-basis",
    "href": "blog/linear-algebra-1.html#linear-combination-linear-independence-and-basis",
    "title": "Basic Concepts in Linear Algebra",
    "section": "Linear Combination, Linear Independence, and Basis",
    "text": "Linear Combination, Linear Independence, and Basis\nA vector space \\(V\\) contains infinitely many vectors. We are interested in constructing a smaller, finite set of vectors that captures the entire structure in a much more tractable manner. For example, consider the vector space \\(\\mathbb{R}^3\\) in Figure 1. It seems like any point on the blue lattice structure can be described by how far it extends along the \\(x\\), \\(y\\), and \\(z\\) axes. This section formalizes this idea.\nLet \\(\\mathcal{A} = \\{\\boldsymbol{u}_1, \\ldots, \\boldsymbol{u}_n\\}\\) be some finite subset of vectors in \\(V\\). One way to formalize the idea of \\(\\mathcal{A}\\) capturing the entire structure of \\(V\\) is if any vector \\(\\boldsymbol{v} \\in V\\) can be expressed as a linear combination of the vectors in \\(\\mathcal{A}\\):\n\\[\n\\boldsymbol v = c_1 \\boldsymbol u_1 + c_2 \\boldsymbol u_2 + \\ldots + c_n \\boldsymbol u_n = \\sum_{i=1}^n c_i \\boldsymbol u_i \\in V,\n\\]\nwhere \\(c_1, \\ldots, c_n \\in F\\). If this is the case, then we say that the set \\(\\mathcal{A}\\) spans the vector space \\(V\\). An equivalent way to say this is that the set of all possible linear combinations of the vectors in \\(\\mathcal{A}\\) — the span of \\(\\mathcal{A}\\) — is equal to \\(V\\).\nWe are also interested in efficiency. That is to say, we want \\(\\mathcal{A}\\) to be as small as possible while still spanning \\(V\\). We call a set of vectors linearly dependent if one of the vectors can be expressed as a linear combination of the others. More formally, the vectors \\(\\boldsymbol{u}_1, \\ldots, \\boldsymbol{u}_n\\) in \\(\\mathcal{A}\\) would be linearly dependent if\n\\[\nc_1 \\boldsymbol u_1 + c_2 \\boldsymbol u_2 + \\ldots + c_n \\boldsymbol u_n = 0,\n\\]\nand not all of the \\(c_i\\) are zero. Thus, we would like to remove any linearly dependent vectors from \\(\\mathcal{A}\\): such vectors are going to be redundant since they will be a part of the span (i.e. the set of all possible linear combinations) of the other vectors in \\(\\mathcal{A}\\). Formally, we say that the vectors \\(\\boldsymbol{u}_1, \\ldots, \\boldsymbol{u}_n\\) in \\(\\mathcal{A}\\) are linearly independent if\n\\[\nc_1 \\boldsymbol u_1 + c_2 \\boldsymbol u_2 + \\ldots + c_n \\boldsymbol u_n = 0,\n\\]\nonly if \\(c_1 = c_2 = \\ldots = c_n = 0\\).\nTaken together, these two ideas — spanning and linear independence — gives us exactly what we were looking for: a minimal yet complete description of the vector space \\(V\\). More formally, we call a set of vectors a basis for the vector space \\(V\\) if it is a linearly independent subset that spans \\(V\\)."
  },
  {
    "objectID": "blog/linear-algebra-1.html#basis-and-dimension",
    "href": "blog/linear-algebra-1.html#basis-and-dimension",
    "title": "Basic Concepts in Linear Algebra",
    "section": "Basis and Dimension",
    "text": "Basis and Dimension\nA vector space \\(V\\) contains infinitely many vectors. We are interested in constructing a smaller set of vectors that captures the entire structure in a much more tractable manner. For example, consider the vector space \\(\\mathbb{R}^3\\) in Figure 1. It seems like any point on the blue lattice structure can be described by how far it extends along the \\(x\\), \\(y\\), and \\(z\\) axes. This section formalizes this idea.\nLet \\(\\mathcal{A} = \\{\\boldsymbol{u}_1, \\ldots, \\boldsymbol{u}_n\\}\\) be some finite subset of vectors in \\(V\\). One way to formalize the idea of \\(\\mathcal{A}\\) capturing the entire structure of \\(V\\) is if any vector \\(\\boldsymbol{v} \\in V\\) can be expressed as a linear combination of the vectors in \\(\\mathcal{A}\\):\n\\[\n\\boldsymbol v = c_1 \\boldsymbol u_1 + c_2 \\boldsymbol u_2 + \\ldots + c_n \\boldsymbol u_n = \\sum_{i=1}^n c_i \\boldsymbol u_i \\in V,\n\\]\nwhere \\(c_1, \\ldots, c_n \\in F\\). If this is the case, then we say that the set \\(\\mathcal{A}\\) spans the vector space \\(V\\). An equivalent way to say this is that the set of all possible linear combinations of the vectors in \\(\\mathcal{A}\\) — the span of \\(\\mathcal{A}\\) — is equal to \\(V\\).\nWe are also interested in efficiency. That is to say, we want \\(\\mathcal{A}\\) to be as small as possible while still spanning \\(V\\). We call a set of vectors linearly dependent if one of the vectors can be expressed as a linear combination of the others. More formally, the vectors \\(\\boldsymbol{u}_1, \\ldots, \\boldsymbol{u}_n\\) in \\(\\mathcal{A}\\) would be linearly dependent if\n\\[\nc_1 \\boldsymbol u_1 + c_2 \\boldsymbol u_2 + \\ldots + c_n \\boldsymbol u_n = 0,\n\\]\nand not all of the \\(c_i\\) are zero. Thus, we would like to remove any linearly dependent vectors from \\(\\mathcal{A}\\): such vectors are going to be redundant since they will be a part of the span (i.e. the set of all possible linear combinations) of the other vectors in \\(\\mathcal{A}\\). Formally, we say that the vectors \\(\\boldsymbol{u}_1, \\ldots, \\boldsymbol{u}_n\\) in \\(\\mathcal{A}\\) are linearly independent if\n\\[\nc_1 \\boldsymbol u_1 + c_2 \\boldsymbol u_2 + \\ldots + c_n \\boldsymbol u_n = 0,\n\\]\nonly if \\(c_1 = c_2 = \\ldots = c_n = 0\\).\nTaken together, these two ideas — spanning and linear independence — gives us exactly what we were looking for: a minimal yet complete description of the vector space \\(V\\). More formally, we call a set of vectors a basis for the vector space \\(V\\) if it is a linearly independent subset that spans \\(V\\). The vectors in a basis are called basis vectors."
  },
  {
    "objectID": "blog/linalg-basics.html",
    "href": "blog/linalg-basics.html",
    "title": "Basic Concepts in Linear Algebra",
    "section": "",
    "text": "In mathematics, an algebraic structure is an abstraction consisting of (i) a set of elements, (ii) operations that manipulate those elements, and (iii) axioms that the operations must satisfy. The power of this abstraction is that once the core properties of the structure are formalized in general, they can be applied to any specific system — mathematical or real-world — that shares the same structure. For example, the field \\(F\\) is an algebraic structure consisting of elements called scalars with operations of addition and multiplication that satisfy a number of axioms. A ubiquitous field is the set of real numbers \\(\\mathbb{R}\\).\nLinear algebra is the study of vector spaces \\(V\\), which is an algebraic structure defined in the context of a field. The elements in a vector space are called vectors. For any two vectors \\(\\boldsymbol u,\\boldsymbol v \\in V\\), the operation of vector addition creates a third vector \\(\\boldsymbol u + \\boldsymbol v \\in V\\); this is known as closure under vector addition. For any scalar \\(c \\in F\\) and vector \\(\\boldsymbol u \\in V\\), the operation of scalar multiplication creates another vector \\(c \\boldsymbol u \\in V\\); this is known as closure under scalar multiplication. The 8 axioms that govern these two operations are listed here.\nAny sets of elements equipped with vector addition and scalar multiplication that satisfy the closure property and the 8 axioms is considered a vector space. Of particular interest are \\(n\\)-tuples of the form\n\\[\n\\boldsymbol u = (u_1, u_2, \\ldots, u_n),\n\\]\nwhere the components \\(u_1, \\ldots, u_n\\) are scalars from a field \\(F\\). The set of all such \\(n\\)-tuples is denoted by \\(F^n\\).1 For example, \\(\\mathbb{R}^3\\) is the set of all 3-tuples of real numbers. Here, vector addition is defined as the component-wise operation:\n\\[\n\\begin{aligned}\n\\boldsymbol u = (u_1, u_2, &\\ldots, u_n) \\in F^n \\quad\\text{and}\\quad \\boldsymbol v = (v_1, v_2, \\ldots, v_n) \\in F^n \\\\\n\\\\\n\\boldsymbol u + \\boldsymbol v &= (u_1 + v_1, u_2 + v_2, \\ldots, u_n + v_n) \\in F^n.\n\\end{aligned}\n\\]\nSimilarly, scalar multiplication is defined as the component-wise operation:\n\\[\n\\begin{aligned}\nc \\in F \\quad&\\text{and}\\quad \\boldsymbol u = (u_1,u_2, \\ldots, u_n) \\in F^n \\\\\n\\\\\nc \\boldsymbol u &= (c u_1, c u_2, \\ldots, c u_n) \\in F^n.\n\\end{aligned}\n\\]\nA natural generalization of \\(n\\)-tuples is the \\(m \\times n\\) array called the matrix:\n\\[\n\\mathbf A = \\begin{pmatrix}\na_{11} & a_{12} & \\ldots & a_{1n} \\\\\na_{21} & a_{22} & \\ldots & a_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{m1} & a_{m2} & \\ldots & a_{mn}\n\\end{pmatrix}, \\quad a_{ij} \\in F.\n\\]\nThe set of all \\(m \\times n\\) matrices with components in a field \\(F\\) is denoted by \\(F^{m \\times n}\\). Vector addition and scalar multiplication are defined analogously to the component-wise operations for \\(n\\)-tuples. Specifically, for any two matrices \\(A, B \\in F^{m \\times n}\\), vector addition creates a third matrix \\(A + B \\in F^{m \\times n}\\) whose components are given by\n\\[\n(\\mathbf{A} + \\mathbf B)_{ij} = \\mathbf A_{ij} + \\mathbf B_{ij}.\n\\]\nFor any scalar \\(c \\in F\\), scalar multiplication creates another matrix \\(cA \\in F^{m \\times n}\\) where\n\\[\nc \\mathbf A_{ij} = c (\\mathbf A_{ij}).\n\\]"
  },
  {
    "objectID": "blog/linalg-basics.html#vector-spaces-vectors-and-matrices",
    "href": "blog/linalg-basics.html#vector-spaces-vectors-and-matrices",
    "title": "Basic Concepts in Linear Algebra",
    "section": "",
    "text": "In mathematics, an algebraic structure is an abstraction consisting of (i) a set of elements, (ii) operations that manipulate those elements, and (iii) axioms that the operations must satisfy. The power of this abstraction is that once the core properties of the structure are formalized in general, they can be applied to any specific system — mathematical or real-world — that shares the same structure. For example, the field \\(F\\) is an algebraic structure consisting of elements called scalars with operations of addition and multiplication that satisfy a number of axioms. A ubiquitous field is the set of real numbers \\(\\mathbb{R}\\).\nLinear algebra is the study of vector spaces \\(V\\), which is an algebraic structure defined in the context of a field. The elements in a vector space are called vectors. For any two vectors \\(\\boldsymbol u,\\boldsymbol v \\in V\\), the operation of vector addition creates a third vector \\(\\boldsymbol u + \\boldsymbol v \\in V\\); this is known as closure under vector addition. For any scalar \\(c \\in F\\) and vector \\(\\boldsymbol u \\in V\\), the operation of scalar multiplication creates another vector \\(c \\boldsymbol u \\in V\\); this is known as closure under scalar multiplication. The 8 axioms that govern these two operations are listed here.\nAny sets of elements equipped with vector addition and scalar multiplication that satisfy the closure property and the 8 axioms is considered a vector space. Of particular interest are \\(n\\)-tuples of the form\n\\[\n\\boldsymbol u = (u_1, u_2, \\ldots, u_n),\n\\]\nwhere the components \\(u_1, \\ldots, u_n\\) are scalars from a field \\(F\\). The set of all such \\(n\\)-tuples is denoted by \\(F^n\\).1 For example, \\(\\mathbb{R}^3\\) is the set of all 3-tuples of real numbers. Here, vector addition is defined as the component-wise operation:\n\\[\n\\begin{aligned}\n\\boldsymbol u = (u_1, u_2, &\\ldots, u_n) \\in F^n \\quad\\text{and}\\quad \\boldsymbol v = (v_1, v_2, \\ldots, v_n) \\in F^n \\\\\n\\\\\n\\boldsymbol u + \\boldsymbol v &= (u_1 + v_1, u_2 + v_2, \\ldots, u_n + v_n) \\in F^n.\n\\end{aligned}\n\\]\nSimilarly, scalar multiplication is defined as the component-wise operation:\n\\[\n\\begin{aligned}\nc \\in F \\quad&\\text{and}\\quad \\boldsymbol u = (u_1,u_2, \\ldots, u_n) \\in F^n \\\\\n\\\\\nc \\boldsymbol u &= (c u_1, c u_2, \\ldots, c u_n) \\in F^n.\n\\end{aligned}\n\\]\nA natural generalization of \\(n\\)-tuples is the \\(m \\times n\\) array called the matrix:\n\\[\n\\mathbf A = \\begin{pmatrix}\na_{11} & a_{12} & \\ldots & a_{1n} \\\\\na_{21} & a_{22} & \\ldots & a_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{m1} & a_{m2} & \\ldots & a_{mn}\n\\end{pmatrix}, \\quad a_{ij} \\in F.\n\\]\nThe set of all \\(m \\times n\\) matrices with components in a field \\(F\\) is denoted by \\(F^{m \\times n}\\). Vector addition and scalar multiplication are defined analogously to the component-wise operations for \\(n\\)-tuples. Specifically, for any two matrices \\(A, B \\in F^{m \\times n}\\), vector addition creates a third matrix \\(A + B \\in F^{m \\times n}\\) whose components are given by\n\\[\n(\\mathbf{A} + \\mathbf B)_{ij} = \\mathbf A_{ij} + \\mathbf B_{ij}.\n\\]\nFor any scalar \\(c \\in F\\), scalar multiplication creates another matrix \\(cA \\in F^{m \\times n}\\) where\n\\[\nc \\mathbf A_{ij} = c (\\mathbf A_{ij}).\n\\]"
  },
  {
    "objectID": "blog/linalg-basics.html#basis-and-dimension",
    "href": "blog/linalg-basics.html#basis-and-dimension",
    "title": "Basic Concepts in Linear Algebra",
    "section": "Basis and Dimension",
    "text": "Basis and Dimension\nA vector space \\(V\\) contains infinitely many vectors. We are interested in finding a smaller set of vectors that captures the entire structure in a much more tractable manner. For example, consider the vector space \\(\\mathbb{R}^3\\) in Figure 1. It seems like any point (i.e. vector or 3-tuple) on the blue lattice structure can be described by how far it extends along the \\(x\\), \\(y\\), and \\(z\\) axes. This section formalizes this idea.\n\n\n\n\n\n\n\n\nFigure 1: The R³ vector space.\n\n\n\n\n\nLinear Combination, Span, and Linear Independence\nLet \\(\\mathcal{A} = \\{\\boldsymbol{u}_1, \\ldots, \\boldsymbol{u}_n\\}\\) be some finite subset of vectors in \\(V\\). One way to formalize the idea of \\(\\mathcal{A}\\) capturing the entire structure of \\(V\\) is if any vector \\(\\boldsymbol{v} \\in V\\) can be expressed as a linear combination of the vectors in \\(\\mathcal{A}\\):\n\\[\n\\boldsymbol v = c_1 \\boldsymbol u_1 + c_2 \\boldsymbol u_2 + \\ldots + c_n \\boldsymbol u_n = \\sum_{i=1}^n c_i \\boldsymbol u_i \\in V,\n\\]\nwhere \\(c_1, \\ldots, c_n \\in F\\). If this is the case, then we say that the set \\(\\mathcal{A}\\) spans the vector space \\(V\\). An equivalent characterization is that the set of all linear combinations of the vectors in \\(\\mathcal{A}\\) — the span of \\(\\mathcal{A}\\) — is equal to \\(V\\).\nWe are also interested in efficiency. That is to say, we want \\(\\mathcal{A}\\) to be as small as possible while still spanning \\(V\\). We call a set of vectors linearly dependent if one of the vectors can be expressed as a linear combination of the others. More formally, the vectors \\(\\boldsymbol{u}_1, \\ldots, \\boldsymbol{u}_n\\) in \\(\\mathcal{A}\\) would be linearly dependent if\n\\[\nc_1 \\boldsymbol u_1 + c_2 \\boldsymbol u_2 + \\ldots + c_n \\boldsymbol u_n = 0,\n\\]\nand not all of the \\(c_i\\) are zero. Thus, we would like to remove any linearly dependent vectors from \\(\\mathcal{A}\\): such vectors are going to be redundant since they will be a part of the span of the other vectors in \\(\\mathcal{A}\\). Formally, we say that the vectors \\(\\boldsymbol{u}_1, \\ldots, \\boldsymbol{u}_n\\) in \\(\\mathcal{A}\\) are linearly independent if\n\\[\nc_1 \\boldsymbol u_1 + c_2 \\boldsymbol u_2 + \\ldots + c_n \\boldsymbol u_n = 0,\n\\]\nonly if \\(c_1 = c_2 = \\ldots = c_n = 0\\).\nTaken together, these two ideas — spanning and linear independence — gives us exactly what we were looking for: a minimal yet complete description of the vector space \\(V\\). More formally, we call a set of vectors a basis \\(\\mathcal{B}\\) for the vector space \\(V\\) if it is a linearly independent subset that spans \\(V\\). The vectors in a basis are called basis vectors.\n\n\nDimension\nNotice that in the above definition of the basis set \\(\\mathcal{B}\\), we did not assume that the number of basis vectors is finite. In fact, there exists vector spaces that require infinitely many basis vectors. For example, an \\(n\\)-degree polynomial is defined as\n\\[\nf(x) = a_nx^n + a_{n-1}x^{n-1} + \\ldots + a_1 x + a_0,\n\\] where \\(x\\) is a variable, \\(a_i \\in F\\), and \\(n \\geq 0\\) is an integer. The vector space of all polynomials with coefficients in a field \\(F\\), denoted \\(P(F)\\), has the infinite basis set\n\\[\n\\mathcal{B}_{P(F)} = \\{1, x, x^2, x^3, \\ldots\\}.\n\\] Nevertheless, note that the definition of linear combination requires a finite number of vectors. Thus, even if a basis set \\(\\mathcal{B}\\) for a vector space \\(V\\) is infinite, any vector \\(\\boldsymbol v \\in V\\) can be represented as a linear combination of a finite subset of vectors in \\(\\mathcal{B}\\).\nTo distinguish between vector spaces with finite and infinite basis sets, we introduce the notion of dimension, which is simply the number of basis vectors in a basis set \\(\\mathcal{B}\\) for the vector space \\(V\\). If \\(V\\) has a finite basis, then we say that \\(V\\) is finite-dimensional; otherwise, it is infinite-dimensional.\n\n\nBasis Sets Are Not Unique\nAn important property of basis sets is that they are not unique. Let us revisit the vector space \\(\\mathbb{R}^3\\) to illustrate this point. At the start of this section, I mentioned that it seems possible to describe any point in \\(\\mathbb{R}^3\\) by how far it extends along the \\(x\\), \\(y\\), and \\(z\\) axes. With this intuition, we can define the following basis set\n\\[\n\\mathcal{B}_{\\mathbb{R}^3} = \\{(1,0,0), (0,1,0), (0,0,1)\\} = \\{\\boldsymbol e_1, \\boldsymbol e_2, \\boldsymbol e_3\\}.\n\\]\nThis is known as the standard basis for \\(\\mathbb{R}^3\\). We can verify that it is a valid basis by confirming that (i) the three vectors are linearly independent and (ii) any vector \\(\\boldsymbol u = (u_1,u_2,u_3) \\in \\mathbb{R}^3\\) can be expressed as a linear combination of the basis vectors. As an example, the vectors \\(w = (2,1.5,3)\\) and \\(v=(-1.5,0.6, -1.2)\\) are plotted below.\n\n\n\n\n\n\n\n\nFigure 2: Basis vectors and their linear combinations in R³.\n\n\n\n\nHowever, any any set of three linearly independent vectors in \\(\\mathbb{R}^3\\) that span \\(\\mathbb{R}^3\\) can serve as a basis. For example, the following set is also a valid basis for \\(\\mathbb{R}^3\\):\n\\[\n\\mathcal{B}'_{\\mathbb{R}^3} = \\{(1,0,0), (1,1,0), (1,1,1)\\}.\n\\] The figure below provides visual intuition for this basis.\n\n\n\n\n\n\n\n\nFigure 3: Alternative basis vectors for R³."
  },
  {
    "objectID": "blog/linalg-basics.html#subspaces",
    "href": "blog/linalg-basics.html#subspaces",
    "title": "Basic Concepts in Linear Algebra",
    "section": "Subspaces",
    "text": "Subspaces\nIt is often useful to consider a lower-dimension vector space that still preserves the properties of the original vector space. Formally, for a vector space \\(V\\), we define the subspace \\(W\\) to be any vector space that consists of a nonempty subset of the vectors in \\(V\\) endowed with the same operations of vector addition and scalar multiplication defined on \\(V\\).\nThis definition can seem abstract at first, and so it’s valuable to walk through a concrete example. Consider the following subset of \\(\\mathbb{R}^3\\)\n\\[\nW = \\{(x,y,0): x, y \\in \\mathbb{R}\\} \\subseteq {\\mathbb{R^3}},\n\\]\nwhich is visualized as the turquoise \\(x-y\\) plane in the figure below.\n\n\n\n\n\n\n\n\nFigure 4: A 2D plane through the origin is a subspace of R³.\n\n\n\n\nThe subset \\(W\\) is a subspace of \\(\\mathbb{R}^3\\). To see this, note that for any \\(\\boldsymbol u = (u_1, u_2, 0) \\in W\\), \\(\\boldsymbol v = (v_1, v_2, 0) \\in W\\), and \\(c \\in \\mathbb{R}\\), the component-wise definitions of vector addition and scalar multiplication for \\(\\mathbb{R}^3\\) are closed in \\(W\\)\n\\[\n\\begin{aligned}\n\\boldsymbol u + \\boldsymbol v = &(u_1 + v_1, u_2 + v_2, 0) \\in W \\\\\n\\\\\nc \\boldsymbol u &= (c u_1, c u_2, 0) \\in W,\n\\end{aligned}\n\\] and thus \\(W\\) is a valid vector space.\n\nImportance of Origin\nNotice that the origin \\((0,0,0)\\) — the additive identity for \\(\\mathbb{R}^3\\) — is contained in \\(W\\). This is not a coincidence: every subspace of \\(\\mathbb{R}^3\\) must contain the origin. To see this, consider the \\(x-y\\) plane shifted up by one unit:\n\\[\nW' = \\{(x,y,1): x, y \\in \\mathbb{R}\\} \\subseteq {\\mathbb{R^3}}.\n\\]\nThis set does not include the origin, and it fails to be a subspace because the operations of vector addition and scalar multiplication are not closed in \\(W'\\). Specifically, for any \\(\\boldsymbol u = (u_1, u_2, 1) \\in W'\\), \\(\\boldsymbol v = (v_1, v_2, 1) \\in W'\\), and \\(c \\in \\{\\mathbb{R} / 1\\}\\), we have that\n\\[\n\\begin{aligned}\n\\boldsymbol u + \\boldsymbol v = &(u_1 + v_1, u_2 + v_2, 2) \\notin W' \\\\\n\\\\\nc \\boldsymbol u &= (c u_1, c u_2, c) \\notin W',\n\\end{aligned}\n\\] and so \\(W'\\) is not a valid vector space. Importantly, this is a general property not limited to \\(\\mathbb{R}^3\\): any subspace must contain the additive identity (also called the zero vector) of the parent vector space.\n\n\nBasis of Subspaces\nA general property of subspaces of a finite-dimensional vector space is that their dimension is less than the dimension of the parent vector space.2 For example, in the example above, a basis for the \\(x-y\\) plane in \\(\\mathbb{R}^3\\) is\n\\[\n\\mathcal{B}_W = \\{(1,0,0), (0,1,0)\\}.\n\\]\nThus, the dimension of \\(W\\) is 2."
  },
  {
    "objectID": "blog/linalg-basics.html#footnotes",
    "href": "blog/linalg-basics.html#footnotes",
    "title": "Basic Concepts in Linear Algebra",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThese tuples are usually called vectors in most applied settings, but to avoid confusion with the more general definition of vectors in a vector space, I refer to them as \\(n\\)-tuples.↩︎\nTo be precise, I should say “less than or equal to”, because the vector space is a subspace of itself.↩︎"
  },
  {
    "objectID": "blog/statistical-inference.html",
    "href": "blog/statistical-inference.html",
    "title": "Random Sampling and Model-Based Inference",
    "section": "",
    "text": "Observational data is inherently random: if we were to measure the same variables repeatedly, we would almost certainly get different values each time. A classical explanation for the source of this randomness is the model-based (or sampling-based) perspective. In this abstraction, we assume there exists an underlying superpopulation or data generating process (DGP): a fixed but unknown probability distribution \\(F\\) that can, in principle, generate infinitely many observations. Randomness then arises because we only observe finitely many realizations from \\(F\\) — never the entire distribution. Within this framework, we define statistical inference as the process of using the observed random data to estimate features of \\(F\\) and quantify the uncertainty in those estimates.\nNotice that the exposition above implicitly assumes that there is a single underlying distribution \\(F\\) from which all observed data are drawn. To make this mathematically precise, we need to introduce the random sampling assumption. This provides the framework to connect the observed data to the DGP, and thereby lays the foundation for statistical inference."
  },
  {
    "objectID": "blog/statistical-inference.html#what-is-statistical-inference",
    "href": "blog/statistical-inference.html#what-is-statistical-inference",
    "title": "Random Sampling and Model-Based Inference",
    "section": "",
    "text": "Observational data is inherently random: if we were to measure the same variables repeatedly, we would almost certainly get different values each time. A classical explanation for the source of this randomness is the model-based (or sampling-based) perspective. In this abstraction, we assume there exists an underlying superpopulation or data generating process (DGP): a fixed but unknown probability distribution \\(F\\) that can, in principle, generate infinitely many observations. Randomness then arises because we only observe finitely many realizations from \\(F\\) — never the entire distribution. Within this framework, we define statistical inference as the process of using the observed random data to estimate features of \\(F\\) and quantify the uncertainty in those estimates.\nNotice that the exposition above implicitly assumes that there is a single underlying distribution \\(F\\) from which all observed data are drawn. To make this mathematically precise, we need to introduce the random sampling assumption. This provides the framework to connect the observed data to the DGP, and thereby lays the foundation for statistical inference."
  },
  {
    "objectID": "blog/statistical-inference.html#the-random-sampling-framework",
    "href": "blog/statistical-inference.html#the-random-sampling-framework",
    "title": "Random Sampling and Model-Based Inference",
    "section": "The Random Sampling Framework",
    "text": "The Random Sampling Framework\nThe discussion going forward will primarily focus on cross-sectional datasets. These consist of several observations of a collection of variables for a given point in time and can be denoted as\n\\[\n\\{x_{i1}, x_{i2} \\ldots, x_{iK}\\}_{i=1}^n,\n\\]\nwhere \\(x_{ik}\\) is the value of the \\(k\\)-th variable for the \\(i\\)-th unit. A typical source of cross-sectional data in economics is through surveys like the Current Population Survey (CPS) or the American Community Survey (ACS).\n\nMathematical Formalization of the Dataset\nSince observational data is random, it is natural to we view the observed data vector \\[\n\\boldsymbol{x}_i = (x_{i1}, \\ldots, x_{iK})' \\in \\mathbb{R}^K \\quad \\text{for } i = 1, \\ldots, n,\n\\] as a realization of the random vector \\[\n\\boldsymbol X_i = (X_{i1}, \\ldots, X_{iK})' \\in \\mathbb{R}^K \\quad \\text{for } i = 1, \\ldots, n,\n\\]\nwith some associated probability distribution. For example, we can think of the CPS dataset as consisting of random vectors\n\\[\n\\boldsymbol X_i = (sex_i, age_i, educ_i, wage_i)' \\quad \\text{for } i = 1, \\ldots, n.\n\\] For some specific individual \\(i\\) in the CPS dataset, we then observe the vector of realized values1\n\\[\n\\boldsymbol x_i = (1, 25, 16, 25000)'.\n\\] Intuitively, the distinction between the random vector \\(\\boldsymbol X_i\\) and the realization \\(\\boldsymbol x_i\\) is that the former represents the \\(i\\)-th observation before viewing the data (unknown and random) and the latter represents the \\(i\\)-th observation after viewing the data (specific known value).\n\n\nRandom Sampling\nWe have now represented the dataset as a collection of random vectors \\(\\boldsymbol X_1, \\ldots, \\boldsymbol X_n\\), each with some probability distribution \\(F_1 \\ldots, F_n\\). However, the connection between the underlying DGP and the observed data remains unclear. Specifically, we need a simplifying assumption that will allow us to connect the distributions of each \\(\\boldsymbol X_i\\) to the underlying DGP in a straightforward manner. The simplest framework is that of random sampling. Here, we assume that the random vectors \\(\\boldsymbol X_1, \\ldots , \\boldsymbol X_n\\) are independent and identically distributed (iid) with some common but unknown distribution \\(F\\) on \\(\\mathbb{R}^K\\). Thus, under this assumption, the data generating process is precisely the distribution that governs each individual random vector \\(\\boldsymbol X_i\\).\n\nAlternative Assumptions\nThe random sampling assumption is one potential way to characterize the dependence structure across the observed data points. It is popular because (i) it is often reasonable when working with cross-sectional datasets2, and (ii) it is the backbone of several statistical theorems and methods.3 However, it does not necessarily have to hold. For example, we often work with data where the units are connected via some underlying factor (location, industry, etc.). In such cases, the assumption of independence across individual units is violated. An alternative approach in such cases is to instead assume mutual independence across clusters of units. Another example of a violation to the independence assumption is time-series data, where the individual unit is indexed by time. Here, consecutive observations are usually correlated and independence is instead formulated in terms of stationarity and other concepts outside the scope of this post.\n\n\n\nNotation and Example\nThe discussion so far has purely been conceptual, and so it is useful to consider a concrete example. Before doing so, however, let’s clarify some notation used in the random sampling framework. We denote the population-level random variables as \\(X_1, \\ldots, X_K\\). The data generating process \\(F\\) is the joint distribution of these random variables. For example, \\(X_1\\) could denote the generic random variable for wage in the entire US population and we are interested in making inferences about its distribution. We denote the sample-level random variables as \\(X_{i1}, \\ldots, X_{iK}\\). These represent the random variables associated with specific units in the sample. Continuing the example, \\(X_{i1}\\) denotes the random variable representing the wage of individual \\(i\\) in the sample. Finally, we denote the realized observations as \\(X_{i1}=x_{i1}, \\ldots, X_{iK}=x_{iK}\\). These are the actual values we observe. So, \\(X_{i1} = 25,000\\) means that the observed wage of individual \\(i\\) is \\(25,000\\).\nNow suppose our data generating process consists of one random variable with an exponential distribution with scale parameter \\(\\beta = 1\\): \\[\nX \\sim \\text{Exp}(1) \\quad \\text{with density } f(x) = e^{-x} \\text{ for } x \\geq 0.\n\\]\nIf we assume our dataset is a random sample of size 30 from the above DGP, then \\[\nX_{i} \\sim \\text{Exp}(1) \\, \\, \\text{ for } i = 1, \\ldots, 30 \\quad  \\text{and} \\quad  X_i \\perp X_j  \\, \\, \\text{ for } i \\neq j.\n\\]\nThe figure below plots the empirical kernel density of such a random sample (black line) and the true density of the exponential distribution (red line). In practice, we do not know the true DGP and need to guess its characteristics using the observed data. Randomness in the finite observed data makes this a non-trivial task — as illustrated by the discrepancy between the empirical and true densities in the figure below.4\n\n\nCode\n# Simulate IID sample of 30 obs from exp(1)\nset.seed(123)\nn &lt;- 30\nx &lt;- rexp(n, rate = 1)\n\n# Empirical Density \ndens &lt;- density(x)\n\n# True Exponential Density\nxs &lt;- seq(0, max(x), length.out = 200)\nys &lt;- dexp(xs, rate = 1)\n\n# Plot\nplot(dens, main = \"\", xlab = \"Observed Data\", ylab = \"Density\", xlim = c(0, 5), ylim = c(0, max(c(dens$y, ys))))\ncurve(dexp(x, rate = 1), from = min(x), add = TRUE, col = \"red\", lwd = 2)"
  },
  {
    "objectID": "blog/statistical-inference.html#point-estimation",
    "href": "blog/statistical-inference.html#point-estimation",
    "title": "Random Sampling and Model-Based Inference",
    "section": "Point Estimation",
    "text": "Point Estimation\nPoint estimation is the first step of statistical inference, and involves constructing a “good guess” for a feature of the unknown data generating process. To be more precise, this feature is called an estimand \\(\\theta\\) and is defined as a function of the data generating process \\(F\\):\n\\[\n\\theta = \\theta(F).\n\\]\nAn example of an estimand is the the population mean of a random variable \\(X\\): \\[\n\\mu = \\mathbb{E}_F[X].\n\\]\nSince the DGP is unknown, the best we can do is use the observed data to guess the value of the estimand. An estimator \\(\\hat \\theta\\) is a function of the sample that is intended to provide a guess of the estimand:\n\\[\n\\hat{\\theta} = \\hat{\\theta}(\\boldsymbol X_1, \\ldots, \\boldsymbol X_n).\n\\]\nWhen the estimator is evaluated at a specific realization of the sample, we obtain an estimate \\(\\hat\\theta(\\boldsymbol x_1, \\ldots, \\boldsymbol x_n)\\) of the estimand. It’s worth emphasizing that the estimand is a fixed but unknown number, the estimator is a random variable, and the estimate is a fixed and known number.\nIn statistics, there are several estimation principles that provide systematic ways (i.e. rules) to construct estimators. One common method is the analog principle (or plug-in principle). The idea is to construct the estimator by replacing the population quantities in the estimand with their sample analogs.5 Thus, the analog estimator for the population mean is the sample mean, defined as\n\\[\n\\hat{\\mu} = \\frac{1}{n} \\sum_{i=1}^n \\boldsymbol X_i.\n\\]\n\nEstimator Properties\nHow do we know if an estimator is any good? To answer this question, statisticians study desirable properties that an estimator should ideally satisfy. A full treatment of estimator properties is typically the focus of a mathematical statistics course, but it is still valuable to briefly highlight some fundamental properties here.\nThe error of an estimator is defined as the difference between the estimate and the estimand:\n\\[\ne(\\boldsymbol x_1, \\ldots, \\boldsymbol x_n) = \\hat\\theta(\\boldsymbol x_1, \\ldots, \\boldsymbol x_n) - \\theta.\n\\] The bias of an estimator is the average error of the estimator across all possible samples of size \\(n\\) from the DGP:\n\\[\nB(\\hat\\theta) = \\mathbb{E}_F[\\hat{\\theta}] - \\theta\n\\] Intuitively, the bias captures the systematic error of the estimator: if the bias is positive, the estimator tends to overestimate the estimand, and if the bias is negative, the estimator tends to underestimate the estimand. We say an estimator \\(\\hat{\\theta}\\) is unbiased for \\(\\theta\\) if its bias is zero:6\n\\[\n\\mathbb{E}_F[\\hat{\\theta}] - \\theta = 0.\n\\] Thus, the errors of an unbiased estimator are purely due to randomness in the data. As it turns out, the sample mean is an unbiased estimator of the population mean under the random sampling assumption:\n\\[\n\\mathbb{E}_F[\\hat{\\mu}] = \\frac{1}{n} \\sum_{i=1}^n \\mathbb{E}_F[\\boldsymbol X_i] = \\frac{1}{n} \\sum_{i=1}^n \\mu = \\mu.\n\\] The first equality follows from the linearity of expectations, the second equality follows from the random sampling assumption, and the third equality is a simplification.\nWhile bias quantifies how far the estimator’s average is from the estimand, the variance (or sampling variance) measures how much the estimator varies across repeated samples of size \\(n\\):\n\\[\nVar(\\hat\\theta) = \\mathbb{E}_F[(\\hat\\theta - \\mathbb{E}_F[\\hat\\theta])^2].\n\\] The variance of the sample mean under the random sampling assumption is given by\n\\[\n\\operatorname {Var} \\left[\\hat\\mu\\right] = \\frac{1}{n^2}\\operatorname{Var} \\left[ \\sum_{i=1}^n \\boldsymbol X_i\\right] = \\frac{1}{n^2} \\sum_{i=1}^n \\operatorname{Var}[\\boldsymbol X_i] = \\frac{1}{n^2} \\sum_{i=1}^n \\sigma^2 = \\frac{\\sigma^2}{n},\n\\] where \\(\\sigma^2\\) is the population variance \\(\\operatorname{Var}[X]\\). The first equality uses the properties of variance. The second equality follows from the fact that the independence of each \\(\\boldsymbol X_i\\) means they are uncorrelated, and so the variance of their sum equals the sum of their variance. The third equality uses the fact that each \\(\\boldsymbol X_i\\) are drawn from an identical distribution and so have the same variance \\(\\sigma^2\\). The fourth equality is an algebraic simplification."
  },
  {
    "objectID": "blog/statistical-inference.html#the-necessity-for-statistical-models",
    "href": "blog/statistical-inference.html#the-necessity-for-statistical-models",
    "title": "Random Sampling and Model-Based Inference",
    "section": "The Necessity for Statistical Models",
    "text": "The Necessity for Statistical Models\nThe sampling distribution of an estimator is the probability distribution that describes how the estimator’s estimates vary across all possible samples of size \\(n\\) drawn from the DGP. Intuitively, it characterizes the behavior of the estimator under repeated sampling. Under the random sampling assumption, the sampling distribution is completely determined by the DGP \\(F\\), the sample size \\(n\\), and the functional form of the estimator \\(\\hat\\theta\\).\nLet’s revisit the example of the sample mean estimator \\(\\hat\\mu\\) for the population mean \\(\\mu\\). We have already established some features of the sampling distribution of \\(\\hat\\mu\\) despite knowing nothing about \\(F\\). Particularly, the mean of \\(\\hat\\mu\\) is \\(\\mu\\) and its variance is \\(\\sigma^2/n\\). However, to say more about the distribution of \\(\\hat\\mu\\) — like its shape — we need to make assumptions about the DGP.\nA statistical model is a set of assumptions about the general structure of the data generating process \\(F\\). Put differently, we can think of a statistical model as a family of possible distributions that \\(F\\) could belong to. To illustrate the added value of statistical models, suppose our sample \\(\\boldsymbol X_1, \\ldots, \\boldsymbol X_n\\) is drawn iid from \\(\\mathcal{N}(\\mu, \\sigma^2)\\). Since\n\\[\n\\hat\\mu = \\frac{1}{n} \\sum_{i=1}^n \\boldsymbol X_i,\n\\]\nis a linear combination of normally distributed random variables, it is also normally distributed. Moreover, we have previously established that \\(\\mathbb{E}_F[\\hat\\mu] = \\mu\\) and \\(\\operatorname{Var}[\\hat\\mu] = \\sigma^2/n\\) for any DGP \\(F\\). Thus, assumption of a normal DGP allows us to completely characterize the sampling distribution of \\(\\hat\\mu\\) as \\(\\mathcal{N}(\\mu, \\sigma^2/n)\\). This is powerful because we can use this sampling distribution to quantify the uncertainty in our estimates by constructing confidence intervals and conducting hypothesis tests.\n\nConstructing Confidence Intervals for \\(\\hat\\mu\\)"
  },
  {
    "objectID": "blog/statistical-inference.html#conclusion",
    "href": "blog/statistical-inference.html#conclusion",
    "title": "Random Sampling and Model-Based Inference",
    "section": "Conclusion",
    "text": "Conclusion\n\n\n\n\n\nflowchart LR\n  classDef box fill:#f8f9fa,stroke:#444,stroke-width:1px,rx:10,ry:10;\n\n  DGP[\"Data Generating Process\"]:::box\n  Data[\"Observed Data\"]:::box\n  Model[\"Statistical Model\"]:::box\n\n  %% Main flows\n  DGP -- \"Random Sampling\" --&gt; Data\n  DGP -. \"Assumptions\" .-&gt; Model\n  Model -- \"Probability\" --&gt; Data\n  Data -- \"Inference\" --&gt; Model\n  Model -. \"Approximate Reality\" .-&gt; DGP\n\n\n\n\n\n\nIt is important to note that statistical inference is valid only if the assumptions of statistical model hold."
  },
  {
    "objectID": "blog/statistical-inference.html#footnotes",
    "href": "blog/statistical-inference.html#footnotes",
    "title": "Random Sampling and Model-Based Inference",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nHere, the value of \\(1\\) refers to the individual being a male and \\(16\\) means \\(16\\) years of education.↩︎\nFor example, if we collected data on a random subset of individuals from a large common population (e.g. the USA), it is reasonable to assume that the characteristics of one individual are independent of another individual and that they all come from the same population distribution.↩︎\nCrucial theorems in asymptotic statistical theory, like the Law of Large Numbers and the Central Limit Theorem, require the random sampling assumption to hold.↩︎\nTechnically, under iid sampling, the empirical distribution converges to the true distribution as the sample size tends to infinity — a result known as the Glivenko–Cantelli theorem. Nevertheless, in any finite sample, uncertainty remains, and with it the need for statistical inference.↩︎\nTo quote my Mathematical Statistics Professor Daniel Weiner: “Do to the sample to get your estimator, as you would do to your population to get your estimand.”↩︎\nTo be more precise, \\(\\hat\\theta\\) is unbiased for \\(\\theta\\) if \\(\\mathbb{E}[\\hat\\theta]=\\theta\\) for all \\(F \\in \\mathcal{F}\\), where \\(\\mathcal{F}\\) is a class of distributions.↩︎"
  },
  {
    "objectID": "blog/linalg-basics.html#importance-of-origin",
    "href": "blog/linalg-basics.html#importance-of-origin",
    "title": "Basic Concepts in Linear Algebra",
    "section": "Importance of Origin",
    "text": "Importance of Origin\nNotice that the origin \\((0,0,0)\\) — the additive identity for \\(\\mathbb{R}^3\\) — is contained in \\(W\\). This is not a coincidence: every subspace of \\(\\mathbb{R}^3\\) must contain the origin. To see this, consider the \\(x-y\\) plane shifted up by one unit:\n\\[\nW' = \\{(x,y,1): x, y \\in \\mathbb{R}\\} \\subseteq {\\mathbb{R^3}}.\n\\]\nThis set does not include the origin, and it fails to be a subspace because the operations of vector addition and scalar multiplication are not closed in \\(W'\\). Specifically, for any \\(u = (u_1, u_2, 1) \\in W'\\), \\(v = (v_1, v_2, 1) \\in W'\\), and \\(c \\in \\{\\mathbb{R} / 1\\}\\), we have that\n\\[\n\\begin{aligned}\nu + v = &(u_1 + v_1, u_2 + v_2, 2) \\notin W' \\\\\n\\\\\ncu &= (c u_1, c u_2, c) \\notin W',\n\\end{aligned}\n\\] and so \\(W'\\) is not a valid vector space. Importantly, this is a general property not limited to \\(\\mathbb{R}^3\\): any subspace must contain the additive identity (also called the zero vector) of the parent vector space.\n\nBasis of Subspaces\nA general property of subspaces of a finite-dimensional vector space is that their dimension is less than to the dimension of the parent vector space.2 For example, in the example above, a basis for the \\(x-y\\) plane in \\(\\mathbb{R}^3\\) is\n\\[\n\\mathcal{B}_W = \\{(1,0,0), (0,1,0)\\}.\n\\]\nThus, the dimension of \\(W\\) is 2."
  }
]