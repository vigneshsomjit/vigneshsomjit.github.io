[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "\nVignesh Somjit\n",
    "section": "",
    "text": "Vignesh Somjit\n\n\nI am a Research Professional at the University of Chicago Booth School of Business, where I work with Professor Richard Hornbeck. Before joining Booth, I received my B.A. in Economics (with honors) and B.A. in Mathematics from Boston University in 2025.\nI maintain a research blog, where I write about topics in applied econometrics and causal inference that I find myself thinking about or using."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "Vignesh Somjit",
    "section": "About Me",
    "text": "About Me\nI am a Research Professional at the University of Chicago Booth School of Business, where I work with Professor Richard Hornbeck. I received my B.A. in Economics and B.A. in Mathematics from Boston University in 2025."
  },
  {
    "objectID": "blog/blog-index.html",
    "href": "blog/blog-index.html",
    "title": "Vignesh Somjit",
    "section": "",
    "text": "The Linear Regression Model and its OLS Estimation\n\n\n\n\n\n\nLinear Regression\n\n\n\nA detailed discussion of the linear regression model and the ordinary least squares (OLS) estimation method for its parameters.\n\n\n\n\n\nAug 31, 2025\n\n\n\n\n\n\n\nHow to Think About Statistical Inference\n\n\n\n\n\n\nEconometrics/Statistics\n\n\n\nBuilding a mental framework for how data, models, inference, and probability come together.\n\n\n\n\n\nAug 31, 2025\n\n\n\n\n\n\n\nLinear Regression as an Approximation Method\n\n\n\n\n\n\nEconometrics/Statistics\n\n\n\nIntuition for the perspective that linear regression is the process of approximating the conditional expectation function (CEF).\n\n\n\n\n\nAug 12, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/regressions.html",
    "href": "blog/regressions.html",
    "title": "An Applied Econometrics Rationale for Linear Regression",
    "section": "",
    "text": "Regression Approximates Conditional Averages\nWe are interested in predicting the value of a random variable \\(Y_i\\), called the outcome, given some realized value of a random vector \\(X_i\\) of covariates. As a starting point, we let our predictions be a function of \\(X_i\\) and want our predictions to minimize the mean squared error (MSE) objective function\n\\[\n\\mathbb{E} [Y_i - f(X_i)]^2.\n\\] With the law of iterated expectation and some calculus, we can show that for every possible realized value \\(X_i = x\\), the optimal function that solves the minimum mean squared error problem is the conditional expectation function (CEF)\n\\[\n\\mu(x) = \\mathbb{E}[Y_i | X_i = x].\n\\]\nIn most empirical cases, however, the CEF of interest has no analytic form. Thus, it is more practical to model the relationship between the outcome and covariates using a simpler function. One option is to find a linear approximation of the CEF, which takes the form\n\\[\n\\ell(x) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_k x_k = x^T\\beta.\n\\]\nNote that the word “linear” here refers to the simplifying assumption that the function is a linear combination of the covariates. Since we want our predictions to minimize the MSE function, the best linear predictor is found by setting the coefficients as\n\\[\n\\beta^\\star = \\underset{\\beta}{\\arg\\min} \\, \\mathbb{E} [Y_i - (X_i^T \\beta)]^2 = \\mathbb{E}[X_i X_i^T]^{-1}\\mathbb{E}[X_iY_i].\n\\] Thus, the linear regression model that describes the relationship between \\(Y_i\\) and \\(X_i\\) is given by \\[\nY_i = \\underbrace{X_i^T \\beta^\\star}_{\\text{best linear predictor}} + \\underbrace{\\epsilon_i}_{\\text{residual}},\n\\] where \\(\\epsilon_i\\) captures the difference between the realized outcome and the best linear predictor.\nTo summarize, the conditional expectation function is the optimal predictor of an outcome given covariates, in the sense that it minimizes the mean squared prediction error. Since the CEF is typically a complex function, linear regression approximates it by selecting the linear combination of covariates that minimizes the mean squared error. The resulting linear predictor provides a simple model of the relationship between the outcome and the covariates.\n\n\nApproximate and Exact Conditional Averages\n\nset.seed(123)\n\n# Sample size \nn &lt;- 1000 \n\n# Covariates\nsex &lt;- rbinom(n, 1, 0.5)\nrace &lt;- rbinom(n, 1, 0.3)                   \n\n# Non-linear data generating process; no interactions\noutcome1 &lt;- 5 + 2^sex + 3^race \n\n# Non-linear data generating process; interactions \noutcome2 &lt;- 5 + 2^sex + 3^race + 1.5^(sex*race)\n\n# Consolidate in a dataset \ndf &lt;- data.frame(\n  outcome1 = outcome1,\n  outcome2 = outcome2,\n  sex = sex,\n  race = race\n)"
  },
  {
    "objectID": "blog/linear-reg-rationale.html",
    "href": "blog/linear-reg-rationale.html",
    "title": "Linear Regression as an Approximation Method",
    "section": "",
    "text": "For observations \\(i = 1, \\ldots, n\\), suppose there is a data generating process (DGP) that describes the joint distribution of a random outcome variable \\(Y_i\\) and a random vector \\(X_i =(X_{1i}, X_{2i}, \\ldots, X_{ki})\\) of covariates.1 We are interested in predicting \\(Y_i\\) given some realized values of the covariates, denoted by \\(x_i = (x_{1i}, x_{2i}, \\ldots, x_{ki})\\). Thus, any prediction can generally be expressed as a function \\(f(X_i)\\) of the covariates. As a starting point, we consider choosing \\(f\\) to minimize the mean squared error (MSE) between our predictions and the observed outcomes:\n\\[\n\\mathbb{E} [Y_i - f(X_i)]^2.\n\\] With the law of iterated expectation and some calculus, we can show that for every possible realized value \\(X_i = x_i\\), the optimal function that solves the minimum mean squared error problem is the conditional expectation function (CEF)\n\\[\nm(x_i) = \\mathbb{E}[Y_i | X_i = x_i].\n\\tag{1}\\]"
  },
  {
    "objectID": "blog/linear-reg-rationale.html#regression-approximates-conditional-averages",
    "href": "blog/linear-reg-rationale.html#regression-approximates-conditional-averages",
    "title": "Motivating Linear Regression as a Tool",
    "section": "",
    "text": "For observations \\(i = 1, \\ldots, n\\), suppose there is a data generating process (DGP) that describes the joint distribution of a random outcome variable \\(Y_i\\) and a random vector \\(X_i =(X_{1i}, X_{2i}, \\ldots, X_{ki})\\) of covariates.1 We are interested in predicting \\(Y_i\\) given some realized values of the covariates, denoted by \\(x_i = (x_{1i}, x_{2i}, \\ldots, x_{ki})\\). Thus, any prediction can generally be expressed as a function \\(f(X_i)\\) of the covariates. As a starting point, we consider choosing \\(f\\) to minimize the mean squared error (MSE) between our predictions and the observed outcomes:\n\\[\n\\mathbb{E} [Y_i - f(X_i)]^2.\n\\] With the law of iterated expectation and some calculus, we can show that for every possible realized value \\(X_i = x_i\\), the optimal function that solves the minimum mean squared error problem is the conditional expectation function (CEF)\n\\[\nm(x) = \\mathbb{E}[Y_i | X_i = x_i].\n\\]\nIn most empirical cases, however, the functional form of the CEF is unknown. Thus, it is more practical to model the relationship between the outcome and covariates using a simpler function. One option is to find a linear approximation of the CEF, which takes the form\n\\[\n\\ell(x_i) = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + \\ldots + \\beta_k x_{ki} = x_i^T\\beta,\n\\] where \\(x_i = (1, x_{1i}, x_{2i}, \\ldots, x_{ki})\\). Note that the word “linear” here refers to the simplifying assumption that the function is a linear combination of the covariates. Since we want our predictions to minimize the MSE function, the best linear predictor is found by setting the coefficients as\n\\[\n\\beta^\\star = \\underset{\\beta}{\\arg\\min} \\, \\mathbb{E} [Y_i - (X_i^T \\beta)]^2 = \\mathbb{E}[X_i X_i^T]^{-1}\\mathbb{E}[X_iY_i].\n\\]\nThus, the linear regression model that describes the relationship between \\(Y_i\\) and \\(X_i\\) is given by \\[\nY_i = \\underbrace{X_i^T \\beta^\\star}_{\\text{best linear predictor}} + \\underbrace{e_i}_{\\text{projection error}},\n\\] where \\(e_i\\) captures the difference between the realized outcome and the best linear predictor.\nTo summarize, the conditional expectation function is the optimal predictor of an outcome given covariates, in the sense that it minimizes the mean squared prediction error. Since the CEF is typically a complex function, linear regression approximates it by selecting the linear combination of covariates that minimizes the mean squared error. The resulting linear predictor provides a simple model of the relationship between the outcome and the covariates."
  },
  {
    "objectID": "blog/linear-reg-rationale.html#footnotes",
    "href": "blog/linear-reg-rationale.html#footnotes",
    "title": "Linear Regression as an Approximation Method",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIn this post, I assume that the \\(n\\) observed data points represents the entire (finite) population. This simplification allows me to focus on the conceptual ideas behind linear regression without having to discuss statistical inference.↩︎\nAgain, to abstract away from statistical inference, I do not distinguish between population and sample averages. In practice, however, it is important to note that linear regression recovers sample conditional averages rather than the “true” population conditional averages when the CEF is linear.↩︎"
  },
  {
    "objectID": "blog/linear-reg-rationale.html#regression-is-about-conditional-averages",
    "href": "blog/linear-reg-rationale.html#regression-is-about-conditional-averages",
    "title": "Motivating Linear Regression",
    "section": "",
    "text": "For observations \\(i = 1, \\ldots, n\\), suppose there is a data generating process (DGP) that describes the joint distribution of a random outcome variable \\(Y_i\\) and a random vector \\(X_i =(X_{1i}, X_{2i}, \\ldots, X_{ki})\\) of covariates.1 We are interested in predicting \\(Y_i\\) given some realized values of the covariates, denoted by \\(x_i = (x_{1i}, x_{2i}, \\ldots, x_{ki})\\). Thus, any prediction can generally be expressed as a function \\(f(X_i)\\) of the covariates. As a starting point, we consider choosing \\(f\\) to minimize the mean squared error (MSE) between our predictions and the observed outcomes:\n\\[\n\\mathbb{E} [Y_i - f(X_i)]^2.\n\\] With the law of iterated expectation and some calculus, we can show that for every possible realized value \\(X_i = x_i\\), the optimal function that solves the minimum mean squared error problem is the conditional expectation function (CEF)\n\\[\nm(x) = \\mathbb{E}[Y_i | X_i = x_i].\n\\]\nIn most empirical cases, however, the functional form of the CEF is unknown. Thus, it is more practical to model the relationship between the outcome and covariates using a simpler function. One option is to find a linear approximation of the CEF, which takes the form\n\\[\n\\ell(x_i) = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + \\ldots + \\beta_k x_{ki} = x_i^T\\beta,\n\\] where \\(x_i = (1, x_{1i}, x_{2i}, \\ldots, x_{ki})\\). Note that the word “linear” here refers to the simplifying assumption that the function is a linear combination of the covariates. Since we want our predictions to minimize the MSE function, the best linear predictor is found by setting the coefficients as\n\\[\n\\beta^\\star = \\underset{\\beta}{\\arg\\min} \\, \\mathbb{E} [Y_i - (X_i^T \\beta)]^2 = \\mathbb{E}[X_i X_i^T]^{-1}\\mathbb{E}[X_iY_i].\n\\]\nThus, the linear regression model that describes the relationship between \\(Y_i\\) and \\(X_i\\) is given by \\[\nY_i = \\underbrace{X_i^T \\beta^\\star}_{\\text{best linear predictor}} + \\underbrace{e_i}_{\\text{projection error}},\n\\] where \\(e_i\\) captures the difference between the realized outcome and the best linear predictor.\nTo summarize, the conditional expectation function is the optimal predictor of an outcome given covariates, in the sense that it minimizes the mean squared prediction error. Since the CEF is typically a complex function, linear regression approximates it by selecting the linear combination of covariates that minimizes the mean squared error. The resulting linear predictor provides a simple model of the relationship between the outcome and the covariates."
  },
  {
    "objectID": "blog/linear-reg-rationale.html#the-conditional-expectation-function-cef-is-an-optimal-predictor",
    "href": "blog/linear-reg-rationale.html#the-conditional-expectation-function-cef-is-an-optimal-predictor",
    "title": "Linear Regression as an Approximation Method",
    "section": "",
    "text": "For observations \\(i = 1, \\ldots, n\\), suppose there is a data generating process (DGP) that describes the joint distribution of a random outcome variable \\(Y_i\\) and a random vector \\(X_i =(X_{1i}, X_{2i}, \\ldots, X_{ki})\\) of covariates.1 We are interested in predicting \\(Y_i\\) given some realized values of the covariates, denoted by \\(x_i = (x_{1i}, x_{2i}, \\ldots, x_{ki})\\). Thus, any prediction can generally be expressed as a function \\(f(X_i)\\) of the covariates. As a starting point, we consider choosing \\(f\\) to minimize the mean squared error (MSE) between our predictions and the observed outcomes:\n\\[\n\\mathbb{E} [Y_i - f(X_i)]^2.\n\\] With the law of iterated expectation and some calculus, we can show that for every possible realized value \\(X_i = x_i\\), the optimal function that solves the minimum mean squared error problem is the conditional expectation function (CEF)\n\\[\nm(x_i) = \\mathbb{E}[Y_i | X_i = x_i].\n\\tag{1}\\]"
  },
  {
    "objectID": "blog/linear-reg-rationale.html#regression-finds-the-best-linear-approximation-to-the-cef",
    "href": "blog/linear-reg-rationale.html#regression-finds-the-best-linear-approximation-to-the-cef",
    "title": "Motivating Linear Regression",
    "section": "Regression Finds the Best Linear Approximation to the CEF",
    "text": "Regression Finds the Best Linear Approximation to the CEF\nIn most empirical cases, however, the functional form of the CEF is unknown. Thus, it is more practical to model the relationship between the outcome and covariates using a simpler function. One option is to find a linear approximation of the CEF, which takes the form\n\\[\n\\ell(x_i) = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + \\ldots + \\beta_k x_{ki} = x_i^T\\beta,\n\\] where \\(x_i = (1, x_{1i}, x_{2i}, \\ldots, x_{ki})\\). Note that the word “linear” here refers to the simplifying assumption that the function is a linear combination of the covariates. Since we want our predictions to minimize the MSE function, the best linear predictor is found by setting the coefficients as\n\\[\n\\beta^\\star = \\underset{\\beta}{\\arg\\min} \\, \\mathbb{E} [Y_i - (X_i^T \\beta)]^2 = \\mathbb{E}[X_i X_i^T]^{-1}\\mathbb{E}[X_iY_i].\n\\]\nThus, the linear regression model that describes the relationship between \\(Y_i\\) and \\(X_i\\) is given by \\[\nY_i = \\underbrace{X_i^T \\beta^\\star}_{\\text{best linear predictor}} + \\underbrace{e_i}_{\\text{projection error}},\n\\] where \\(e_i\\) captures the difference between the realized outcome and the best linear predictor.\nTo summarize, the conditional expectation function is the optimal predictor of an outcome given covariates, in the sense that it minimizes the mean squared prediction error. Since the CEF is typically a complex function, linear regression approximates it by selecting the linear combination of covariates that minimizes the mean squared error. The resulting linear predictor provides a simple model of the relationship between the outcome and the covariates."
  },
  {
    "objectID": "blog/linear-reg-rationale.html#linear-regression-finds-the-best-linear-approximation-to-the-cef",
    "href": "blog/linear-reg-rationale.html#linear-regression-finds-the-best-linear-approximation-to-the-cef",
    "title": "Linear Regression as an Approximation Method",
    "section": "Linear Regression Finds the Best Linear Approximation to the CEF",
    "text": "Linear Regression Finds the Best Linear Approximation to the CEF\nIn most empirical cases, however, the functional form of the CEF is unknown. Thus, it is more practical to model the relationship between the outcome and covariates using a simpler function. One option is to find a linear approximation of the CEF, which takes the form\n\\[\n\\ell(x_i) = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + \\ldots + \\beta_k x_{ki} = x_i^T\\beta,\n\\tag{2}\\] where \\(x_i^T = (1, x_{1i}, x_{2i}, \\ldots, x_{ki})\\). Note that the word “linear” here refers to the simplifying assumption that the function is a linear combination of the covariates.\nHow do we choose the specific form of our approximating linear function? Since we want our predictions to minimize the MSE function, we set the parameters in Equation 2 as\n\\[\n\\beta^\\star = \\underset{\\beta}{\\arg\\min} \\, \\mathbb{E} [Y_i - (X_i^T \\beta)]^2.\n\\]\nIn words, we are choosing the linear function with the lowest MSE among all possible linear functions. The resulting linear predictor\n\\[x_i^T \\beta^\\star \\approx \\mathbb{E}[Y_i|X_i =x_i]\\]\nis often called the “linear regression of \\(Y\\) on \\(X\\)” by economists. This simply refers to the process of finding the best linear predictor of \\(Y_i\\) given some realized values of \\(X_i\\) by minimizing the mean squared error function."
  },
  {
    "objectID": "blog/linear-reg-rationale.html#linear-regression-can-sometimes-recover-true-conditional-averages",
    "href": "blog/linear-reg-rationale.html#linear-regression-can-sometimes-recover-true-conditional-averages",
    "title": "Linear Regression as an Approximation Method",
    "section": "Linear Regression Can (Sometimes) Recover True Conditional Averages",
    "text": "Linear Regression Can (Sometimes) Recover True Conditional Averages\nSo far, we have discussed linear regression as a tool to approximate the conditional expectation function. However, it can be shown that when the CEF is itself linear, the best linear predictor exactly equals the CEF. That is to say linear regression recovers the true conditional averages in this case.2\nTo see this in practice, I simulate data on income using the following equation\n\\[\n\\begin{align}\nincome_i &= \\beta_0 + \\beta_1 \\times white_i + \\beta_2 \\times male_i \\\\ & \\quad \\quad + \\beta_3 \\times literacy_i^{(1+0.3 \\times sex_i)} + \\beta_4 (male_i \\times black_i) + \\varepsilon_i.\n\\end{align}\n\\]\n\n\nShow the code that simulates the dataset.\nset.seed(123)\n\n# Population size \nn &lt;- 1000\n\n# Noise \nnoise &lt;- rnorm(n, 0, 2000)\n\n# Covariates \nmale &lt;- rbinom(n, 1, 0.5)\nsex &lt;- factor(male, labels = c(\"Female\", \"Male\"))\nwhite &lt;- rbinom(n, 1, 0.7)\nrace &lt;- factor(white, labels = c(\"Black\", \"White\"))\nliteracy &lt;- runif(n, 0, 100)\n\n# Outcome \nwhite_female_base &lt;- 30000                           # white females with 0 literacy earn 30000\nliteracy_effect &lt;- 50\nmale_premium &lt;- 200                                    # + 200 male premium \nwhite_premium &lt;- 1500                                  # + 1500 white premium\nwhite_male_premium &lt;- 1000                              # + 1000 white male premium \n\n# Create income \nincome &lt;- \n  white_female_base +\n  white_premium       * white +                          \n  male_premium        * male +                            \n  literacy_effect   * (literacy^(1 + 0.3 * male)) +    # literate males are better off than literate females \n  white_male_premium   * (male * white) +                  \n  noise\n\n# Combine into a dataset\nsim_data &lt;- data.frame(sex = sex,\n                     race = race,\n                     literacy,\n                     income)\n\n\n\nA Simple Univariate Example\nRecall from Equation 1 that the CEF is a function of the covariates we use to predict the outcome variable. So, as a simple example of a linear CEF, let us consider the conditional expectation of \\(income_i\\) as a function of \\(male_i\\). The conditional expectation can be written as the step-wise function \\[\n\\mathbb{E}[Y_i|male_i] = \\begin{cases} \\mu_0 \\quad  \\text{if} \\quad male_i = 0 \\\\ \\mu_1 \\quad  \\text{if} \\quad male_i = 1 \\end{cases}.\n\\tag{3}\\] To illustrate that Equation 3 is a linear combination of \\(male_i\\), we can rewrite it as\n\\[\n\\mathbb{E}[Y_i |male_i] = \\mu_0 + (\\mu_1-\\mu_0) \\times male_i.\n\\tag{4}\\]\nNow, we can use R to regress \\(income_i\\) on \\(sex_i\\) as follows.\n\nreg_model &lt;- feols(\n  income ~ sex, \n  data = sim_data\n)\n\nsummary(reg_model)\n\nOLS estimation, Dep. Var.: income\nObservations: 1,000\nStandard-errors: IID \n            Estimate Std. Error  t value  Pr(&gt;|t|)    \n(Intercept) 33423.97    207.948 160.7323 &lt; 2.2e-16 ***\nsexMale      7044.47    295.863  23.8099 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 4,673.0   Adj. R2: 0.361625\n\n\nTo verify that the linear regression above does in fact recover Equation 4, we need to check that (i) its intercept parameter equals the average income of females in the dataset and (ii) the coefficient on \\(male_i\\) is the difference in the average income of males and females in the dataset. As shown below, this is indeed the case.\n\nsim_data %&gt;%\n  filter(sex == \"Female\") %&gt;%\n  summarise(female_avg = mean(income, na.rm = TRUE))\n\n  female_avg\n1   33423.97\n\nsim_data %&gt;%\n  summarise(\n    sex_diff = mean(income[sex == \"Male\"], na.rm = TRUE) -\n                       mean(income[sex == \"Female\"], na.rm = TRUE)\n  )\n\n  sex_diff\n1 7044.472\n\n\n\n\nSaturated Regressions\nThe exercise of transforming the step-wise function in Equation 3 into a linear combination of the covariate in Equation 4 provides an important insight. It turns out that we can perform a similar transformation for any CEF where the covariates are discrete variables that take on a finite set of values.\nTODO: Add example of sex x race. Talk about main effect and interaction.\nIn general, the idea is to include a separate parameter for each possible value of the discrete covariates. The linear regression corresponding to such a CEF is said to be saturated.\nTODO: Saturated models makes inference easier: give example of comparing means."
  },
  {
    "objectID": "blog/statistical-modeling.html",
    "href": "blog/statistical-modeling.html",
    "title": "How to Think About Statistical Inference",
    "section": "",
    "text": "We can think of our observed data to be the result of some true, unknown Data Generating Process (DGP) that is inherently random.1 That is to say if we repeated the data collection, we would almost certainly get different values. The source of this randomness is varied, and includes (i) natural variation in the phenomenon being measured, (ii) measurement error, and (iii) sampling error.\nThe presence of such randomness makes it challenging to learn about the unobserved characteristics of the DGP using the observed data. Statistical inference is the process of (i) using random observed data to make informed predictions about unobserved features of the DGP and (ii) using probability to quantify the uncertainty in those predictions. 2"
  },
  {
    "objectID": "blog/statistical-modeling.html#an-abstract-view-of-statistical-inference-with-a-focus-on-parametric-models",
    "href": "blog/statistical-modeling.html#an-abstract-view-of-statistical-inference-with-a-focus-on-parametric-models",
    "title": "How to Think About Statistical Models and Inference",
    "section": "",
    "text": "flowchart LR\n  classDef box fill:#f8f9fa,stroke:#444,stroke-width:1px,rx:10,ry:10;\n\n  M[\"Statistical Model\"]:::box\n  D[\"Observed Data\"]:::box\n\n  M -- \"Probability\" --&gt; D\n  D -- \"Inference\" --&gt; M\n\n  linkStyle 0 stroke-width:2px;\n  linkStyle 1 stroke-width:2px;"
  },
  {
    "objectID": "blog/statistical-modeling.html#an-framework-for-statistical-inference-with-a-focus-on-parametric-models",
    "href": "blog/statistical-modeling.html#an-framework-for-statistical-inference-with-a-focus-on-parametric-models",
    "title": "How to Think About Statistical Models and Inference",
    "section": "",
    "text": "flowchart LR\n  classDef box fill:#f8f9fa,stroke:#444,stroke-width:1px,rx:10,ry:10;\n\n  M[\"Statistical Model\"]:::box\n  D[\"Observed Data\"]:::box\n\n  M -- \"Probability\" --&gt; D\n  D -- \"Inference\" --&gt; M\n\n  linkStyle 0 stroke-width:2px;\n  linkStyle 1 stroke-width:2px;"
  },
  {
    "objectID": "blog/statistical-modeling.html#statistical-models-provide-structure-to-randomness-inference-quantifies-uncertainty",
    "href": "blog/statistical-modeling.html#statistical-models-provide-structure-to-randomness-inference-quantifies-uncertainty",
    "title": "How to Think About Statistical Models and Inference",
    "section": "",
    "text": "flowchart LR\n  classDef box fill:#f8f9fa,stroke:#444,stroke-width:1px,rx:10,ry:10;\n\n  M[\"Statistical Model\"]:::box\n  D[\"Observed Data\"]:::box\n\n  M -- \"Probability\" --&gt; D\n  D -- \"Inference\" --&gt; M\n\n  linkStyle 0 stroke-width:2px;\n  linkStyle 1 stroke-width:2px;"
  },
  {
    "objectID": "blog/statistical-modeling.html#modeling-randomness-quantifying-uncertainty",
    "href": "blog/statistical-modeling.html#modeling-randomness-quantifying-uncertainty",
    "title": "How to Think About Statistical Models and Inference",
    "section": "",
    "text": "flowchart LR\n  classDef box fill:#f8f9fa,stroke:#444,stroke-width:1px,rx:10,ry:10;\n\n  M[\"Statistical Model\"]:::box\n  D[\"Observed Data\"]:::box\n\n  M -- \"Probability\" --&gt; D\n  D -- \"Inference\" --&gt; M\n\n  linkStyle 0 stroke-width:2px;\n  linkStyle 1 stroke-width:2px;"
  },
  {
    "objectID": "blog/statistical-modeling.html#modeling-randomness-and-quantifying-uncertainty",
    "href": "blog/statistical-modeling.html#modeling-randomness-and-quantifying-uncertainty",
    "title": "How to Think About Statistical Models and Inference",
    "section": "",
    "text": "flowchart LR\n  classDef box fill:#f8f9fa,stroke:#444,stroke-width:1px,rx:10,ry:10;\n\n  DGP[\"Data Generating Process\"]:::box\n  Data[\"Observed Data\"]:::box\n  Model[\"Statistical Model\"]:::box\n\n  %% Main flows\n  DGP -- \"Randomness\" --&gt; Data\n  DGP -. \"Assumptions\" .-&gt; Model\n  Model -- \"Probability\" --&gt; Data\n  Data -- \"Inference\" --&gt; Model\n  Model -. \"Approximate Reality\" .-&gt; DGP"
  },
  {
    "objectID": "blog/statistical-modeling.html#modeling-randomness-estimation-and-quantifying-uncertainty",
    "href": "blog/statistical-modeling.html#modeling-randomness-estimation-and-quantifying-uncertainty",
    "title": "How to Think About Statistical Models and Inference",
    "section": "",
    "text": "In their book “Information Criteria and Statistical Modeling”, Konishi and Kitagawa state “the majority of the problems in statistical inference can be considered to be problems related to statistical modeling.”\n\n\n\n\n\nflowchart LR\n  classDef box fill:#f8f9fa,stroke:#444,stroke-width:1px,rx:10,ry:10;\n\n  DGP[\"Data Generating Process\"]:::box\n  Data[\"Observed Data\"]:::box\n  Model[\"Statistical Model\"]:::box\n\n  %% Main flows\n  DGP -- \"Randomness\" --&gt; Data\n  DGP -. \"Assumptions\" .-&gt; Model\n  Model -- \"Probability\" --&gt; Data\n  Data -- \"Inference\" --&gt; Model"
  },
  {
    "objectID": "blog/statistical-modeling.html#a-frequentist-framework",
    "href": "blog/statistical-modeling.html#a-frequentist-framework",
    "title": "How to Think About Statistical Models and Inference",
    "section": "",
    "text": "Suppose there exists some true Data Generating Process (DGP) that is responsible for producing the data we observe. We can think of this DGP to consist of a (i) deterministic component and a (ii) stochastic (or random) component.\n\n\n\n\n\nflowchart LR\n  classDef box fill:#f8f9fa,stroke:#444,stroke-width:1px,rx:10,ry:10;\n\n  DGP[\"Data Generating Process\"]:::box\n  Data[\"Observed Data\"]:::box\n  Model[\"Statistical Model\"]:::box\n\n  %% Main flows\n  DGP -- \"Randomness\" --&gt; Data\n  DGP -. \"Assumptions\" .-&gt; Model\n  Model -- \"Probability\" --&gt; Data\n  Data -- \"Inference\" --&gt; Model\n\n\n\n\n\n\nIn their book “Information Criteria and Statistical Modeling”, Konishi and Kitagawa state “the majority of the problems in statistical inference can be considered to be problems related to statistical modeling.”"
  },
  {
    "objectID": "blog/statistical-modeling.html#a-single-stochastic-data-generating-proces",
    "href": "blog/statistical-modeling.html#a-single-stochastic-data-generating-proces",
    "title": "How to Think About Statistical Models and Inference",
    "section": "",
    "text": "We can think of our observed data to be the result of a true underlying Data Generating Process (DGP) that is random. In other words, we repeated the data collection, we’d almost certainly get different values. The sources of randomness include (i) inherent fluctuation in the process itself (e.g., )\nWhen we collect data, we’re only seeing one realization of a process that has both structure and randomness. Beneath our dataset lies a true Data Generating Process (DGP) — the systematic part of how the world works — combined with stochastic elements that introduce variability. If we repeated the data collection, we’d almost certainly get different values. This variability comes from the inherent randomness in the process itself, as well as things like measurement error (imperfections in how we record data) and sampling error (the fact that we only observe a subset of the population).\nWe can think of observed data as being generated by the combination of some true underlying Data Generating Process (DGP) and some stochastic processes that introduce randomness. Thus, if we collected the data again, we would likely see different values. The source of this randomness in the observed data is usually a combination of inherent randomness in the DGP as well as measurement error and/or sampling error.\nsome true underlying Data Generating Process (DGP) that is inherently random. In other words, if we collected the data again, we would likely see different values. The source of this randomness is usually a combination of\nSuppose there exists some true Data Generating Process (DGP) that is responsible for producing the data we observe. This DGP can be decomposed into a (i) deterministic component and a (ii) stochastic component. The source of uncertainty is usually a combination of inherent randomness in the process as well as measurement error and/or sampling.\n\n\n\n\n\nflowchart LR\n  classDef box fill:#f8f9fa,stroke:#444,stroke-width:1px,rx:10,ry:10;\n\n  DGP[\"Data Generating Process\"]:::box\n  Data[\"Observed Data\"]:::box\n  Model[\"Statistical Model\"]:::box\n\n  %% Main flows\n  DGP -- \"Randomness\" --&gt; Data\n  DGP -. \"Assumptions\" .-&gt; Model\n  Model -- \"Probability\" --&gt; Data\n  Data -- \"Inference\" --&gt; Model\n\n\n\n\n\n\nIn their book “Information Criteria and Statistical Modeling”, Konishi and Kitagawa state “the majority of the problems in statistical inference can be considered to be problems related to statistical modeling.”"
  },
  {
    "objectID": "blog/statistical-modeling.html#a-single-stochastic-data-generating-process",
    "href": "blog/statistical-modeling.html#a-single-stochastic-data-generating-process",
    "title": "How to Think About Statistical Models and Inference",
    "section": "",
    "text": "We can think of our observed data to be the result of a true underlying Data Generating Process (DGP) that is inherently random.1 In other words, if we repeated the data collection, we’d almost certainly get different values.\n\n\n\n\n\nflowchart LR\n  classDef box fill:#f8f9fa,stroke:#444,stroke-width:1px,rx:10,ry:10;\n\n  DGP[\"Data Generating Process\"]:::box\n  Data[\"Observed Data\"]:::box\n  Model[\"Statistical Model\"]:::box\n\n  %% Main flows\n  DGP -- \"Randomness\" --&gt; Data\n  DGP -. \"Assumptions\" .-&gt; Model\n  Model -- \"Probability\" --&gt; Data\n  Data -- \"Inference\" --&gt; Model\n\n\n\n\n\n\nIn their book “Information Criteria and Statistical Modeling”, Konishi and Kitagawa state “the majority of the problems in statistical inference can be considered to be problems related to statistical modeling.”"
  },
  {
    "objectID": "blog/statistical-modeling.html#footnotes",
    "href": "blog/statistical-modeling.html#footnotes",
    "title": "How to Think About Statistical Inference",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nTo be clear, this is a simplification of reality because (i) it assumes that the DGP exists and (ii) it assumes that the DGP is stable over time. In reality, some events can fundamentally alter the DGP (if it even exists). For example, consider the impacts of the widespread adoption of artificial intelligence. Nevertheless, we often do observe an approximately stable distribution of data over time. Thus, the fixed-DGP is often reasonable in practice and, more importantly, provides a useful framework for applying probabilistic models to learn from the data we have.↩︎\nNote that if we instead defined statistical inference as “the process of using sample data to learn about the population,” we would be describing only one specific case. It is entirely possible to have population-level data and still require statistical inference due to natural variation or measurement error.↩︎\nWhen we think of the word “prediction” we usually think of forecasts. But notice how point estimation and hypothesis testing can also be viewed as a form of prediction, as we are extrapolating from observed data to the statistical model’s parameters. This guided the choice of using “informed predictions” as the umbrella term describing inference.↩︎\nSince the number of parameters defining the distribution is finite, this is an example of a parametric statistical model.↩︎\nOne way to find such estimates is through maximum likelihood estimation.↩︎"
  },
  {
    "objectID": "blog/statistical-modeling.html#the-goal",
    "href": "blog/statistical-modeling.html#the-goal",
    "title": "How to Think About Statistical Models and Inference",
    "section": "",
    "text": "We can think of our observed data to be the result of some true underlying Data Generating Process (DGP) that is inherently random.1 That is to say if we repeated the data collection, we would almost certainly get different values. The sources of this randomness are varied, and includes (i) natural variation in the phenomenon being measured, (ii) measurement error, and (iii) sampling error.\nThe presence of such randomness makes it challenging to learn about the unobserved characteristics of the DGP using the observed data. Statistical inference is the process of using observed data to make informed predictions about those unobserved features and quantifying the uncertainty in those predictions using probability.\n\n\n\n\n\nflowchart LR\n  classDef box fill:#f8f9fa,stroke:#444,stroke-width:1px,rx:10,ry:10;\n\n  DGP[\"Data Generating Process\"]:::box\n  Data[\"Observed Data\"]:::box\n  Model[\"Statistical Model\"]:::box\n\n  %% Main flows\n  DGP -- \"Randomness\" --&gt; Data\n  DGP -. \"Assumptions\" .-&gt; Model\n  Model -- \"Probability\" --&gt; Data\n  Data -- \"Inference\" --&gt; Model\n\n\n\n\n\n\nIn their book “Information Criteria and Statistical Modeling”, Konishi and Kitagawa state “the majority of the problems in statistical inference can be considered to be problems related to statistical modeling.”"
  },
  {
    "objectID": "blog/statistical-modeling.html#what-is-statistical-inference",
    "href": "blog/statistical-modeling.html#what-is-statistical-inference",
    "title": "How to Think About Statistical Models and Inference",
    "section": "",
    "text": "We can think of our observed data to be the result of some true underlying Data Generating Process (DGP) that is inherently random.1 That is to say if we repeated the data collection, we would almost certainly get different values. The sources of this randomness are varied, and includes (i) natural variation in the phenomenon being measured, (ii) measurement error, and (iii) sampling error.\nThe presence of such randomness makes it challenging to learn about the unobserved characteristics of the DGP using the observed data. Statistical inference is the process of using observed data to make informed predictions about those unobserved features and quantifying the uncertainty in those predictions using probability."
  },
  {
    "objectID": "blog/statistical-modeling.html#how-do-we-conduct-statistical-inference",
    "href": "blog/statistical-modeling.html#how-do-we-conduct-statistical-inference",
    "title": "How to Think About Statistical Models and Inference",
    "section": "How Do We Conduct Statistical Inference?",
    "text": "How Do We Conduct Statistical Inference?\n\n\n\n\n\nflowchart LR\n  classDef box fill:#f8f9fa,stroke:#444,stroke-width:1px,rx:10,ry:10;\n\n  DGP[\"Data Generating Process\"]:::box\n  Data[\"Observed Data\"]:::box\n  Model[\"Statistical Model\"]:::box\n\n  %% Main flows\n  DGP -- \"Randomness\" --&gt; Data\n  DGP -. \"Assumptions\" .-&gt; Model\n  Model -- \"Probability\" --&gt; Data\n  Data -- \"Inference\" --&gt; Model\n\n\n\n\n\n\nIn their book “Information Criteria and Statistical Modeling”, Konishi and Kitagawa state “the majority of the problems in statistical inference can be considered to be problems related to statistical modeling.”"
  },
  {
    "objectID": "blog/statistical-modeling.html#using-models-to-enable-inference",
    "href": "blog/statistical-modeling.html#using-models-to-enable-inference",
    "title": "How to Think About Statistical Models and Inference",
    "section": "Using Models to Enable Inference",
    "text": "Using Models to Enable Inference\n\n\n\n\n\nflowchart LR\n  classDef box fill:#f8f9fa,stroke:#444,stroke-width:1px,rx:10,ry:10;\n\n  DGP[\"Data Generating Process\"]:::box\n  Data[\"Observed Data\"]:::box\n  Model[\"Statistical Model\"]:::box\n\n  %% Main flows\n  DGP -- \"Randomness\" --&gt; Data\n  DGP -. \"Assumptions\" .-&gt; Model\n  Model -- \"Probability\" --&gt; Data\n  Data -- \"Inference\" --&gt; Model\n\n\n\n\n\n\nIn their book “Information Criteria and Statistical Modeling”, Konishi and Kitagawa state “the majority of the problems in statistical inference can be considered to be problems related to statistical modeling.”"
  },
  {
    "objectID": "blog/statistical-modeling.html#inference-are-confined-within-models",
    "href": "blog/statistical-modeling.html#inference-are-confined-within-models",
    "title": "How to Think About Statistical Models and Inference",
    "section": "Inference are Confined Within Models",
    "text": "Inference are Confined Within Models\n\n\n\n\n\nflowchart LR\n  classDef box fill:#f8f9fa,stroke:#444,stroke-width:1px,rx:10,ry:10;\n\n  DGP[\"Data Generating Process\"]:::box\n  Data[\"Observed Data\"]:::box\n  Model[\"Statistical Model\"]:::box\n\n  %% Main flows\n  DGP -- \"Randomness\" --&gt; Data\n  DGP -. \"Assumptions\" .-&gt; Model\n  Model -- \"Probability\" --&gt; Data\n  Data -- \"Inference\" --&gt; Model\n\n\n\n\n\n\nIn their book “Information Criteria and Statistical Modeling”, Konishi and Kitagawa state “the majority of the problems in statistical inference can be considered to be problems related to statistical modeling.”"
  },
  {
    "objectID": "blog/statistical-modeling.html#models-make-inference-possible",
    "href": "blog/statistical-modeling.html#models-make-inference-possible",
    "title": "How to Think About Statistical Inference",
    "section": "Models Make Inference Possible",
    "text": "Models Make Inference Possible\n\nA Set of Assumptions\nSince the true DGP is unknown, we need to make assumptions about its general structure in order to make inference feasible. A statistical model is a mathematical characterization of how the data is generated using probability theory. Specifically, we view the observed data points as realizations of a collection of random variables with some joint probability distribution. The set of assumptions that define this probability distribution — such as its functional form and the structural relationships among the random variables — is exactly what constitutes the statistical model.\n\n\nForms of Inference\nSo far, I have referred to statistical inference as involving informed predictions about unobserved features of the DGP. This is a broad umbrella term that encompasses various forms of inference, and is therefore worth clarifying.\nPerhaps the most common form of inference is point estimation. This is the process of using the observed data to produce a single “best guess” value (the point estimate) for an unknown numerical quantity that characterizes the probability distribution in the statistical model (the parameter). Another type of inference is hypothesis testing, where we use the data to assess specific claims about the model parameters. Finally, forecasting is a form of inference where the model is used to predict future realizations of the random variables.3"
  },
  {
    "objectID": "blog/statistical-modeling.html#predicting-the-unknown",
    "href": "blog/statistical-modeling.html#predicting-the-unknown",
    "title": "How to Think About Statistical Models and Inference",
    "section": "",
    "text": "We can think of our observed data to be the result of some true underlying Data Generating Process (DGP) that is inherently random.1 That is to say if we repeated the data collection, we would almost certainly get different values. The sources of this randomness are varied, and includes (i) natural variation in the phenomenon being measured, (ii) measurement error, and (iii) sampling error.\nThe presence of such randomness makes it challenging to learn about the unobserved characteristics of the DGP using the observed data. Statistical inference is the process of using observed data to make informed predictions about those unobserved features and quantifying the uncertainty in those predictions using probability."
  },
  {
    "objectID": "blog/statistical-modeling.html#learning-about-the-unknown",
    "href": "blog/statistical-modeling.html#learning-about-the-unknown",
    "title": "How to Think About Statistical Inference",
    "section": "",
    "text": "We can think of our observed data to be the result of some true, unknown Data Generating Process (DGP) that is inherently random.1 That is to say if we repeated the data collection, we would almost certainly get different values. The source of this randomness is varied, and includes (i) natural variation in the phenomenon being measured, (ii) measurement error, and (iii) sampling error.\nThe presence of such randomness makes it challenging to learn about the unobserved characteristics of the DGP using the observed data. Statistical inference is the process of (i) using random observed data to make informed predictions about unobserved features of the DGP and (ii) using probability to quantify the uncertainty in those predictions. 2"
  },
  {
    "objectID": "blog/statistical-modeling.html#forms-of-inference",
    "href": "blog/statistical-modeling.html#forms-of-inference",
    "title": "How to Think About Statistical Models and Inference",
    "section": "Forms of Inference",
    "text": "Forms of Inference"
  },
  {
    "objectID": "blog/statistical-modeling.html#parametric-statistical-models",
    "href": "blog/statistical-modeling.html#parametric-statistical-models",
    "title": "How to Think About Statistical Models and Inference",
    "section": "Parametric Statistical Models",
    "text": "Parametric Statistical Models\nTo clarify the ideas captured by the diagram above, let us ground ourselves to a specific class of statistical models: parametric models (or families)."
  },
  {
    "objectID": "blog/statistical-modeling.html#example-parametric-models",
    "href": "blog/statistical-modeling.html#example-parametric-models",
    "title": "How to Think About Statistical Inference",
    "section": "Example: Parametric Models",
    "text": "Example: Parametric Models\nTo clarify the ideas captured by the concept map above, let us ground ourselves to a specific class of statistical models: parametric models (or families).\nTODO."
  },
  {
    "objectID": "blog/statistical-modeling.html#inference-is-only-as-good-as-our-assumptions",
    "href": "blog/statistical-modeling.html#inference-is-only-as-good-as-our-assumptions",
    "title": "How to Think About Statistical Inference",
    "section": "Inference is Only as Good as Our Assumptions",
    "text": "Inference is Only as Good as Our Assumptions\nInitially, I had defined inference as the process of using observed data to make informed, probabilistic predictions about the unknown features of the data generating process. However, the above discussion of the forms of inference made no direct mention of the DGP. Rather, the common threat across all forms of inference was that they use observed data to learn about the statistical model. Thus, any conclusions we draw about the DGP from statistical inference are indirect and are entirely dependent on the model’s specification. Specifically, if the assumptions accurately reflect the true DGP, then inference can meaningfully inform us about the DGP. However, if the model is misspecified, the inferences we make do not tell us anything about reality. To quote Konishi and Kitagawa in their book Information Criteria and Statistical Modeling”, “the majority of the problems in statistical inference can be considered to be problems related to statistical modeling.”\n\n\n\n\n\nflowchart LR\n  classDef box fill:#f8f9fa,stroke:#444,stroke-width:1px,rx:10,ry:10;\n\n  DGP[\"Data Generating Process\"]:::box\n  Data[\"Observed Data\"]:::box\n  Model[\"Statistical Model\"]:::box\n\n  %% Main flows\n  DGP -- \"Randomness\" --&gt; Data\n  DGP -. \"Assumptions\" .-&gt; Model\n  Model -- \"Probability\" --&gt; Data\n  Data -- \"Inference\" --&gt; Model\n  Model -. \"Approximate Reality\" .-&gt; DGP\n\n\n\n\n\n\n\nExample: Parametric Models\nTo clarify the ideas captured by the concept map above, let us ground ourselves to a specific class of statistical models: parametric models (or families).\nTODO."
  },
  {
    "objectID": "blog/statistical-modeling.html#inference-lives-within-a-model",
    "href": "blog/statistical-modeling.html#inference-lives-within-a-model",
    "title": "How to Think About Statistical Inference",
    "section": "Inference Lives Within a Model",
    "text": "Inference Lives Within a Model\nInitially, I had defined inference as the process of using observed data to make informed, probabilistic predictions about the unknown features of the data generating process. However, the above discussion of the forms of inference made no direct mention of the DGP. Rather, the common threat across all forms of inference was that they use observed data to learn about the statistical model. Thus, any conclusions we draw about the DGP from statistical inference are indirect and are entirely dependent on the model’s specification. Specifically, if the assumptions accurately reflect the true DGP, then inference can meaningfully inform us about the DGP. However, if the model is misspecified, the inferences we make do not tell us anything about reality. To quote Konishi and Kitagawa in their book Information Criteria and Statistical Modeling”, “the majority of the problems in statistical inference can be considered to be problems related to statistical modeling.”\n\n\n\n\n\nflowchart LR\n  classDef box fill:#f8f9fa,stroke:#444,stroke-width:1px,rx:10,ry:10;\n\n  DGP[\"Data Generating Process\"]:::box\n  Data[\"Observed Data\"]:::box\n  Model[\"Statistical Model\"]:::box\n\n  %% Main flows\n  DGP -- \"Randomness\" --&gt; Data\n  DGP -. \"Assumptions\" .-&gt; Model\n  Model -- \"Probability\" --&gt; Data\n  Data -- \"Inference\" --&gt; Model\n  Model -. \"Approximate Reality\" .-&gt; DGP\n\n\n\n\n\n\n\nExample: Parametric Models\nTo clarify the ideas captured by the concept map above, let us ground ourselves to a specific class of statistical models: parametric models (or families).\nTODO."
  },
  {
    "objectID": "blog/statistical-modeling.html#all-inference-is-within-model",
    "href": "blog/statistical-modeling.html#all-inference-is-within-model",
    "title": "How to Think About Statistical Inference",
    "section": "All Inference is Within Model",
    "text": "All Inference is Within Model\nInitially, I had defined inference as the process of using observed data to make informed, probabilistic predictions about unknown features of the data generating process. However, notice that in our discussion of the different forms of inference, the DGP was never mentioned. In every case — point estimation, hypothesis testing, and forecasting — we use the observed data to learn more about the statistical model.\nAny conclusions we make about the DGP are therefore indirect and entirely dependent on the model’s specification. If the model’s assumptions accurately approximate the true DGP, then inference can meaningfully describe reality. However, if the model is misspecified, our inference may be well-calculated but ultimately irrelevant: a precise answer to the wrong question. As Konishi and Kitagawa put it in Information Criteria and Statistical Modeling, “the majority of the problems in statistical inference can be considered to be problems related to statistical modeling.”"
  },
  {
    "objectID": "blog/statistical-modeling.html#concept-map-and-example",
    "href": "blog/statistical-modeling.html#concept-map-and-example",
    "title": "How to Think About Statistical Inference",
    "section": "Concept Map and Example",
    "text": "Concept Map and Example\n\n\n\n\n\nflowchart LR\n  classDef box fill:#f8f9fa,stroke:#444,stroke-width:1px,rx:10,ry:10;\n\n  DGP[\"Data Generating Process\"]:::box\n  Data[\"Observed Data\"]:::box\n  Model[\"Statistical Model\"]:::box\n\n  %% Main flows\n  DGP -- \"Randomness\" --&gt; Data\n  DGP -. \"Assumptions\" .-&gt; Model\n  Model -- \"Probability\" --&gt; Data\n  Data -- \"Inference\" --&gt; Model\n  Model -. \"Approximate Reality\" .-&gt; DGP"
  },
  {
    "objectID": "blog/statistical-modeling.html#concept-map",
    "href": "blog/statistical-modeling.html#concept-map",
    "title": "How to Think About Statistical Inference",
    "section": "Concept Map",
    "text": "Concept Map\n\n\n\n\n\nflowchart LR\n  classDef box fill:#f8f9fa,stroke:#444,stroke-width:1px,rx:10,ry:10;\n\n  DGP[\"Data Generating Process\"]:::box\n  Data[\"Observed Data\"]:::box\n  Model[\"Statistical Model\"]:::box\n\n  %% Main flows\n  DGP -- \"Randomness\" --&gt; Data\n  DGP -. \"Assumptions\" .-&gt; Model\n  Model -- \"Probability\" --&gt; Data\n  Data -- \"Inference\" --&gt; Model\n  Model -. \"Approximate Reality\" .-&gt; DGP\n\n\n\n\n\n\n\nExample: Parametric Models\nTo clarify the ideas captured by the concept map above, let us ground ourselves to a specific class of statistical models: parametric models (or families).\nTODO."
  },
  {
    "objectID": "blog/statistical-modeling.html#a-concept-map",
    "href": "blog/statistical-modeling.html#a-concept-map",
    "title": "How to Think About Statistical Inference",
    "section": "A Concept Map",
    "text": "A Concept Map\n\n\n\n\n\nflowchart LR\n  classDef box fill:#f8f9fa,stroke:#444,stroke-width:1px,rx:10,ry:10;\n\n  DGP[\"Data Generating Process\"]:::box\n  Data[\"Observed Data\"]:::box\n  Model[\"Statistical Model\"]:::box\n\n  %% Main flows\n  DGP -- \"Randomness\" --&gt; Data\n  DGP -. \"Assumptions\" .-&gt; Model\n  Model -- \"Probability\" --&gt; Data\n  Data -- \"Inference\" --&gt; Model\n  Model -. \"Approximate Reality\" .-&gt; DGP\n\n\n\n\n\n\n\nExample: Parametric Models\nTo clarify the ideas captured by the concept map above, let us ground ourselves to a specific class of statistical models: parametric models (or families).\nTODO."
  },
  {
    "objectID": "blog/statistical-modeling.html#example-a-very-simple-parametric-model",
    "href": "blog/statistical-modeling.html#example-a-very-simple-parametric-model",
    "title": "How to Think About Statistical Inference",
    "section": "Example: A Very Simple Parametric Model",
    "text": "Example: A Very Simple Parametric Model\nThe relationship between the data generating process, observed data, and statistical model is summarized in the concept map below.\n\n\n\n\n\nflowchart LR\n  classDef box fill:#f8f9fa,stroke:#444,stroke-width:1px,rx:10,ry:10;\n\n  DGP[\"Data Generating Process\"]:::box\n  Data[\"Observed Data\"]:::box\n  Model[\"Statistical Model\"]:::box\n\n  %% Main flows\n  DGP -- \"Randomness\" --&gt; Data\n  DGP -. \"Assumptions\" .-&gt; Model\n  Model -- \"Probability\" --&gt; Data\n  Data -- \"Inference\" --&gt; Model\n  Model -. \"Approximate Reality\" .-&gt; DGP\n\n\n\n\n\n\nTo make this figure more concrete, it’s useful to walk through a simple example. Suppose the true data generating process is a standard normal distribution with mean \\(0\\) and variance \\(1\\). However, we don’t actually know this DGP and thus need to make assumptions about its general structure. After analyzing the random observed data, one possible statistical model we could assert is\n\\[\nX \\sim \\mathcal{N}(\\mu, \\sigma^2).\n\\] In words, we are treating our observed data as a random variable \\(X\\) with a normal probability distribution with unknown mean \\(\\mu\\) and variance \\(\\sigma^2\\). Since the number of parameters defining the distribution is finite, this is an example of a parametric statistical model.\nWith this model in hand, we can now perform statistical inference to learn about the unknown model parameters. For example, we can use the observed data to find point estimates for \\(\\mu\\) and \\(\\sigma^2\\), and quantify the uncertainty associated with those estimates. Since in this toy example our normal distribution assumption is correct, the inference would yield meaningful conclusions about the true DGP. However, if the DGP was something entirely different — say, a uniform distribution — then our estimated model parameters would provide limited insight, if any, into the underlying structure of the data generating process."
  },
  {
    "objectID": "blog/statistical-modeling.html#a-very-simple-parametric-model",
    "href": "blog/statistical-modeling.html#a-very-simple-parametric-model",
    "title": "How to Think About Statistical Inference",
    "section": "A Very Simple Parametric Model",
    "text": "A Very Simple Parametric Model\nThe relationship between the data generating process, observed data, and statistical model is summarized in the concept map below.\n\n\n\n\n\nflowchart LR\n  classDef box fill:#f8f9fa,stroke:#444,stroke-width:1px,rx:10,ry:10;\n\n  DGP[\"Data Generating Process\"]:::box\n  Data[\"Observed Data\"]:::box\n  Model[\"Statistical Model\"]:::box\n\n  %% Main flows\n  DGP -- \"Randomness\" --&gt; Data\n  DGP -. \"Assumptions\" .-&gt; Model\n  Model -- \"Probability\" --&gt; Data\n  Data -- \"Inference\" --&gt; Model\n  Model -. \"Approximate Reality\" .-&gt; DGP\n\n\n\n\n\n\nTo make this figure more concrete, it’s useful to walk through a simple example. Suppose the true data generating process is a standard normal distribution with mean \\(0\\) and variance \\(1\\). However, we don’t actually know this DGP and thus need to make assumptions about its general structure. After analyzing the random observed data, one possible statistical model we could assert is\n\\[\nX \\sim \\mathcal{N}(\\mu, \\sigma^2).\n\\] In words, we are treating our observed data as a random variable \\(X\\) with a normal probability distribution with unknown mean \\(\\mu\\) and variance \\(\\sigma^2\\). Since the number of parameters defining the distribution is finite, this is an example of a parametric statistical model.\nWith this model in hand, we can now perform statistical inference to learn about the unknown model parameters. For example, we can use the observed data to find point estimates for \\(\\mu\\) and \\(\\sigma^2\\), and quantify the uncertainty associated with those estimates.4 Since in this toy example our normal distribution assumption is correct, the inference would yield meaningful conclusions about the true DGP. However, if the DGP was something entirely different — say, a uniform distribution — then our estimated model parameters would provide limited insight, if any, into the underlying structure of the data generating process."
  },
  {
    "objectID": "blog/statistical-modeling.html#a-very-simple-model",
    "href": "blog/statistical-modeling.html#a-very-simple-model",
    "title": "How to Think About Statistical Inference",
    "section": "A Very Simple Model",
    "text": "A Very Simple Model\nThe relationship between the data generating process, observed data, and statistical model is summarized in the concept map below.\n\n\n\n\n\nflowchart LR\n  classDef box fill:#f8f9fa,stroke:#444,stroke-width:1px,rx:10,ry:10;\n\n  DGP[\"Data Generating Process\"]:::box\n  Data[\"Observed Data\"]:::box\n  Model[\"Statistical Model\"]:::box\n\n  %% Main flows\n  DGP -- \"Randomness\" --&gt; Data\n  DGP -. \"Assumptions\" .-&gt; Model\n  Model -- \"Probability\" --&gt; Data\n  Data -- \"Inference\" --&gt; Model\n  Model -. \"Approximate Reality\" .-&gt; DGP\n\n\n\n\n\n\nTo make this figure more concrete, it’s useful to walk through a simple example. Suppose the true data generating process is a standard normal distribution with mean \\(0\\) and variance \\(1\\). However, we don’t actually know this DGP and thus need to make assumptions about its general structure. After analyzing the random observed data, one possible statistical model we could assert is\n\\[\nX \\sim \\mathcal{N}(\\mu, \\sigma^2).\n\\] In words, we are treating our observed data as a random variable \\(X\\) with a normal probability distribution with unknown mean \\(\\mu\\) and variance \\(\\sigma^2\\).4\nWith this model in hand, we can now perform statistical inference to learn about the unknown model parameters. For example, we can use the observed data to find point estimates for \\(\\mu\\) and \\(\\sigma^2\\), and quantify the uncertainty associated with those estimates.5 Since in this toy example our normal distribution assumption is correct, the inference would yield meaningful conclusions about the true DGP. However, if the DGP was something entirely different — say, a uniform distribution — then our estimated model parameters would provide limited insight, if any, into the underlying structure of the data generating process."
  },
  {
    "objectID": "blog/linear-reg-model.html",
    "href": "blog/linear-reg-model.html",
    "title": "The Linear Regression Model and its OLS Estimation",
    "section": "",
    "text": "We are interested in datasets of the form \\(\\{y_{i}, x_{i1}, \\ldots, x_{iK}\\}_{i=1}^n\\) where \\(y_{i}\\) is the outcome and \\(x_{i1}, \\ldots, x_{iK}\\) are the regressors. To mathematically formalize how this data was generated, we view the observations as realizations of random variables that are drawn from some joint distribution \\(F\\), also called the data generating process (DGP). The random sampling assumption is one possible characterization of these draws.\n\nAssumption 1. Random Sampling\nThe observations \\(\\{y_i, x_{i1}, \\ldots, x_{iK}\\}^n_{i=1}\\) are realizations of the random variables \\(\\{Y_i, X_{i1}, \\ldots , X_{iK}\\}_{i=1}^n\\), which are independent and identically distributed (i.i.d) draws from the joint distribution \\(F(Y,X_1, \\ldots, X_K)\\).\n\nIt’s easy to get confused about the notation and terminology here, so let’s clarify. The random variables \\(Y, X_1, \\ldots, X_K\\) are theoretical objects that are used when talking about the data generating process in general. The random variables \\(Y_i, X_{i1}, \\ldots, X_{iK}\\) are the theoretical objects that correspond to each observation in the sample. Intuitively, we can think of these as the data before it was collected. The realizations \\(y_i, x_{i1}, \\ldots, x_{iK}\\) are the actual data (numbers) we observe after data collection. This abstraction allows us to use the tools of probability theory and mathematical statistics to infer from our dataset.1"
  },
  {
    "objectID": "blog/linear-reg-model.html#defining-the-model",
    "href": "blog/linear-reg-model.html#defining-the-model",
    "title": "The Linear Regression Model and its Various Estimation Methods",
    "section": "Defining the Model",
    "text": "Defining the Model\nRecall that a statistical model is simply a set of assumptions about some DGP. In this case, our DGP of interest is the joint distribution \\(F(Y, X_1, \\ldots, X_k)\\) of the outcome variable \\(Y\\) and the predictor variables \\((X_1, \\ldots, X_k)\\). However, under random sampling, it is equivalent to make assumptions about the distribution of the observations \\((Y_i, X_{i1}, \\ldots, X_{ik})\\) since they are i.i.d draws from \\(F(Y, X_1, \\ldots, X_k)\\).\n\nAssumption 1. The Linear Regression Model\n(A) (Linearity) The random variable \\(Y\\) is a linear combination of the predictor variables \\((X_1,\\ldots, X_k)\\), plus some random noise variable \\(e\\) that captures measurement error and idiosyncratic fluctuation in \\(Y\\): \\[\nY = \\beta_1X_1 + \\ldots + \\beta_kX_k + e.\n\\tag{1}\\]\n(B) (No Multicollinearity) None of the predictor variables are an exact linear combination of the other predictors.\n(C) (Zero Conditional Mean) For any given value of \\((X_1, \\ldots, X_k)\\), the noise \\(e\\) has a mean of zero: \\[\n\\mathbb{E}[e \\mid X_1, \\ldots , X_k] = 0.\n\\]\n\nIt is good practice to carefully think through what is being assumed in any given model. The linearity assumption states that (i) the functional form of the relationship between the outcome and predictors is linear in the parameters \\(\\boldsymbol{\\beta}=(\\beta_1, \\ldots, \\beta_k)\\), and (ii) the noise \\(e\\) is additive. The assumption of no multicollinearity is a technical assumption that ensures that there is a unique solution for the parameters. Lastly, the zero conditional mean assumption states that the conditional expectation of the noise given the predictors is zero. This is a crucial assumption of the model, and has several implications:\n\nThe conditional expectation of \\(Y\\) given \\(\\boldsymbol{X}=(X_1, \\ldots, X_k)\\) is a linear function of the realized values[^1]: \\[\n\\mathbb{E}[Y \\mid \\boldsymbol{X}] = \\boldsymbol{X}'\\beta.\n\\]\nThe noise term \\(e\\) has an unconditional mean of zero \\[\n\\mathbb{E}[e] = \\mathbb{E}[\\mathbb{E}[e \\mid \\boldsymbol{X}]] = 0.\n\\]\nThe noise term \\(e\\) is uncorrelated with the predictor variables \\[\n\\text{Cov}(e, \\boldsymbol{X}) = \\mathbb{E}[e \\boldsymbol{X}] = \\mathbb{E}\\big[\\mathbb{E}[e\\boldsymbol{X} \\mid \\boldsymbol{X}]\\big] = \\mathbb{E}\\big[\\boldsymbol{X}\\mathbb{E}[e|\\boldsymbol{X}]\\big]=0.\n\\] Intuitively, this means the noise term and predictors do not contain any information about one another.\n\nIt is also equally important to clarify what we are not assuming. First, we do not assume that \\(Y\\) is a linear function of \\((X_1, \\ldots, X_k)\\) in Equation 1; we only require that the parameters \\(\\boldsymbol{\\beta}\\) enter the equation linearly. This means that we are free to include non-linear transformations of the predictor variables in Equation 1 as long linearity in \\(\\boldsymbol{\\beta}\\) is preserved. Second, we make no assumptions about the distribution of \\(\\boldsymbol{X}\\). Third, we do not make assumptions about the distribution or variance of the noise term \\(e\\).\n\nSampling Procedure\nSince we do not observe the true data generating process \\(F(Y, X_1, \\ldots, X_k)\\), our objective is to infer the parameters \\(\\boldsymbol{\\beta}\\) in Equation 1 from the sample (observed) data. To do so, we need to make assumptions about the sampling procedure. Specifically, we will treat each observation \\((y_i, x_{i1}, \\ldots, x_{ik})\\) as realizations of the random variables \\((Y_i, X_{i1}, \\ldots, X_{ik})\\) that are drawn independently from the joint distribution \\(F(Y, X_1, \\ldots, X_k)\\).\n\nAssumption 2. Random Sampling\nThe observations \\(\\{(y_i, x_{i1}, \\ldots, x_{ik})\\}^n_{i=1}\\) are realizations of the random variables \\(\\{(Y_i, X_{i1}, \\ldots , X_{ik})\\}_{i=1}^n\\) which are independent and identically distributed (i.i.d) draws from common distribution \\(F\\).\n\nIt’s easy to get confused about the notation and terminology here, so let’s clarify. The random variables \\((Y, X_1, \\ldots, X_k)\\) are theoretical objects that are used when talking about the data generating process in general. The random variables \\((Y_i, X_{1i}, \\ldots, X_{ik})\\) are the theoretical objects that correspond to each observation in the sample. Intuitively, we can think of these as the data before it was collected. The realizations \\((y_i, x_{i1}, \\ldots, x_{ik})\\) are the actual data (numbers) we observe after data collection. Viewing the observations as realizations from i.i.d random variables will allow us to use probability theory to derive the estimators of the parameters \\(\\boldsymbol{\\beta}\\) and quantify the uncertainty around them.\n\n\nRecasting in Matrix Notation\nSince the linear regression model is a set of assumptions about the DGP \\(F(Y, X_1, \\ldots, X_k)\\), we only made statements about the generic random variables \\((Y, X_1, \\ldots, X_k)\\) when first describing it. The assumption of random sampling is useful because it allows us to connect the linear regression model directly to our dataset. Specifically, we can rewrite the model as a system of \\(n\\) equations\n\\[\n\\begin{aligned}\nY_1 &= \\beta_1 X_{11} + \\ldots + \\beta_k X_{1k} + e_1 = \\boldsymbol{X_1'}\\boldsymbol{\\beta} + e_1\\\\\\\nY_2 &= \\beta_1 X_{21}+ \\ldots + \\beta_k X_{2k} + e_2 = \\boldsymbol{X_2'}\\boldsymbol{\\beta} + e_2 \\\\\n&\\;\\;\\vdots \\\\\nY_n &= \\beta_1X_{n1} + \\ldots + \\beta_k X_{nk} + e_n= \\boldsymbol{X_n'}\\boldsymbol{\\beta} + e_n,\n\\end{aligned}\n\\tag{2}\\] where \\[\n\\boldsymbol{X_i} = \\begin{pmatrix} X_{i1} \\\\ \\vdots \\\\ X_{ik} \\end{pmatrix} \\in \\mathbb{R}^{k \\times 1} \\quad  \\text{and} \\quad \\boldsymbol{\\beta}= \\begin{pmatrix} \\beta_1 \\\\ \\vdots \\\\ \\beta_k \\end{pmatrix} \\in \\mathbb{R}^{k \\times 1}.\n\\]\nNote that since each observation \\((Y_i, \\boldsymbol{X_i})\\) is drawn i.i.d from \\(F(Y, \\boldsymbol{X})\\), the three assumptions of the linear regression model apply separately to each equation in the system above.\nEven more compactly, we can write the system of equations as \\[\n\\boldsymbol{Y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{e},\n\\] where \\[\n\\boldsymbol{Y} = \\begin{pmatrix} Y_1 \\\\ \\vdots \\\\ Y_n \\end{pmatrix} \\in \\mathbb{R}^{n \\times 1}, \\quad \\boldsymbol{e} = \\begin{pmatrix} e_1 \\\\ \\vdots \\\\ e_n \\end{pmatrix} \\in \\mathbb{R}^{n \\times 1}, \\quad  \\mathbf{X} = \\begin{pmatrix} \\boldsymbol{X_1'} \\\\ \\vdots \\\\ \\boldsymbol{X_n'} \\end{pmatrix} = \\begin{pmatrix} X_{11} & \\ldots & X_{1k} \\\\ \\vdots & \\ddots & \\vdots \\\\ X_{n1} & \\ldots & X_{nk} \\end{pmatrix} \\in \\mathbb{R}^{n \\times k}.\n\\]\nThe quantity \\(\\mathbf{X}\\)1 is called the design matrix.2\n\n\nRevisiting the Assumptions\nThe matrix representation of the linear regression model is useful because it helps us interpret the model in terms of our data and better understand the practical implications of the assumptions being made.\nLet’s revisit the zero conditional mean assumption. This assumption states that the error term has a conditional mean of zero given all the predictors. In matrix form, this is written as \\[\n\\mathbb{E}[e_i \\mid \\mathbf{X}] = \\mathbb{E}[e_i \\mid \\boldsymbol{X}_1, \\ldots, \\boldsymbol{X}_n] = 0 \\quad \\forall \\quad i = 1, \\ldots, n.\n\\tag{3}\\] This representation of the assumption emphasizes that the mean is conditional on the predictors of all the observations. We will refer to Equation 3 as the strict exogeneity condition.\nBy the random sampling assumption, we have that \\[\n(Y_i, \\boldsymbol{X}_i) \\perp (Y_j, \\boldsymbol{X}_j) \\quad \\forall \\quad i \\neq j.\n\\]\nNow, since the error term is a function of the outcome and predictors \\[\ne_i = Y_i - \\boldsymbol{X_i'}\\boldsymbol{\\beta} = f(Y_i, \\boldsymbol{X_i}) \\quad \\forall \\quad i = 1, \\ldots, n,\n\\] and independence is preserved under transformations, we have \\[\n(e_i, \\boldsymbol{X_i}) \\perp (e_j, \\boldsymbol{X_j}) \\quad \\forall \\quad i \\neq j.\n\\]\nNow, since \\((Y_i, \\boldsymbol{X}_i) \\perp (Y_j, \\boldsymbol{X}_j)\\) for all \\(i \\neq j\\) by the random sampling assumption, it follows that \\[\n(e_i, \\boldsymbol{X_i}) \\perp (e_j, \\boldsymbol{X_j}) \\quad \\forall \\quad i \\neq j,\n\\]\nIt’s also useful to revisit the no multicollinearity assumption. Recall from linear algebra that a rectangular matrix has a left inverse if and only if all of its columns are linearly independent (i.e. the matrix has full column rank). Since we assume no multicollinearity, this means that the design matrix \\(\\mathbf{X}\\) has full column rank3, and therefore has a left inverse. This, as we will see, is useful in deriving the ordinary least squares (OLS) estimator for the parameters."
  },
  {
    "objectID": "blog/linear-reg-model.html#estimation",
    "href": "blog/linear-reg-model.html#estimation",
    "title": "The Linear Regression Model and its Various Estimation Methods",
    "section": "Estimation",
    "text": "Estimation\n\nDeriving the OLS Estimator\n\n\nDeriving the MLE Estimator"
  },
  {
    "objectID": "blog/linear-reg-model.html#footnotes",
    "href": "blog/linear-reg-model.html#footnotes",
    "title": "The Linear Regression Model and its OLS Estimation",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis post discuses the big picture intuition for statistical modeling and inference in more detail.↩︎\nThere is some notational ambiguity here. Any bold, italicized letter represents a vector. Any bold, upright letter represents a matrix. So, \\(\\boldsymbol{X}_i\\) refers to the vector of random regressor variables \\((X_{i1}, \\ldots, X_{ik})\\), while \\(\\mathbf{X}\\) refers to the design matrix.↩︎\nBecause in any given row of the design matrix, the column entries are linearly independent of one another.↩︎\nThis is again because \\((e_i, \\boldsymbol X_i)\\) and \\(\\boldsymbol X_j\\) are simply functional transformations of the outcome and regressors.↩︎\nTechnically we have to also check the second order condition as well. But this follows from the fact that \\(\\boldsymbol{X}\\) has a full column rank, and so \\(\\boldsymbol X' \\boldsymbol X\\) is positive definite.↩︎"
  },
  {
    "objectID": "blog/linear-reg-model.html#estimating-the-model-parameters",
    "href": "blog/linear-reg-model.html#estimating-the-model-parameters",
    "title": "The Linear Regression Model and its Various Estimation Methods",
    "section": "Estimating the Model Parameters",
    "text": "Estimating the Model Parameters\n\nDeriving the OLS Estimator\nTODO.\n\n\nDeriving the MLE Estimator\nTODO."
  },
  {
    "objectID": "blog/linear-reg-model.html#random-sampling",
    "href": "blog/linear-reg-model.html#random-sampling",
    "title": "The Linear Regression Model and its Various Estimation Methods",
    "section": "",
    "text": "Suppose our data generating process of interest is the joint distribution \\(F\\) of some outcome variable \\(Y\\) and a set of predictor variables \\(\\boldsymbol{X}=(X_1, \\ldots, X_k)\\). In theory, we could now make assumptions about the joint distribution \\(F(X,Y)\\) and this would constitute our statistical model."
  },
  {
    "objectID": "blog/linear-reg-model.html#motivating-the-random-sampling-assumption",
    "href": "blog/linear-reg-model.html#motivating-the-random-sampling-assumption",
    "title": "The Linear Regression Model and its Various Estimation Methods",
    "section": "",
    "text": "Suppose we observe a dataset of \\(n\\) units, where each observation consists of an outcome \\(y_i\\) and a set of predictors \\((x_{i1}, \\ldots, x_{ik})\\).\nSuppose we collect a dataset with \\(n\\) observations of an outcome \\(y_i\\) and \\(k\\) predictors $$\n\\(\\{y_i, x_{i1}, \\ldots, x_{ik}\\}_{i=1}^n\\)\nSuppose we have a dataset with \\(n\\) observations of an outcome variable \\(y_i\\) and \\(k\\) predictor variables \\((x_i1, \\ldots, X_k)\\). Our goal is to learn about the relationship between \\(Y\\) and \\((X_1, \\ldots, X_k)\\) from the data.\nRecall that a statistical model is simply a set of assumptions about some data generating process (DGP). Suppose our DGP of interest is the joint distribution \\(F\\) of some outcome variable \\(Y\\) and a set of predictor variables \\(\\boldsymbol{X}=(X_1, \\ldots, X_k)\\).\nWe could now make assumptions about the joint distribution \\(F(Y, \\boldsymbol{X})\\) and this would constitute our statistical model. However, we are also interested in learning about the model from our data. Thus, it is useful to state the model in terms that is more tangible to our data. This is where the random sampling assumption comes in.\nSpecifically, we will treat each observation \\((y_i, x_{i1}, \\ldots, x_{ik})\\) as realizations of the random variables \\((Y_i, X_{i1}, \\ldots, X_{ik})\\) that are drawn independently from the joint distribution \\(F(Y, X_1, \\ldots, X_k)\\).\n\nAssumption 2. Random Sampling\nThe observations \\(\\{(y_i, x_{i1}, \\ldots, x_{ik})\\}^n_{i=1}\\) are realizations of the random variables \\(\\{(Y_i, X_{i1}, \\ldots , X_{ik})\\}_{i=1}^n\\) which are independent and identically distributed (i.i.d) draws from common distribution \\(F\\).\n\nIt’s easy to get confused about the notation and terminology here, so let’s clarify. The random variables \\((Y, X_1, \\ldots, X_k)\\) are theoretical objects that are used when talking about the data generating process in general. The random variables \\((Y_i, X_{1i}, \\ldots, X_{ik})\\) are the theoretical objects that correspond to each observation in the sample. Intuitively, we can think of these as the data before it was collected. The realizations \\((y_i, x_{i1}, \\ldots, x_{ik})\\) are the actual data (numbers) we observe after data collection. Viewing the observations as realizations from i.i.d random variables will allow us to use probability theory to derive the estimators of the parameters \\(\\boldsymbol{\\beta}\\) and quantify the uncertainty around them."
  },
  {
    "objectID": "blog/linear-reg-model.html#random-sampling-framework",
    "href": "blog/linear-reg-model.html#random-sampling-framework",
    "title": "The Linear Regression Model and its OLS Estimation",
    "section": "",
    "text": "We are interested in datasets of the form \\(\\{y_{i}, x_{i1}, \\ldots, x_{iK}\\}_{i=1}^n\\) where \\(y_{i}\\) is the outcome and \\(x_{i1}, \\ldots, x_{iK}\\) are the regressors. To mathematically formalize how this data was generated, we view the observations as realizations of random variables that are drawn from some joint distribution \\(F\\), also called the data generating process (DGP). The random sampling assumption is one possible characterization of these draws.\n\nAssumption 1. Random Sampling\nThe observations \\(\\{y_i, x_{i1}, \\ldots, x_{iK}\\}^n_{i=1}\\) are realizations of the random variables \\(\\{Y_i, X_{i1}, \\ldots , X_{iK}\\}_{i=1}^n\\), which are independent and identically distributed (i.i.d) draws from the joint distribution \\(F(Y,X_1, \\ldots, X_K)\\).\n\nIt’s easy to get confused about the notation and terminology here, so let’s clarify. The random variables \\(Y, X_1, \\ldots, X_K\\) are theoretical objects that are used when talking about the data generating process in general. The random variables \\(Y_i, X_{i1}, \\ldots, X_{iK}\\) are the theoretical objects that correspond to each observation in the sample. Intuitively, we can think of these as the data before it was collected. The realizations \\(y_i, x_{i1}, \\ldots, x_{iK}\\) are the actual data (numbers) we observe after data collection. This abstraction allows us to use the tools of probability theory and mathematical statistics to infer from our dataset.1"
  },
  {
    "objectID": "blog/linear-reg-model.html#defining-the-linear-regression-model",
    "href": "blog/linear-reg-model.html#defining-the-linear-regression-model",
    "title": "The Linear Regression Model and its OLS Estimation",
    "section": "Defining the Linear Regression Model",
    "text": "Defining the Linear Regression Model\nRecall that a statistical model is simply a set of assumptions about some DGP. In the case of the linear regression model, we are interested in the joint distribution \\(F(Y, X_1, \\ldots, X_K)\\). Under the random sampling assumption, however, each observation \\(Y_i, X_{i1}, \\ldots X_{iK}\\) is an i.i.d draw from \\(F\\). This means that we can just as well frame our assumptions in terms of the random variables \\(\\{Y_i, X_{i1}, \\ldots, X_{iK}\\}_{i=1}^n\\). As we will see, this formulation of the model is useful because it allows us to use matrix algebra to derive the estimator for the parameters.\n\nAssumption 2. The Linear Regression Model\n(A) (Linearity) The outcome \\(Y_i\\) is a linear combination of the regressors \\(X_{i1}, \\ldots, X_{iK}\\), plus some random error \\(e_i\\) that captures measurement error and idiosyncratic fluctuation in \\(Y_i\\): \\[\nY_i = \\beta_1X_{i1} + \\ldots + \\beta_kX_{iK} + e_i \\quad \\forall \\, i = 1, \\ldots, n.\n\\tag{1}\\]\n(B) (No Multicollinearity) None of the regressors are an exact linear combination of each other.\n(C) (Strict Exogeneity) The error \\(e_i\\) has a conditional mean of zero given the regressors of all observations \\[\n\\mathbb{E}[e_i \\mid \\boldsymbol{X}_1 \\ldots , \\boldsymbol{X}_n] = 0 \\quad \\forall \\, i = 1, \\ldots, n\n\\tag{2}\\] where \\(\\boldsymbol{X}_i = (X_{i1}, \\ldots, X_{iK})'\\).\n\nFor notational compactness, it is useful to recast the model in matrix notation. First note that we can stack Equation 1 as a system of \\(n\\) equations\n\\[\n\\begin{aligned}\nY_1 &= \\beta_1 X_{11} + \\ldots + \\beta_k X_{1K} + e_1 = \\boldsymbol{X}_1'\\boldsymbol{\\beta} + e_1\\\\\\\nY_2 &= \\beta_1 X_{21}+ \\ldots + \\beta_k X_{2K} + e_2 = \\boldsymbol{X}_2'\\boldsymbol{\\beta} + e_2 \\\\\n&\\;\\;\\vdots \\\\\nY_n &= \\beta_1X_{n1} + \\ldots + \\beta_k X_{nK} + e_n= \\boldsymbol{X}_n'\\boldsymbol{\\beta} + e_n,\n\\end{aligned}\n\\] where \\[\n\\boldsymbol{X}_i = \\begin{pmatrix} X_{i1} \\\\ \\vdots \\\\ X_{iK} \\end{pmatrix} \\in \\mathbb{R}^{K \\times 1} \\quad  \\text{and} \\quad \\boldsymbol{\\beta}= \\begin{pmatrix} \\beta_1 \\\\ \\vdots \\\\ \\beta_K \\end{pmatrix} \\in \\mathbb{R}^{K \\times 1}.\n\\]\nNow, we can collapse this system of equations into a single matrix equation: \\[\n\\boldsymbol{Y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{e},\n\\tag{3}\\] where \\[\n\\boldsymbol{Y} = \\begin{pmatrix} Y_1 \\\\ \\vdots \\\\ Y_n \\end{pmatrix} \\in \\mathbb{R}^{n \\times 1}, \\quad \\boldsymbol{e} = \\begin{pmatrix} e_1 \\\\ \\vdots \\\\ e_n \\end{pmatrix} \\in \\mathbb{R}^{n \\times 1}, \\quad  \\mathbf{X} = \\begin{pmatrix} \\boldsymbol{X}_1' \\\\ \\vdots \\\\ \\boldsymbol{X}_n' \\end{pmatrix} = \\begin{pmatrix} X_{11} & \\ldots & X_{1K} \\\\ \\vdots & \\ddots & \\vdots \\\\ X_{n1} & \\ldots & X_{nK} \\end{pmatrix} \\in \\mathbb{R}^{n \\times K}.\n\\]\nThe quantity \\(\\mathbf{X}\\) is called the design matrix. 2"
  },
  {
    "objectID": "blog/linear-reg-model.html#deriving-the-ols-estimator",
    "href": "blog/linear-reg-model.html#deriving-the-ols-estimator",
    "title": "The Linear Regression Model and its OLS Estimation",
    "section": "Deriving the OLS Estimator",
    "text": "Deriving the OLS Estimator\n\nPrinciple of Ordinary Least Squares\nRecall that an estimation method is the guiding principle we follow to construct an estimator — a function that maps the data to the parameters of a statistical model. One common estimation method for the linear regression model is the principle of ordinary least squares (OLS). This method finds the parameter values that minimize the sum of squared differences between the observed outcomes and the outcomes predicted by the model. Formally, the ordinary least squares estimator \\(\\hat{\\boldsymbol{\\beta}}_{OLS}\\) is defined as the solution of the quadratic minimization problem\n\\[\n\\begin{aligned}\n\\hat{\\boldsymbol{\\beta}} &:= \\underset{\\boldsymbol\\beta \\in \\mathbb{R^{k\\times1}}}{\\arg\\min} \\, S(\\boldsymbol\\beta) \\\\\n\\text{where} \\quad S(\\boldsymbol{\\beta}) &:= \\sum_{i=1}^n (Y_i - \\boldsymbol{X}_i'\\boldsymbol{\\beta})^2 = || \\boldsymbol{Y} - \\mathbf{X}\\boldsymbol{\\beta} ||^2.\n\\end{aligned}\n\\]\nThe quantity \\(S(\\boldsymbol\\beta)\\) is called the sum of squared errors (SSE). Intuitively, the motivation behind OLS is to choose the parameters such that the fitted outcomes \\(\\hat{Y}_i = \\boldsymbol X_i' \\hat{\\boldsymbol\\beta}\\) lie, on average, as close as possible to the observed outcomes \\(Y_i\\) in terms of their total squared vertical distance.5 This is visualized in the figure below."
  },
  {
    "objectID": "blog/linear-reg-model.html#ordinary-least-squares-ols-estimation",
    "href": "blog/linear-reg-model.html#ordinary-least-squares-ols-estimation",
    "title": "The Linear Regression Model and its OLS Estimation",
    "section": "Ordinary Least Squares (OLS) Estimation",
    "text": "Ordinary Least Squares (OLS) Estimation\n\nPrinciple of OLS\nRecall that an estimation method is the guiding principle we follow to construct an estimator — a function that maps the data to the parameters of a statistical model. One common estimation method for the linear regression model is the principle of ordinary least squares (OLS). This method finds the parameter values that minimize the sum of squared differences between the observed outcomes and the part of the outcome explained by the regressors. Formally, the ordinary least squares estimator \\(\\hat{\\boldsymbol{\\beta}}_{OLS}\\) is defined as the solution of the quadratic minimization problem\n\\[\n\\begin{aligned}\n\\hat{\\boldsymbol{\\beta}} &:= \\underset{\\boldsymbol\\beta \\in \\mathbb{R^{k\\times1}}}{\\arg\\min} \\, S(\\boldsymbol\\beta) \\\\\n\\text{where} \\quad S(\\boldsymbol{\\beta}) := \\sum_{i=1}^n (Y_i &- \\boldsymbol{X}_i'\\boldsymbol{\\beta})^2 =  (\\boldsymbol{Y} - \\mathbf{X}\\boldsymbol{\\beta})'(\\boldsymbol{Y} - \\mathbf{X}\\boldsymbol{\\beta}).\n\\end{aligned}\n\\tag{8}\\]\nThe quantity \\(S(\\boldsymbol\\beta)\\) is called the sum of squared errors (SSE). Intuitively, the motivation behind OLS is to choose the parameters such that the fitted outcomes of the form \\(\\hat{Y}_i = \\boldsymbol X_i' \\hat{\\boldsymbol\\beta}\\) lie as close as possible to the observed outcomes \\(Y_i\\) in terms of their total squared vertical distance. This is visualized in Figure 2 below.\n\n\nDeriving the OLS Estimator\nWe choose the OLS estimator to be the solution to the minimization problem in Equation 8. In other words, it is the estimator that satisfies the first-order condition (FOC).5 Using matrix algebra and calculus, the FOC is given by \\[\n\\begin{aligned}\n\\frac{\\partial}{\\partial \\boldsymbol\\beta}S(\\boldsymbol\\beta)\n&\\overset{(a)}{=} \\frac{\\partial}{\\partial \\boldsymbol\\beta}\\big( \\boldsymbol Y'\\boldsymbol Y-\\boldsymbol Y'\\boldsymbol X\\boldsymbol \\beta -  (\\boldsymbol X \\boldsymbol \\beta)'\\boldsymbol Y + (\\boldsymbol X \\boldsymbol \\beta)'(\\boldsymbol X \\boldsymbol \\beta) \\big) \\\\\n&\\overset{(b)}{=} \\frac{\\partial}{\\partial \\boldsymbol\\beta} \\big( \\boldsymbol Y'\\boldsymbol Y - \\boldsymbol Y' \\boldsymbol X \\boldsymbol \\beta - \\boldsymbol\\beta ' \\boldsymbol X' \\boldsymbol Y + \\boldsymbol \\beta' \\boldsymbol X' \\boldsymbol X \\boldsymbol \\beta \\big) \\\\\n&\\overset{(c)}{=} \\frac{\\partial}{\\partial \\boldsymbol\\beta} \\big( \\boldsymbol Y'\\boldsymbol Y - \\boldsymbol Y' \\boldsymbol X \\boldsymbol \\beta - (\\boldsymbol Y' \\boldsymbol X \\boldsymbol \\beta)' + \\boldsymbol \\beta' \\boldsymbol X' \\boldsymbol X \\boldsymbol \\beta \\big) \\\\\n&\\overset{(d)}{=} \\frac{\\partial}{\\partial \\boldsymbol\\beta} \\big( \\boldsymbol Y'\\boldsymbol Y - 2(\\boldsymbol Y' \\boldsymbol X \\boldsymbol \\beta) + \\boldsymbol \\beta' \\boldsymbol X' \\boldsymbol X \\boldsymbol \\beta \\big) \\\\\n&\\overset{(e)}{=} -2 \\boldsymbol X' \\boldsymbol Y + 2 \\boldsymbol X' \\boldsymbol X \\boldsymbol \\beta. \\\\\n&= 0,\n\\end{aligned}\n\\]\nwhere \\((a)\\) expands Equation 8, \\((b)\\) follows from \\((\\boldsymbol X \\boldsymbol \\beta)' = \\boldsymbol \\beta' \\boldsymbol X'\\), \\((c)\\) follows from \\((\\boldsymbol Y' \\boldsymbol X \\boldsymbol \\beta)' = \\boldsymbol \\beta' \\boldsymbol X' \\boldsymbol Y\\), \\((d)\\) uses the fact that the scalar \\(\\boldsymbol Y' \\boldsymbol X \\boldsymbol \\beta\\) equals its transpose, and \\((e)\\) applies the following matrix calculus results:\n\\[\n\\frac{\\partial}{\\partial \\boldsymbol b}(\\boldsymbol a' \\boldsymbol b) = \\boldsymbol a \\quad \\text{and} \\quad \\frac{\\partial}{\\partial \\boldsymbol b}(\\boldsymbol b' \\boldsymbol A \\boldsymbol b) = 2\\boldsymbol A\\boldsymbol b,\n\\] for any conformable vectors \\(\\boldsymbol a, \\boldsymbol b\\) and any conformable symmetric matrix \\(\\boldsymbol A\\).\nRearranging the FOC above, we see that the OLS estimator satisfies the least squares normal equations \\[\n\\boldsymbol X' \\boldsymbol X \\hat{\\boldsymbol \\beta}_{OLS}  = \\boldsymbol X' \\boldsymbol Y.\n\\] Since the matrix \\(\\boldsymbol X' \\boldsymbol X\\) is invertible, we have the the following closed form representation of the OLS estimator\n\\[\n\\hat{\\boldsymbol\\beta}_{OLS} = (\\boldsymbol X' \\boldsymbol X)^{-1} \\boldsymbol X' \\boldsymbol Y.\n\\tag{9}\\]"
  },
  {
    "objectID": "blog/linear-reg-model.html#interpreting-the-assumptions",
    "href": "blog/linear-reg-model.html#interpreting-the-assumptions",
    "title": "The Linear Regression Model and its OLS Estimation",
    "section": "Interpreting the Assumptions",
    "text": "Interpreting the Assumptions\nIt is good practice to carefully think through what is being assumed in any given model. The linearity assumption in Equation 1 and Equation 3 states that (i) the functional form of the relationship between the outcome and regressors is linear in the parameters, and (ii) the error is additive.\nThe assumption of no multicollinearity ensures that the design matrix \\(\\mathbf{X}\\) has full column rank.3 This in turn means the square matrix \\(\\mathbf{X}'\\mathbf{X}\\) is invertible, which is a crucial property when deriving the ordinary least squares (OLS) estimator for the parameters.\nThe strict exogeneity assumption states that the conditional expectation of the error given the regressors of all observations is zero. This is a crucial assumption of the model, and has several implications:\n\nThe unconditional mean of the error is zero: \\[\n\\mathbb{E}[e_i] \\overset{(a)}{=} \\mathbb{E}[\\mathbb{E}[e_i \\mid \\mathbf{X}]] \\overset{(b)}{=} 0 \\quad \\forall \\, i = 1, \\ldots, n,\n\\tag{4}\\] where \\((a)\\) uses the law of iterated expectations and \\((b)\\) follow from Equation 2. Also, since the conditional mean equals the unconditional mean, the error \\(e_i\\) is mean independent of the regressors \\(\\boldsymbol X_1, \\ldots, \\boldsymbol X_n.\\)\nThe error is orthogonal to the regressors of all observations: \\[\n\\begin{align}\n\\mathbb{E}[X_{jk}e_i]\n&\\overset{(a)}{=} \\mathbb{E}\\big[\\mathbb{E}[X_{jk}e_i \\mid X_{jk}]\\big] \\\\\n&\\overset{(b)}{=} \\mathbb{E}\\big[X_{jk} \\mathbb{E}[e_i \\mid X_{jk}]\\big]  \\\\\n&\\overset{(c)}{=} 0 \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\forall \\, i,j = 1, \\ldots, n; k = 1, \\ldots, K,\n\\end{align}\n\\tag{5}\\] where \\((a)\\) uses the law of iterated expectations, \\((b)\\) uses the linearity of expectations, and \\((c)\\) follows from the fact that \\[\n\\mathbb{E}[e_i \\mid X_{jk} ]= \\mathbb{E}\\big[\\mathbb{E}[e_i \\mid \\mathbf{X} ] \\mid X_{jk}\\big] =0\n\\] by the generalized law of iterated expectations.\nThe error is uncorrelated with the regressors of all observations: \\[\n\\text{Cov}(e_i, X_{jk})\n\\overset{(a)}{=} \\mathbb{E}[X_{jk}e_i] - \\mathbb{E}[X_{jk}]\\mathbb{E}[e_i]\n\\overset{(b)}{=} \\mathbb{E}[e_iX_{jk}]\n\\overset{(c)}{=} 0\n\\quad \\forall \\, i,j,k,\n\\tag{6}\\] where \\((a)\\) is the definition, \\((b)\\) uses Equation 4, and \\((c)\\) uses Equation 5. Intuitively, this means the error term and regressors do not contain any information about one another.\nThe conditional expectation of \\(\\boldsymbol{Y}\\) given \\(\\mathbf{X}\\), called the regression of \\(\\boldsymbol{Y}\\) on \\(\\mathbf{X}\\), is a linear function of the realized values: \\[\n\\mathbb{E}[\\boldsymbol{Y} \\mid \\mathbf{X}] = \\mathbb{E}[\\mathbf{X}\\beta + \\boldsymbol{e} \\mid \\mathbf{X}]=\\mathbb{E}[{\\mathbf{X}\\beta \\mid  \\mathbf{X}}]=\\mathbf{X}\\beta.\n\\]\n\n\nRestrictiveness of the Assumptions\nThe linearity and no-multicollinearity assumptions are non-trivial in the sense that it is possible for a dataset to violate them. The assumption that the conditional mean of the error is a constant function is also non-trivial, but the fact that this constant equals zero is trivial (i.e. not restrictive) if the linear regression model is of the form \\[\nY_i = \\beta_1 + \\beta_2X_{i2} + \\ldots + \\beta_kX_{iK} + e_i \\quad \\forall \\, i = 1, \\ldots, n.\n\\tag{7}\\]\nIn Equation 7, we set the first regressor to equal one for all observations and thus obtain a constant intercept term \\(\\beta_1\\) in the model. In this setting, if the conditional mean of the error is some non-zero constant \\(\\mu\\), we can simply redefine the intercept term to be \\(\\beta_1^\\star = \\beta_1 + \\mu\\) and the error to be \\(e_i^\\star = e_i - \\mu\\). Then, the conditional mean of the new error \\(e_i^\\star\\) is zero:\n\\[\n\\mathbb{E}[e_i^\\star \\mid \\boldsymbol{X}_1, \\ldots, \\boldsymbol{X}_n] = \\mathbb{E}[e_i - \\mu \\mid \\boldsymbol{X}_1, \\ldots, \\boldsymbol{X}_n] = \\mathbb{E}[e_i \\mid \\boldsymbol{X}_1, \\ldots, \\boldsymbol{X}_n] - \\mu = 0.\n\\]\nFor this reason, it is generally recommended to include a constant intercept term in the linear regression model.\n\n\nImplications of Random Sampling\nRandom sampling has two key implications for the linear regression model. First, it implies that the errors are independent (and therefore uncorrelated) across observations. To see this, first note that under random sampling\n\\[\n(Y_i, \\boldsymbol{X}_i) \\perp (Y_j, \\boldsymbol{X}_j) \\quad \\forall \\, i \\neq j.\n\\] Since independence is preserved under functional transformations, and the error is a function of the outcome and regressors, \\[\ne_i = f(Y_i, \\boldsymbol{X}_i) = Y_i - \\boldsymbol{X}_i\\boldsymbol{\\beta} \\quad \\forall \\, i = 1, \\ldots, n,\n\\]\nit follows that the errors \\(e_i\\) are independent across observations.\nA second implication is that random sampling allows us to simplify the strict exogeneity assumption. Specifically, since random sampling implies4\n\\[\n(e_i, \\boldsymbol{X}_i) \\perp (\\boldsymbol{X}_j) \\quad \\forall \\, i \\neq j,\n\\] it follows that\n\\[\n\\mathbb{E}[e_i \\mid \\boldsymbol{X}_1, \\ldots, \\boldsymbol{X}_n] = \\mathbb{E}[e_i \\mid \\boldsymbol{X}_i] \\quad \\forall \\, i = 1, \\ldots, n.\n\\]\nThus, under random sampling, the strict exogeneity assumption is equivalent to the simpler assumption\n\\[\n\\mathbb{E}[e_i \\mid \\boldsymbol{X}_i] = 0 \\quad \\forall \\, i = 1, \\ldots, n.\n\\]\nIntuitively, random sampling eliminates any cross-sectional dependence between the error term and the regressors of other observations. As a result, the linear regression model only needs to assert that each error \\(e_i\\) is mean independent of its own regressors \\(\\boldsymbol{X}_i\\).\n\n\nWhat is NOT Assumed\nBefore proceeding, it is worth clarifying what we do not assume in the linear regression model. First, we do not assume that \\(Y\\) is a linear function of \\((X_1, \\ldots, X_k)\\) in Equation 1; we only require that the parameters \\(\\boldsymbol{\\beta}\\) enter the equation linearly. This means that we are free to include non-linear transformations of the regressor variables in Equation 1 as long as linearity in \\(\\boldsymbol{\\beta}\\) is preserved (see Equation 10 for an example). Second, we make no assumptions about the distribution of the covariates. Third, we do not make assumptions about the distribution or variance of the error term \\(e\\)."
  },
  {
    "objectID": "blog/linear-reg-model.html#example-ols-in-action",
    "href": "blog/linear-reg-model.html#example-ols-in-action",
    "title": "The Linear Regression Model and its OLS Estimation",
    "section": "Example: OLS in Action",
    "text": "Example: OLS in Action\nThe phrase “OLS fits the best line to the data” is often used to describe what the OLS estimator does. However, this language is a bit loose and perpetuates the common misconception that the least squares estimation method can only fit a straight line to the data. In reality, the functional form of the relationship between \\(Y\\) and \\(X\\) is entirely determined by the researcher’s choice of regressors to include when specifying the model. Least squares simply chooses the linear combination of those regressors that best fits the data in terms of minimizing the total squared vertical distance to the observed outcomes.\nLet’s walk through a simple example to illustrate this point. Suppose we observe the following dataset:\n\n\nShow code\nset.seed(42)\n\n# Simulated data\nn  &lt;- 30\nX  &lt;- seq(-3, 3, length.out = n)\nY  &lt;- 2 + 0.5 * X^2 + rnorm(n, 0, 1)\n\ndf &lt;- data.frame(X = X, Y = Y)\n\n# Plot the observed data\nggplot(df, aes(X, Y)) +\n  geom_point(color = \"black\", shape = 4, size = 2) +\n  labs(x = \"X\", y = \"Y\") +\n  theme_classic() +\n  theme(panel.border = element_rect(color = \"black\", fill = NA))\n\n\n\n\n\n\n\n\nFigure 1: Observed Data with Nonlinear Relationship Between Outcome and Regressor\n\n\n\n\n\nThe scatter plot in Figure 1 suggests that the functional form of the relationship between \\(Y\\) and \\(X\\) is approximately a quadratic function. Thus, we can specify the linear regression model as \\[\nY = \\beta_1 + \\beta_2 X + \\beta_3 X^2 + e.\n\\tag{10}\\]\nThe next step is to estimate the parameters \\(\\beta_1, \\beta_2, \\beta_3\\) using the principle of ordinary least squares. We can do this using the lm function in R.\n\nfit &lt;- lm(Y ~ X + I(X^2), data = df)\nprint(fit)\n\n\nCall:\nlm(formula = Y ~ X + I(X^2), data = df)\n\nCoefficients:\n(Intercept)            X       I(X^2)  \n     1.9706      -0.1942       0.5306  \n\n\nThese numbers correspond to what we would get if we applied Equation 9 to the data.\n\nX &lt;- cbind(1, df$X, (df$X)^2)                     # Design matrix\nY &lt;- df$Y                                         # Outcome vector\nbeta_hat &lt;- solve(crossprod(X), crossprod(X, Y))  # Solves (X'X) b = X'Y\nc((beta_hat[1]), (beta_hat[2]), (beta_hat[3]))\n\n[1]  1.9705793 -0.1941996  0.5305615\n\n\nThe benefit of using the lm function is that it also can report inferential statistics — such as, the p-value, standard error, etc. — for the estimates. The meaning of these quantities is discussed in this post.\n\nsummary(fit)\n\n\nCall:\nlm(formula = Y ~ X + I(X^2), data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.5348 -0.4087 -0.1239  0.7664  2.1883 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.97058    0.34115   5.776 3.82e-06 ***\nX           -0.19420    0.12688  -1.531    0.138    \nI(X^2)       0.53056    0.07935   6.686 3.54e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.245 on 27 degrees of freedom\nMultiple R-squared:  0.6354,    Adjusted R-squared:  0.6084 \nF-statistic: 23.53 on 2 and 27 DF,  p-value: 1.216e-06\n\n\nFinally, we can visualize the estimation results. In Figure 2, the black crosses are the observed data points, the red curve is a plot of the fitted function \\[\n\\hat{Y}_i = \\hat{\\beta}_1 + \\hat{\\beta}_2 X_i + \\hat{\\beta}_3 X_i^2\n\\]\nin the \\(X-Y\\) plane, and the vertical dotted lines represent the residuals \\(e_i = Y_i - \\hat{Y}_i\\).\n\n\nShow code\n# Fitted values\ndf$Yhat &lt;- fitted(fit)\n\n# Smooth curve for plotting (only over observed range)\nxg   &lt;- seq(min(df$X), max(df$X), length.out = 400)\ngrid &lt;- data.frame(X = xg)\ngrid$Yhat &lt;- predict(fit, newdata = grid)\n\n# Plot\nggplot(df, aes(X, Y)) +\n  geom_point(aes(color = \"Observed Data\"), shape = 4, size = 2) +\n  geom_segment(aes(x = X, xend = X,\n                   y = pmin(Y, Yhat), yend = pmax(Y, Yhat)),\n               linetype = \"dotted\", linewidth = 0.4, color = \"blue\") +\n  geom_line(data = grid, aes(y = Yhat, color = \"OLS Best Fit Curve\"), linewidth = 1) +\n  scale_color_manual(values = c(\"Observed Data\" = \"black\",\n                                \"OLS Best Fit Curve\" = \"red\"),\n                     breaks = c(\"Observed Data\", \"OLS Best Fit Curve\"),\n                     name = NULL) +\n  labs(x = \"X\", y = \"Y\") +\n  coord_cartesian(xlim = range(df$X)) +  # avoid accidental extrapolation\n  theme_classic() +\n  theme(panel.border = element_rect(color = \"black\", fill = NA)) +\n  theme(\n    legend.position = c(0.15, 0.18),\n    legend.background = element_blank(),\n    legend.key = element_blank()\n  )\n\n\n\n\n\n\n\n\nFigure 2: OLS Estimation for Linear Regression Model with Quadratic Regressor"
  },
  {
    "objectID": "blog/linear-reg-model.html#references",
    "href": "blog/linear-reg-model.html#references",
    "title": "The Linear Regression Model and its OLS Estimation",
    "section": "References",
    "text": "References\n\n\nHansen, Bruce E. 2022. Econometrics. Princeton University Press.\n\n\nHayashi, Fumio. 2000. Econometrics. Princeton University Press.\n\n\nShalizi, Cosma Rohilla. 2024. “The Truth about Linear Regression.” https://www.stat.cmu.edu/~cshalizi/TALR/TALR.pdf."
  }
]