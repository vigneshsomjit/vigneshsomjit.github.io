[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Vignesh Somjit",
    "section": "",
    "text": "I recently graduated from Boston University, where I studied Economics and Mathematics. Starting in Summer 2025, I will be joining the University of Chicago Booth School of Business as a Research Professional. My current research interests lie in causal inference, applied econometrics, and machine learning."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "Vignesh Somjit",
    "section": "",
    "text": "I recently graduated from Boston University, where I studied Economics and Mathematics. Starting in Summer 2025, I will be joining the University of Chicago Booth School of Business as a Research Professional. My current research interests lie in causal inference, applied econometrics, and machine learning."
  },
  {
    "objectID": "blog/blog-index.html",
    "href": "blog/blog-index.html",
    "title": "Vignesh Somjit",
    "section": "",
    "text": "Interpreting Linear Regression\n\n\n\n\n\n\nEconometrics\n\n\n\nThinking about baselines and comparisons is the key to interpreting the results of linear regressions.\n\n\n\n\n\nJul 23, 2025\n\n\n\n\n\n\n\nUnderstanding the Bootstrap Method\n\n\n\n\n\n\nStatistics\n\n\n\nWalking through the Efron (1979) bootstrap method.\n\n\n\n\n\nJun 1, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/regressions.html",
    "href": "blog/regressions.html",
    "title": "Interpreting Linear Regression",
    "section": "",
    "text": "Linear Regression Approximates the Conditional Expectation Function\nRecall the canonical problem that contextualizes the use of linear regression. We have a random variable \\(Y_i\\), called the outcome, and a random vector \\(X_i\\) of covariates. We are interested in predicting the value of \\(Y_i\\) given some value of \\(X_i\\).\nAs a starting point, suppose for generality that our predictions are a function of \\(X_i\\) and we want our predictions to minimize the mean squared error (MSE) objective function:\n\\[\n\\mathbb{E} [Y_i - f(X_i)]^2.\n\\] With the law of iterated expectation and some calculus, we can show that for every possible realized value \\(X_i = x\\), the optimal function that solves the minimum mean squared error problem is the conditional expectation function (CEF)\n\\[\n\\mu(x) = \\mathbb{E}[Y_i | X_i = x].\n\\]\nIn most empirical cases, the CEF has no analytic form. Thus, it is more practical to model the relationship between the outcome and covariates using a simpler function. One option is to find a linear approximation of the CEF of the form\n\\[\n\\ell(x) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_k x_k = x^T\\beta.\n\\tag{1}\\] Equation 1 is our linear regression model, and the word “linear” here emphasizes the simplifying assumption that the function is a linear combination of the covariates. Since we want our predictions to minimize the MSE function, the best linear predictor is given by solving\n\\[\n\\beta^\\star = \\underset{\\beta}{\\arg\\min} \\, \\mathbb{E} [Y_i - (X_i^T \\beta)]^2.\n\\]\nThe main takeaway here is that linear regression provides a linear approximation to the conditional expectation function, allowing us to model the potentially complicated relationship between an outcome \\(Y_i\\) and covariates \\(X_i\\) with a simpler function.\n\n\nInterpreting Indicators and Interactions\nTODO\n\n\nFixed Effects as Baselines\nTODO"
  }
]