[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Vignesh Somjit",
    "section": "",
    "text": "I am a Research Professional at the University of Chicago Booth School of Business, where I work with Professor Richard Hornbeck. I received my B.A. in Economics and B.A. in Mathematics from Boston University in 2025."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "Vignesh Somjit",
    "section": "",
    "text": "I am a Research Professional at the University of Chicago Booth School of Business, where I work with Professor Richard Hornbeck. I received my B.A. in Economics and B.A. in Mathematics from Boston University in 2025."
  },
  {
    "objectID": "blog/blog-index.html",
    "href": "blog/blog-index.html",
    "title": "Vignesh Somjit",
    "section": "",
    "text": "An Applied Econometrics Rationale for Linear Regression\n\n\n\n\n\n\nEconometrics\n\n\n\nLinear regression can be motivated as a tool to approximate the conditional expectation function. I discuss this perspective and build intuition for interpreting regression parameters.\n\n\n\n\n\nJul 24, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/regressions.html",
    "href": "blog/regressions.html",
    "title": "An Applied Econometrics Rationale for Linear Regression",
    "section": "",
    "text": "Regression Approximates Conditional Averages\nWe are interested in predicting the value of a random variable \\(Y_i\\), called the outcome, given some realized value of a random vector \\(X_i\\) of covariates. As a starting point, we let our predictions be a function of \\(X_i\\) and want our predictions to minimize the mean squared error (MSE) objective function\n\\[\n\\mathbb{E} [Y_i - f(X_i)]^2.\n\\] With the law of iterated expectation and some calculus, we can show that for every possible realized value \\(X_i = x\\), the optimal function that solves the minimum mean squared error problem is the conditional expectation function (CEF)\n\\[\n\\mu(x) = \\mathbb{E}[Y_i | X_i = x].\n\\]\nIn most empirical cases, however, the CEF of interest has no analytic form. Thus, it is more practical to model the relationship between the outcome and covariates using a simpler function. One option is to find a linear approximation of the CEF, which takes the form\n\\[\n\\ell(x) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_k x_k = x^T\\beta.\n\\]\nNote that the word “linear” here refers to the simplifying assumption that the function is a linear combination of the covariates. Since we want our predictions to minimize the MSE function, the best linear predictor is found by setting the coefficients as\n\\[\n\\beta^\\star = \\underset{\\beta}{\\arg\\min} \\, \\mathbb{E} [Y_i - (X_i^T \\beta)]^2 = \\mathbb{E}[X_i X_i^T]^{-1}\\mathbb{E}[X_iY_i].\n\\] Thus, the linear regression model that describes the relationship between \\(Y_i\\) and \\(X_i\\) is given by \\[\nY_i = \\underbrace{X_i^T \\beta^\\star}_{\\text{best linear predictor}} + \\underbrace{\\epsilon_i}_{\\text{residual}},\n\\] where \\(\\epsilon_i\\) captures the difference between the realized outcome and the best linear predictor.\nTo summarize, the conditional expectation function is the optimal predictor of an outcome given covariates, in the sense that it minimizes the mean squared prediction error. Since the CEF is typically a complex function, linear regression approximates it by selecting the linear combination of covariates that minimizes the mean squared error. The resulting linear predictor provides a simple model of the relationship between the outcome and the covariates.\n\n\nApproximate and Exact Conditional Averages\n\nset.seed(123)\n\n# Sample size \nn &lt;- 1000 \n\n# Covariates\nsex &lt;- rbinom(n, 1, 0.5)\nrace &lt;- rbinom(n, 1, 0.3)                   \n\n# Non-linear data generating process; no interactions\noutcome1 &lt;- 5 + 2^sex + 3^race \n\n# Non-linear data generating process; interactions \noutcome2 &lt;- 5 + 2^sex + 3^race + 1.5^(sex*race)\n\n# Consolidate in a dataset \ndf &lt;- data.frame(\n  outcome1 = outcome1,\n  outcome2 = outcome2,\n  sex = sex,\n  race = race\n)"
  },
  {
    "objectID": "blog/linear-reg-rationale.html",
    "href": "blog/linear-reg-rationale.html",
    "title": "Linear Regression as an Approximation Method",
    "section": "",
    "text": "For observations \\(i = 1, \\ldots, n\\), suppose there is a data generating process (DGP) that describes the joint distribution of a random outcome variable \\(Y_i\\) and a random vector \\(X_i =(X_{1i}, X_{2i}, \\ldots, X_{ki})\\) of covariates.1 We are interested in predicting \\(Y_i\\) given some realized values of the covariates, denoted by \\(x_i = (x_{1i}, x_{2i}, \\ldots, x_{ki})\\). Thus, any prediction can generally be expressed as a function \\(f(X_i)\\) of the covariates. As a starting point, we consider choosing \\(f\\) to minimize the mean squared error (MSE) between our predictions and the observed outcomes:\n\\[\n\\mathbb{E} [Y_i - f(X_i)]^2.\n\\] With the law of iterated expectation and some calculus, we can show that for every possible realized value \\(X_i = x_i\\), the optimal function that solves the minimum mean squared error problem is the conditional expectation function (CEF)\n\\[\nm(x_i) = \\mathbb{E}[Y_i | X_i = x_i].\n\\tag{1}\\]"
  },
  {
    "objectID": "blog/linear-reg-rationale.html#regression-approximates-conditional-averages",
    "href": "blog/linear-reg-rationale.html#regression-approximates-conditional-averages",
    "title": "Motivating Linear Regression as a Tool",
    "section": "",
    "text": "For observations \\(i = 1, \\ldots, n\\), suppose there is a data generating process (DGP) that describes the joint distribution of a random outcome variable \\(Y_i\\) and a random vector \\(X_i =(X_{1i}, X_{2i}, \\ldots, X_{ki})\\) of covariates.1 We are interested in predicting \\(Y_i\\) given some realized values of the covariates, denoted by \\(x_i = (x_{1i}, x_{2i}, \\ldots, x_{ki})\\). Thus, any prediction can generally be expressed as a function \\(f(X_i)\\) of the covariates. As a starting point, we consider choosing \\(f\\) to minimize the mean squared error (MSE) between our predictions and the observed outcomes:\n\\[\n\\mathbb{E} [Y_i - f(X_i)]^2.\n\\] With the law of iterated expectation and some calculus, we can show that for every possible realized value \\(X_i = x_i\\), the optimal function that solves the minimum mean squared error problem is the conditional expectation function (CEF)\n\\[\nm(x) = \\mathbb{E}[Y_i | X_i = x_i].\n\\]\nIn most empirical cases, however, the functional form of the CEF is unknown. Thus, it is more practical to model the relationship between the outcome and covariates using a simpler function. One option is to find a linear approximation of the CEF, which takes the form\n\\[\n\\ell(x_i) = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + \\ldots + \\beta_k x_{ki} = x_i^T\\beta,\n\\] where \\(x_i = (1, x_{1i}, x_{2i}, \\ldots, x_{ki})\\). Note that the word “linear” here refers to the simplifying assumption that the function is a linear combination of the covariates. Since we want our predictions to minimize the MSE function, the best linear predictor is found by setting the coefficients as\n\\[\n\\beta^\\star = \\underset{\\beta}{\\arg\\min} \\, \\mathbb{E} [Y_i - (X_i^T \\beta)]^2 = \\mathbb{E}[X_i X_i^T]^{-1}\\mathbb{E}[X_iY_i].\n\\]\nThus, the linear regression model that describes the relationship between \\(Y_i\\) and \\(X_i\\) is given by \\[\nY_i = \\underbrace{X_i^T \\beta^\\star}_{\\text{best linear predictor}} + \\underbrace{e_i}_{\\text{projection error}},\n\\] where \\(e_i\\) captures the difference between the realized outcome and the best linear predictor.\nTo summarize, the conditional expectation function is the optimal predictor of an outcome given covariates, in the sense that it minimizes the mean squared prediction error. Since the CEF is typically a complex function, linear regression approximates it by selecting the linear combination of covariates that minimizes the mean squared error. The resulting linear predictor provides a simple model of the relationship between the outcome and the covariates."
  },
  {
    "objectID": "blog/linear-reg-rationale.html#footnotes",
    "href": "blog/linear-reg-rationale.html#footnotes",
    "title": "Linear Regression as an Approximation Method",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIn this post, I assume that the \\(n\\) observed data points represents the entire (finite) population. This simplification allows me to focus on the conceptual ideas behind linear regression without having to discuss statistical inference.↩︎\nAgain, to abstract away from statistical inference, I do not distinguish between population and sample averages. In practice, however, it is important to note that linear regression recovers sample conditional averages rather than the “true” population conditional averages when the CEF is linear.↩︎"
  },
  {
    "objectID": "blog/linear-reg-rationale.html#regression-is-about-conditional-averages",
    "href": "blog/linear-reg-rationale.html#regression-is-about-conditional-averages",
    "title": "Motivating Linear Regression",
    "section": "",
    "text": "For observations \\(i = 1, \\ldots, n\\), suppose there is a data generating process (DGP) that describes the joint distribution of a random outcome variable \\(Y_i\\) and a random vector \\(X_i =(X_{1i}, X_{2i}, \\ldots, X_{ki})\\) of covariates.1 We are interested in predicting \\(Y_i\\) given some realized values of the covariates, denoted by \\(x_i = (x_{1i}, x_{2i}, \\ldots, x_{ki})\\). Thus, any prediction can generally be expressed as a function \\(f(X_i)\\) of the covariates. As a starting point, we consider choosing \\(f\\) to minimize the mean squared error (MSE) between our predictions and the observed outcomes:\n\\[\n\\mathbb{E} [Y_i - f(X_i)]^2.\n\\] With the law of iterated expectation and some calculus, we can show that for every possible realized value \\(X_i = x_i\\), the optimal function that solves the minimum mean squared error problem is the conditional expectation function (CEF)\n\\[\nm(x) = \\mathbb{E}[Y_i | X_i = x_i].\n\\]\nIn most empirical cases, however, the functional form of the CEF is unknown. Thus, it is more practical to model the relationship between the outcome and covariates using a simpler function. One option is to find a linear approximation of the CEF, which takes the form\n\\[\n\\ell(x_i) = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + \\ldots + \\beta_k x_{ki} = x_i^T\\beta,\n\\] where \\(x_i = (1, x_{1i}, x_{2i}, \\ldots, x_{ki})\\). Note that the word “linear” here refers to the simplifying assumption that the function is a linear combination of the covariates. Since we want our predictions to minimize the MSE function, the best linear predictor is found by setting the coefficients as\n\\[\n\\beta^\\star = \\underset{\\beta}{\\arg\\min} \\, \\mathbb{E} [Y_i - (X_i^T \\beta)]^2 = \\mathbb{E}[X_i X_i^T]^{-1}\\mathbb{E}[X_iY_i].\n\\]\nThus, the linear regression model that describes the relationship between \\(Y_i\\) and \\(X_i\\) is given by \\[\nY_i = \\underbrace{X_i^T \\beta^\\star}_{\\text{best linear predictor}} + \\underbrace{e_i}_{\\text{projection error}},\n\\] where \\(e_i\\) captures the difference between the realized outcome and the best linear predictor.\nTo summarize, the conditional expectation function is the optimal predictor of an outcome given covariates, in the sense that it minimizes the mean squared prediction error. Since the CEF is typically a complex function, linear regression approximates it by selecting the linear combination of covariates that minimizes the mean squared error. The resulting linear predictor provides a simple model of the relationship between the outcome and the covariates."
  },
  {
    "objectID": "blog/linear-reg-rationale.html#the-conditional-expectation-function-cef-is-an-optimal-predictor",
    "href": "blog/linear-reg-rationale.html#the-conditional-expectation-function-cef-is-an-optimal-predictor",
    "title": "Linear Regression as an Approximation Method",
    "section": "",
    "text": "For observations \\(i = 1, \\ldots, n\\), suppose there is a data generating process (DGP) that describes the joint distribution of a random outcome variable \\(Y_i\\) and a random vector \\(X_i =(X_{1i}, X_{2i}, \\ldots, X_{ki})\\) of covariates.1 We are interested in predicting \\(Y_i\\) given some realized values of the covariates, denoted by \\(x_i = (x_{1i}, x_{2i}, \\ldots, x_{ki})\\). Thus, any prediction can generally be expressed as a function \\(f(X_i)\\) of the covariates. As a starting point, we consider choosing \\(f\\) to minimize the mean squared error (MSE) between our predictions and the observed outcomes:\n\\[\n\\mathbb{E} [Y_i - f(X_i)]^2.\n\\] With the law of iterated expectation and some calculus, we can show that for every possible realized value \\(X_i = x_i\\), the optimal function that solves the minimum mean squared error problem is the conditional expectation function (CEF)\n\\[\nm(x_i) = \\mathbb{E}[Y_i | X_i = x_i].\n\\tag{1}\\]"
  },
  {
    "objectID": "blog/linear-reg-rationale.html#regression-finds-the-best-linear-approximation-to-the-cef",
    "href": "blog/linear-reg-rationale.html#regression-finds-the-best-linear-approximation-to-the-cef",
    "title": "Motivating Linear Regression",
    "section": "Regression Finds the Best Linear Approximation to the CEF",
    "text": "Regression Finds the Best Linear Approximation to the CEF\nIn most empirical cases, however, the functional form of the CEF is unknown. Thus, it is more practical to model the relationship between the outcome and covariates using a simpler function. One option is to find a linear approximation of the CEF, which takes the form\n\\[\n\\ell(x_i) = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + \\ldots + \\beta_k x_{ki} = x_i^T\\beta,\n\\] where \\(x_i = (1, x_{1i}, x_{2i}, \\ldots, x_{ki})\\). Note that the word “linear” here refers to the simplifying assumption that the function is a linear combination of the covariates. Since we want our predictions to minimize the MSE function, the best linear predictor is found by setting the coefficients as\n\\[\n\\beta^\\star = \\underset{\\beta}{\\arg\\min} \\, \\mathbb{E} [Y_i - (X_i^T \\beta)]^2 = \\mathbb{E}[X_i X_i^T]^{-1}\\mathbb{E}[X_iY_i].\n\\]\nThus, the linear regression model that describes the relationship between \\(Y_i\\) and \\(X_i\\) is given by \\[\nY_i = \\underbrace{X_i^T \\beta^\\star}_{\\text{best linear predictor}} + \\underbrace{e_i}_{\\text{projection error}},\n\\] where \\(e_i\\) captures the difference between the realized outcome and the best linear predictor.\nTo summarize, the conditional expectation function is the optimal predictor of an outcome given covariates, in the sense that it minimizes the mean squared prediction error. Since the CEF is typically a complex function, linear regression approximates it by selecting the linear combination of covariates that minimizes the mean squared error. The resulting linear predictor provides a simple model of the relationship between the outcome and the covariates."
  },
  {
    "objectID": "blog/linear-reg-rationale.html#linear-regression-finds-the-best-linear-approximation-to-the-cef",
    "href": "blog/linear-reg-rationale.html#linear-regression-finds-the-best-linear-approximation-to-the-cef",
    "title": "Linear Regression as an Approximation Method",
    "section": "Linear Regression Finds the Best Linear Approximation to the CEF",
    "text": "Linear Regression Finds the Best Linear Approximation to the CEF\nIn most empirical cases, however, the functional form of the CEF is unknown. Thus, it is more practical to model the relationship between the outcome and covariates using a simpler function. One option is to find a linear approximation of the CEF, which takes the form\n\\[\n\\ell(x_i) = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + \\ldots + \\beta_k x_{ki} = x_i^T\\beta,\n\\tag{2}\\] where \\(x_i^T = (1, x_{1i}, x_{2i}, \\ldots, x_{ki})\\). Note that the word “linear” here refers to the simplifying assumption that the function is a linear combination of the covariates.\nHow do we choose the specific form of our approximating linear function? Since we want our predictions to minimize the MSE function, we set the parameters in Equation 2 as\n\\[\n\\beta^\\star = \\underset{\\beta}{\\arg\\min} \\, \\mathbb{E} [Y_i - (X_i^T \\beta)]^2.\n\\]\nIn words, we are choosing the linear function with the lowest MSE among all possible linear functions. The resulting linear predictor\n\\[x_i^T \\beta^\\star \\approx \\mathbb{E}[Y_i|X_i =x_i]\\]\nis often called the “linear regression of \\(Y\\) on \\(X\\)” by economists. This simply refers to the process of finding the best linear predictor of \\(Y_i\\) given some realized values of \\(X_i\\) by minimizing the mean squared error function."
  },
  {
    "objectID": "blog/linear-reg-rationale.html#linear-regression-can-sometimes-recover-true-conditional-averages",
    "href": "blog/linear-reg-rationale.html#linear-regression-can-sometimes-recover-true-conditional-averages",
    "title": "Linear Regression as an Approximation Method",
    "section": "Linear Regression Can (Sometimes) Recover True Conditional Averages",
    "text": "Linear Regression Can (Sometimes) Recover True Conditional Averages\nSo far, we have discussed linear regression as a tool to approximate the conditional expectation function. However, it can be shown that when the CEF is itself linear, the best linear predictor exactly equals the CEF. That is to say linear regression recovers the true conditional averages in this case.2\nTo see this in practice, I simulate data on income using the following equation\n\\[\n\\begin{align}\nincome_i &= \\beta_0 + \\beta_1 \\times white_i + \\beta_2 \\times male_i \\\\ & \\quad \\quad + \\beta_3 \\times literacy_i^{(1+0.3 \\times sex_i)} + \\beta_4 (male_i \\times black_i) + \\varepsilon_i.\n\\end{align}\n\\]\n\n\nShow the code that simulates the dataset.\nset.seed(123)\n\n# Population size \nn &lt;- 1000\n\n# Noise \nnoise &lt;- rnorm(n, 0, 2000)\n\n# Covariates \nmale &lt;- rbinom(n, 1, 0.5)\nsex &lt;- factor(male, labels = c(\"Female\", \"Male\"))\nwhite &lt;- rbinom(n, 1, 0.7)\nrace &lt;- factor(white, labels = c(\"Black\", \"White\"))\nliteracy &lt;- runif(n, 0, 100)\n\n# Outcome \nwhite_female_base &lt;- 30000                           # white females with 0 literacy earn 30000\nliteracy_effect &lt;- 50\nmale_premium &lt;- 200                                    # + 200 male premium \nwhite_premium &lt;- 1500                                  # + 1500 white premium\nwhite_male_premium &lt;- 1000                              # + 1000 white male premium \n\n# Create income \nincome &lt;- \n  white_female_base +\n  white_premium       * white +                          \n  male_premium        * male +                            \n  literacy_effect   * (literacy^(1 + 0.3 * male)) +    # literate males are better off than literate females \n  white_male_premium   * (male * white) +                  \n  noise\n\n# Combine into a dataset\nsim_data &lt;- data.frame(sex = sex,\n                     race = race,\n                     literacy,\n                     income)\n\n\n\nA Simple Univariate Example\nRecall from Equation 1 that the CEF is a function of the covariates we use to predict the outcome variable. So, as a simple example of a linear CEF, let us consider the conditional expectation of \\(income_i\\) as a function of \\(male_i\\). The conditional expectation can be written as the step-wise function \\[\n\\mathbb{E}[Y_i|male_i] = \\begin{cases} \\mu_0 \\quad  \\text{if} \\quad male_i = 0 \\\\ \\mu_1 \\quad  \\text{if} \\quad male_i = 1 \\end{cases}.\n\\tag{3}\\] To illustrate that Equation 3 is a linear combination of \\(male_i\\), we can rewrite it as\n\\[\n\\mathbb{E}[Y_i |male_i] = \\mu_0 + (\\mu_1-\\mu_0) \\times male_i.\n\\tag{4}\\]\nNow, we can use R to regress \\(income_i\\) on \\(sex_i\\) as follows.\n\nreg_model &lt;- feols(\n  income ~ sex, \n  data = sim_data\n)\n\nsummary(reg_model)\n\nOLS estimation, Dep. Var.: income\nObservations: 1,000\nStandard-errors: IID \n            Estimate Std. Error  t value  Pr(&gt;|t|)    \n(Intercept) 33423.97    207.948 160.7323 &lt; 2.2e-16 ***\nsexMale      7044.47    295.863  23.8099 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 4,673.0   Adj. R2: 0.361625\n\n\nTo verify that the linear regression above does in fact recover Equation 4, we need to check that (i) its intercept parameter equals the average income of females in the dataset and (ii) the coefficient on \\(male_i\\) is the difference in the average income of males and females in the dataset. As shown below, this is indeed the case.\n\nsim_data %&gt;%\n  filter(sex == \"Female\") %&gt;%\n  summarise(female_avg = mean(income, na.rm = TRUE))\n\n  female_avg\n1   33423.97\n\nsim_data %&gt;%\n  summarise(\n    sex_diff = mean(income[sex == \"Male\"], na.rm = TRUE) -\n                       mean(income[sex == \"Female\"], na.rm = TRUE)\n  )\n\n  sex_diff\n1 7044.472\n\n\n\n\nSaturated Regressions\nThe exercise of transforming the step-wise function in Equation 3 into a linear combination of the covariate in Equation 4 provides an important insight. It turns out that we can perform a similar transformation for any CEF where the covariates are discrete variables that take on a finite set of values.\nTODO: Add example of sex x race. Talk about main effect and interaction.\nIn general, the idea is to include a separate parameter for each possible value of the discrete covariates. The linear regression corresponding to such a CEF is said to be saturated."
  }
]