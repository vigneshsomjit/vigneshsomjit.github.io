[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "\nVignesh Somjit\n",
    "section": "",
    "text": "Vignesh Somjit\n\n\nI am a Research Professional at the University of Chicago Booth School of Business, where I work with Professor Richard Hornbeck. Before joining Booth, I received my B.A. in Economics (with honors) and B.A. in Mathematics from Boston University in 2025. I maintain a research blog, where I write about econometrics, causal inference, and machine learning."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "Vignesh Somjit",
    "section": "About Me",
    "text": "About Me\nI am a Research Professional at the University of Chicago Booth School of Business, where I work with Professor Richard Hornbeck. I received my B.A. in Economics and B.A. in Mathematics from Boston University in 2025."
  },
  {
    "objectID": "blog/blog-index.html",
    "href": "blog/blog-index.html",
    "title": "Vignesh Somjit",
    "section": "",
    "text": "An Introduction to Structural Equation Modeling\n\n\n\n\n\n\nStructural Equation Modeling\n\n\n\nI walk through the basics of structural equation modeling, focusing on model specification and estimation.\n\n\n\n\n\nSep 15, 2025\n\n\n\n\n\n\n\nRandom Sampling and Model-Based Inference\n\n\n\n\n\n\nMathematical Statistics\n\n\n\nIntuitive but detailed treatment of what statistical inference entails in the context of a random sampling framework.\n\n\n\n\n\nSep 15, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/regressions.html",
    "href": "blog/regressions.html",
    "title": "An Applied Econometrics Rationale for Linear Regression",
    "section": "",
    "text": "Regression Approximates Conditional Averages\nWe are interested in predicting the value of a random variable \\(Y_i\\), called the outcome, given some realized value of a random vector \\(X_i\\) of covariates. As a starting point, we let our predictions be a function of \\(X_i\\) and want our predictions to minimize the mean squared error (MSE) objective function\n\\[\n\\mathbb{E} [Y_i - f(X_i)]^2.\n\\] With the law of iterated expectation and some calculus, we can show that for every possible realized value \\(X_i = x\\), the optimal function that solves the minimum mean squared error problem is the conditional expectation function (CEF)\n\\[\n\\mu(x) = \\mathbb{E}[Y_i | X_i = x].\n\\]\nIn most empirical cases, however, the CEF of interest has no analytic form. Thus, it is more practical to model the relationship between the outcome and covariates using a simpler function. One option is to find a linear approximation of the CEF, which takes the form\n\\[\n\\ell(x) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_k x_k = x^T\\beta.\n\\]\nNote that the word “linear” here refers to the simplifying assumption that the function is a linear combination of the covariates. Since we want our predictions to minimize the MSE function, the best linear predictor is found by setting the coefficients as\n\\[\n\\beta^\\star = \\underset{\\beta}{\\arg\\min} \\, \\mathbb{E} [Y_i - (X_i^T \\beta)]^2 = \\mathbb{E}[X_i X_i^T]^{-1}\\mathbb{E}[X_iY_i].\n\\] Thus, the linear regression model that describes the relationship between \\(Y_i\\) and \\(X_i\\) is given by \\[\nY_i = \\underbrace{X_i^T \\beta^\\star}_{\\text{best linear predictor}} + \\underbrace{\\epsilon_i}_{\\text{residual}},\n\\] where \\(\\epsilon_i\\) captures the difference between the realized outcome and the best linear predictor.\nTo summarize, the conditional expectation function is the optimal predictor of an outcome given covariates, in the sense that it minimizes the mean squared prediction error. Since the CEF is typically a complex function, linear regression approximates it by selecting the linear combination of covariates that minimizes the mean squared error. The resulting linear predictor provides a simple model of the relationship between the outcome and the covariates.\n\n\nApproximate and Exact Conditional Averages\n\nset.seed(123)\n\n# Sample size \nn &lt;- 1000 \n\n# Covariates\nsex &lt;- rbinom(n, 1, 0.5)\nrace &lt;- rbinom(n, 1, 0.3)                   \n\n# Non-linear data generating process; no interactions\noutcome1 &lt;- 5 + 2^sex + 3^race \n\n# Non-linear data generating process; interactions \noutcome2 &lt;- 5 + 2^sex + 3^race + 1.5^(sex*race)\n\n# Consolidate in a dataset \ndf &lt;- data.frame(\n  outcome1 = outcome1,\n  outcome2 = outcome2,\n  sex = sex,\n  race = race\n)"
  },
  {
    "objectID": "blog/sem-intro.html",
    "href": "blog/sem-intro.html",
    "title": "An Introduction to Structural Equation Modeling",
    "section": "",
    "text": "Structural equation modeling (SEM) is a general statistical framework that unifies a range of techniques used to analyze the relationships among random variables within a single overarching methodology. The key philosophy behind SEM is to explicitly distinguish between the observed variables (the dataset) and the latent variables (the unobserved quantities of theoretical interest). This distinction allows researchers to specify and estimate more realistic models that capture the complex interdependencies between variables.\nTo ground this in a concrete example, consider the classic study presented in Bollen (1989) on the relationship between industrialization and political democracy. Both concepts are inherently abstract and cannot be directly measured, and so they are the latent variables in our analysis. To be able to evaluate this relationship empirically, we need to specify a set of observed indicators that serve as imperfect but informative signals of the latent variables. In this example, we use (i) gross national product (GNP) per capita, (ii) energy consumption per capita, and (iii) the percentage of labor force in industry as indicators of industrialization. For political democracy, we use expert ratings of the (i) freedom of the press, (ii) freedom of political opposition, (iii) fairness of elections, and (iv) effectiveness of the elected legislature. We will see how SEM allows us to infer the relationship between industrialization and political democracy through these observed indicators."
  },
  {
    "objectID": "blog/sem-intro.html#structural-equation-modeling-a-way-of-thinking",
    "href": "blog/sem-intro.html#structural-equation-modeling-a-way-of-thinking",
    "title": "An Introduction to Structural Equation Modeling",
    "section": "",
    "text": "Structural equation modeling (SEM) is a general statistical framework that unifies a range of techniques used to analyze the relationships among random variables within a single overarching methodology. The key philosophy behind SEM is to explicitly distinguish between the observed variables (the dataset) and the latent variables (the unobserved quantities of theoretical interest). This distinction allows researchers to specify and estimate more realistic models that capture the complex interdependencies between variables.\nTo ground this in a concrete example, consider the classic study presented in Bollen (1989) on the relationship between industrialization and political democracy. Both concepts are inherently abstract and cannot be directly measured, and so they are the latent variables in our analysis. To be able to evaluate this relationship empirically, we need to specify a set of observed indicators that serve as imperfect but informative signals of the latent variables. In this example, we use (i) gross national product (GNP) per capita, (ii) energy consumption per capita, and (iii) the percentage of labor force in industry as indicators of industrialization. For political democracy, we use expert ratings of the (i) freedom of the press, (ii) freedom of political opposition, (iii) fairness of elections, and (iv) effectiveness of the elected legislature. We will see how SEM allows us to infer the relationship between industrialization and political democracy through these observed indicators."
  },
  {
    "objectID": "blog/sem-intro.html#the-dual-architecture-of-structural-equation-models",
    "href": "blog/sem-intro.html#the-dual-architecture-of-structural-equation-models",
    "title": "An Introduction to Structural Equation Modeling",
    "section": "The Dual Architecture of Structural Equation Models",
    "text": "The Dual Architecture of Structural Equation Models\nAt its core, a structural equation model is simply a set of equations that consist of (i) random variables and (ii) parameters that describe the relationships between those variables. The distinction between observed and latent variables motivates the decomposition of a structural equation model into two main subsystems: the (i) latent variable model and the (ii) measurement model. I discuss each of these subsystems in turn.\n\nLatent Variable Model\n\n\nMeasurement Model"
  },
  {
    "objectID": "blog/sem-intro.html#a-dual-system-of-equations",
    "href": "blog/sem-intro.html#a-dual-system-of-equations",
    "title": "An Introduction to Structural Equation Modeling",
    "section": "A Dual System of Equations",
    "text": "A Dual System of Equations\nAt its core, a structural equation model is simply a set of equations that consist of (i) random variables and (ii) parameters that describe the relationships between those variables. The distinction between observed and latent variables motivates the decomposition of a structural equation model into two main subsystems: the latent variable model and the measurement model. I discuss each of these subsystems in turn.\n\nLatent Variable Model\nThe latent variable model is the set of structural equations that describe the relationships between the latent variables in the model. Specifically, the model is given by\n\\[\n\\boldsymbol{\\eta}= \\mathrm{B} \\boldsymbol{\\eta} + \\Gamma \\boldsymbol{\\xi} + \\boldsymbol{\\zeta}\n\\]\nwhere \\(\\boldsymbol{\\eta} \\in \\mathbb{R}^{m \\times 1}\\) is a vector of endogenous latent variables (i.e. determined by variables in the model), \\(\\boldsymbol{\\xi} \\in \\mathbb{R}^{n \\times 1}\\) is a vector of exogenous latent variables (i.e. determined outside the model), \\(\\mathrm{B} \\in \\mathbb{R}^{m \\times m}\\) is a matrix of coefficients that describe the relationships among the endogenous latent variables, \\(\\Gamma \\in \\mathbb{R}^{n \\times n}\\) is a matrix of coefficients that describe the relationships between the exogenous and endogenous latent variables, and \\(\\boldsymbol{\\zeta} \\in \\mathbb{R}^{m \\times 1}\\) is a vector of error terms that capture the remaining unexplained variation in the endogenous latent variables.\n\n\nMeasurement Model\nThe measurement model is the set of equations that describe the relationships between the observed and latent variables in the model. The model is given by\n\\[\n\\mathbf{y} = \\Lambda_y \\boldsymbol{\\eta} + \\boldsymbol{\\epsilon} \\quad \\text{and} \\quad \\mathbf{x} = \\Lambda_x \\boldsymbol{\\xi} + \\boldsymbol{\\delta}\n\\]"
  },
  {
    "objectID": "blog/sem-intro.html#estimation",
    "href": "blog/sem-intro.html#estimation",
    "title": "An Introduction to Structural Equation Modeling",
    "section": "Estimation",
    "text": "Estimation\n\nAn Emphasis On Covariance Structures\nTODO"
  },
  {
    "objectID": "blog/sem-intro.html#estimating-the-model-parameters",
    "href": "blog/sem-intro.html#estimating-the-model-parameters",
    "title": "An Introduction to Structural Equation Modeling",
    "section": "Estimating the Model Parameters",
    "text": "Estimating the Model Parameters\n\nEstimation Principle\nRecall that in the linear regression model,\n\n\nMaximum Likelihood Estimation\nTODO."
  },
  {
    "objectID": "blog/sem-intro.html#regression-as-a-structural-equation-model",
    "href": "blog/sem-intro.html#regression-as-a-structural-equation-model",
    "title": "An Introduction to Structural Equation Modeling",
    "section": "Regression as a Structural Equation Model",
    "text": "Regression as a Structural Equation Model\nTODO."
  },
  {
    "objectID": "blog/sem-intro.html#implied-covariance-matrix",
    "href": "blog/sem-intro.html#implied-covariance-matrix",
    "title": "An Introduction to Structural Equation Modeling",
    "section": "Implied Covariance Matrix",
    "text": "Implied Covariance Matrix\nA key feature of SEM is its emphasis on the covariance structure of the observed variables in the model. Specifically, we are interested in the implied (joint) covariance matrix \\[\n\\Sigma(\\boldsymbol{\\theta}) = \\begin{pmatrix} \\Sigma_{\\mathbf{yy}}(\\boldsymbol{\\theta}) & \\Sigma_{\\mathbf{yx}}(\\boldsymbol{\\theta}) \\\\ \\Sigma_{\\mathbf{xy}}(\\boldsymbol{\\theta}) & \\Sigma_{\\mathbf{xx}}(\\boldsymbol{\\theta}) \\end{pmatrix} \\in \\mathbb{R}^{(p+q) \\times (p+q)},\n\\tag{1}\\]\nwhere \\(\\boldsymbol{\\theta}\\) is a vector of all the model parameters, \\(\\Sigma_{\\mathbf{yy}}(\\boldsymbol\\theta)\\) is the covariance matrix of the observed endogenous variables written in terms of the model parameters, \\(\\Sigma_{\\mathbf{yx}}(\\boldsymbol\\theta)\\) is the cross covariance of the observed endogenous and exogenous variables, \\(\\Sigma_{\\mathbf{xy}}(\\boldsymbol\\theta)\\) is the transpose of the cross covariance matrix, and \\(\\Sigma_{\\mathbf{xx}}(\\boldsymbol\\theta)\\) is the covariance of the observed exogenous variables. Intuitively, the quantity \\(\\Sigma(\\boldsymbol\\theta)\\) captures the relationships between the observed variables under the assumptions (i.e. restrictions) imposed by the structural equation model.\nThe derivation of \\(\\Sigma(\\boldsymbol\\theta)\\) is somewhat tedious, but can be found in Bollen (1989). The essence is that for each element in Equation 1, (i) we substitute the measurement model equations into the definition of the covariance matrix, and (ii) we use the reduced-form version of the endogenous variables to simplify the expressions. This results in the following expression for the implied covariance matrix:\n\\[\n\\Sigma(\\boldsymbol{\\theta}) = \\begin{pmatrix} \\Lambda_{\\mathbf{y}} (\\mathrm{I} - \\mathrm{B})^{-1} (\\Gamma \\Phi \\Gamma' + \\Psi) (\\mathrm{I} - \\mathrm{B})^{-1'} \\Lambda_{\\mathbf{y}}' + \\Theta_{\\boldsymbol\\epsilon} & \\Lambda_{\\mathbf{y}} (\\mathrm{I} - B)^{-1} \\Gamma \\Phi \\Lambda_{\\mathbf{x}}' \\\\ \\Lambda_{\\mathbf{x}} \\Phi \\Gamma' (\\mathrm{I} - \\mathrm{B})^{-1'} \\Lambda_{\\mathbf{y}}' & \\Lambda_{\\mathbf{x}} \\Phi \\Lambda_{\\mathbf{x}}' + \\Theta_{\\boldsymbol{\\delta}} \\end{pmatrix}.\n\\tag{2}\\]\nThe main takeaway here is that we can always write the covariance structure of the observed variables as a function of the model parameters. As we will see, this fact is crucial for estimating the model parameters.\nThe implied joint covariance matrix for the industrialization and political democracy example is a \\(11 \\times 11\\) matrix that can be found by substituting the specific parameter matrices into Equation 2."
  },
  {
    "objectID": "blog/sem-intro.html#implied-joint-covariance-matrix",
    "href": "blog/sem-intro.html#implied-joint-covariance-matrix",
    "title": "An Introduction to Structural Equation Modeling",
    "section": "Implied Joint Covariance Matrix",
    "text": "Implied Joint Covariance Matrix\nA key feature of SEM is its emphasis on the covariance structure of the observed variables in the model. Specifically, we are interested in the implied joint covariance matrix \\[\n\\Sigma(\\boldsymbol{\\theta}) = \\begin{pmatrix} \\Sigma_{\\mathbf{yy}}(\\boldsymbol{\\theta}) & \\Sigma_{\\mathbf{yx}}(\\boldsymbol{\\theta}) \\\\ \\Sigma_{\\mathbf{xy}}(\\boldsymbol{\\theta}) & \\Sigma_{\\mathbf{xx}}(\\boldsymbol{\\theta}) \\end{pmatrix} \\in \\mathbb{R}^{(p+q) \\times (p+q)},\n\\tag{1}\\]\nwhere \\(\\boldsymbol{\\theta}\\) is a vector of all the model parameters, \\(\\Sigma_{\\mathbf{yy}}(\\boldsymbol\\theta)\\) is the covariance matrix of the observed endogenous variables written in terms of the model parameters, \\(\\Sigma_{\\mathbf{yx}}(\\boldsymbol\\theta)\\) is the cross covariance of the observed endogenous and exogenous variables, \\(\\Sigma_{\\mathbf{xy}}(\\boldsymbol\\theta)\\) is the transpose of the cross covariance matrix, and \\(\\Sigma_{\\mathbf{xx}}(\\boldsymbol\\theta)\\) is the covariance of the observed exogenous variables. Intuitively, the quantity \\(\\Sigma(\\boldsymbol\\theta)\\) captures the relationships between the observed variables under the assumptions (i.e. restrictions) imposed by the structural equation model.\nThe derivation of \\(\\Sigma(\\boldsymbol\\theta)\\) is somewhat tedious, but can be found in Bollen (1989). The essence is that for each element in Equation 1, (i) we substitute the measurement model equations into the definition of the covariance matrix, and (ii) we use the reduced-form version of the endogenous variables to simplify the expressions. This results in the following expression for the implied covariance matrix:\n\\[\n\\Sigma(\\boldsymbol{\\theta}) = \\begin{pmatrix} \\Lambda_{\\mathbf{y}} (\\mathrm{I} - \\mathrm{B})^{-1} (\\Gamma \\Phi \\Gamma' + \\Psi) (\\mathrm{I} - \\mathrm{B})^{-1'} \\Lambda_{\\mathbf{y}}' + \\Theta_{\\boldsymbol\\epsilon} & \\Lambda_{\\mathbf{y}} (\\mathrm{I} - B)^{-1} \\Gamma \\Phi \\Lambda_{\\mathbf{x}}' \\\\ \\Lambda_{\\mathbf{x}} \\Phi \\Gamma' (\\mathrm{I} - \\mathrm{B})^{-1'} \\Lambda_{\\mathbf{y}}' & \\Lambda_{\\mathbf{x}} \\Phi \\Lambda_{\\mathbf{x}}' + \\Theta_{\\boldsymbol{\\delta}} \\end{pmatrix}.\n\\tag{2}\\]\nThe main takeaway here is that we can always write the covariance structure of the observed variables as a function of the model parameters. For example, the implied joint covariance matrix for the industrialization and political democracy study is a \\(11 \\times 11\\) matrix that can be found by substituting the specific parameter matrices into Equation 2."
  },
  {
    "objectID": "blog/random-sampling.html",
    "href": "blog/random-sampling.html",
    "title": "Random Sampling and Model-Based Inference",
    "section": "",
    "text": "Observational data is inherently random: if we were to measure the same variables repeatedly, we would almost certainly get different values each time. A classical explanation for the source of this randomness is the model-based (or sampling-based) perspective. In this abstraction, we assume there exists an underlying superpopulation or data generating process (DGP): a fixed but unknown probability distribution \\(F\\) that can, in principle, generate infinitely many observations. Randomness then arises because we only observe finitely many realizations from \\(F\\) — never the entire distribution. Within this framework, we define statistical inference as the process of using the observed random data to estimate features of \\(F\\) and quantify the uncertainty in those estimates.\nNotice that the exposition above implicitly assumes that there is a single underlying distribution \\(F\\) from which all observed data are drawn. To make this mathematically precise, we need to introduce the random sampling assumption. This provides the framework to connect the observed data to the DGP, and thereby lays the foundation for statistical inference."
  },
  {
    "objectID": "blog/random-sampling.html#mathematical-formalization-of-observational-datasets",
    "href": "blog/random-sampling.html#mathematical-formalization-of-observational-datasets",
    "title": "Random Sampling Framework",
    "section": "",
    "text": "Cross-sectional datasets consist of several observations of a collection of variables for a given point in time and can be denoted as\n\\[\n\\{x_{1i}, x_{2i}, \\ldots, x_{Ki}\\}_{i=1}^n,\n\\]\nwhere \\(x_{ki}\\) is the value of the \\(k\\)-th variable for the \\(i\\)-th observation. A typical source of cross-sectional data in economics is through surveys like the Current Population Survey (CPS) or the American Community Survey (ACS).\nWe are interested in making statements about the underlying process that generates the dataset. In statistics, we do this by mathematically formalizing the way the data was generated. Specifically, we view each observational vector \\[\n\\boldsymbol{x}_i = (x_{1i}, \\ldots, x_{Ki})' \\in \\mathbb{R}^k \\quad \\text{for } i = 1, \\ldots, n,\n\\] as a realization of the random vector \\[\n\\boldsymbol X_i = (X_{1i}, \\ldots, X_{K_i})' \\in \\mathbb{R}^k \\quad \\text{for } i = 1, \\ldots, n,\n\\]\nwith some associated probability distribution. For example, we can think of the CPS dataset as consisting of random vectors\n\\[\n\\boldsymbol X_i = (sex_i, age_i, educ_i, wage_i)' \\quad \\text{for } i = 1, \\ldots, n.\n\\] For some specific individual \\(i\\) in the CPS dataset, we then observe the vector of realized values1\n\\[\n\\boldsymbol x_i = (1, 25, 16, 25000)'.\n\\] Intuitively, the distinction between the random vector \\(\\boldsymbol X_i\\) and the realization \\(\\boldsymbol x_i\\) is that the former represents the \\(i\\)-th observation before viewing the data (unknown and random) and the latter represents the \\(i\\)-th observation after viewing the data (specific known value)."
  },
  {
    "objectID": "blog/random-sampling.html#random-sampling-assumption",
    "href": "blog/random-sampling.html#random-sampling-assumption",
    "title": "Random Sampling Framework",
    "section": "Random Sampling Assumption",
    "text": "Random Sampling Assumption\nThe assumption of random sampling is a characterization of the probability distribution of the random vectors \\(\\boldsymbol X_1, \\ldots, \\boldsymbol X_n\\).\n\nRandom Sampling Assumption. The random vectors \\(\\boldsymbol X_1, \\ldots , \\boldsymbol X_n\\) are mutually independent and have the same probability distribution \\(F(X_1, \\ldots, X_K)\\). Equivalently, we say that the random vectors \\(\\boldsymbol X_1, \\ldots , \\boldsymbol X_n\\) are independent and identically distributed (iid) with distribution \\(F\\).\n\nThe random sampling framework is one potential way to characterize the dependence structure across the observed data points. It is popular because (i) it is often reasonable when working with cross-sectional datasets2, and (ii) it is the backbone of several statistical theorems and methods.3\n\nAlternative Sampling Assumptions"
  },
  {
    "objectID": "blog/random-sampling.html#footnotes",
    "href": "blog/random-sampling.html#footnotes",
    "title": "Random Sampling and Model-Based Inference",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nHere, the value of \\(1\\) refers to the individual being a male and \\(16\\) means \\(16\\) years of education.↩︎\nFor example, if we collected data on a random subset of individuals from a large common population (e.g. the USA), it is reasonable to assume that the characteristics of one individual are independent of another individual and that they all come from the same population distribution.↩︎\nCrucial theorems in asymptotic statistical theory, like the Law of Large Numbers and the Central Limit Theorem, require the random sampling assumption to hold.↩︎\nTechnically, under iid sampling, the empirical distribution converges to the true distribution as the sample size tends to infinity — a result known as the Glivenko–Cantelli theorem. Nevertheless, in any finite sample, uncertainty remains, and with it the need for statistical inference.↩︎\nTo quote my Mathematical Statistics Professor Daniel Weiner: “Do to the sample to get your estimator, as you would do to your population to get your estimand.”↩︎"
  },
  {
    "objectID": "blog/random-sampling.html#parameters-and-statistics",
    "href": "blog/random-sampling.html#parameters-and-statistics",
    "title": "Random Sampling Framework",
    "section": "Parameters and Statistics",
    "text": "Parameters and Statistics"
  },
  {
    "objectID": "blog/random-sampling.html#sampling-distribution",
    "href": "blog/random-sampling.html#sampling-distribution",
    "title": "Random Sampling Framework",
    "section": "Sampling Distribution",
    "text": "Sampling Distribution"
  },
  {
    "objectID": "blog/random-sampling.html#estimands-estimators-and-sampling-distribution",
    "href": "blog/random-sampling.html#estimands-estimators-and-sampling-distribution",
    "title": "Random Sampling Framework",
    "section": "Estimands, Estimators, and Sampling Distribution",
    "text": "Estimands, Estimators, and Sampling Distribution\nAn estimand \\(\\theta\\) is a function of the data generating process \\(F\\). For example, the population mean of the \\(k\\)-th variable is an estimand defined as\n\\[\n\\mu_k = \\mathbb{E}_F[ X_k].\n\\] However, we do not know the data generating process \\(F\\) and therefore cannot calculate any estimand. Instead, we make certain simplifying assumptions about the general structure of \\(F\\) and then use the observed data to guess the value of the estimand. More formally, an estimator is a function that maps the sample to an estimand under the assumptions we make about \\(F\\). For example, the sample mean of the \\(k\\)-th variable is an estimator for the population mean:\n\\[\n\\hat{\\mu}_k = \\frac{1}{n} \\sum_{i=1}^n x_{ki}.\n\\]"
  },
  {
    "objectID": "blog/random-sampling.html#estimands-and-estimators",
    "href": "blog/random-sampling.html#estimands-and-estimators",
    "title": "Model-Based Inference Under Random Sampling",
    "section": "Estimands and Estimators",
    "text": "Estimands and Estimators\nAn estimand \\(\\theta\\) is a function of the data generating process \\(F\\). For example, the the population mean of a random variable \\(X\\) \\[\n\\mu = \\mathbb{E}_F[X]\n\\] is an estimand. However, we do not know the data generating process \\(F\\) and therefore cannot calculate any estimand. Instead, we use the observed data to guess the value of the estimand. An estimator \\(\\hat \\theta\\) is a function of the sample that provides a “good guess” for \\(\\theta\\).\nIn statistics, there are several estimation methods that provide systematic ways (i.e. rules) to construct estimators. One common method is the analog principle (or plug-in principle). The idea is to construct the estimator by replacing the population quantities in the estimand with their sample analogs.4 Thus, the sample mean is the analog estimator for the population mean:\n\\[\n\\hat{\\mu} = \\frac{1}{n} \\sum_{i=1}^n X_i.\n\\]\n\nSampling Distribution\nHow do we define a good guess? How do we choose between two different estimators?\nwe make certain simplifying assumptions about the general structure of \\(F\\) and then\nWe will be able to quantify the uncertainty of the guess if we make stronger starting assumptions about the data generating process."
  },
  {
    "objectID": "blog/random-sampling.html#random-sampling",
    "href": "blog/random-sampling.html#random-sampling",
    "title": "Random Sampling Framework",
    "section": "Random Sampling",
    "text": "Random Sampling\nThe mathematical formalization of the dataset now allows us to make a connection between the observed data and the underlying process that generates it. Specifically, we establish this connection by characterizing the probability distribution of each random vector \\(\\boldsymbol X_1, \\ldots, \\boldsymbol X_n\\).\nThe simplest characterization is the random sampling framework. Here, we assume that the random vectors \\(\\boldsymbol X_1, \\ldots , \\boldsymbol X_n\\) are independent and identically distributed (iid) with some common but unknown distribution \\(F(X_1, \\ldots, X_K)\\). Notice that \\(F\\) is exactly the underlying process we want to learn about, and is thus aptly called the data generating process (DGP). We also sometimes refer to \\(F\\) as the population. This captures the idea that the dataset is a random subset of some infinitely large population.\nIt’s useful to explicitly clarify the notation being used above. The random variables \\(X_1, \\ldots, X_K\\) are mathematical objects used to represent the generic or population-level variable associated with the DGP. For example, the random variable \\(wage\\) represents the generic variable for wage in the population. The random vector \\(\\boldsymbol X_i = (X_{i1}, \\ldots, X_{iK})\\) are mathematical objects used to represent the sample-level variable associated with specific units. For example, the random variable \\(wage_i\\) represents the wage of individual \\(i\\) before the data is observed. Lastly, the realizations \\(\\boldsymbol{x}_i = (x_{i1}, \\ldots, x_{iK})\\) are the actual observed values of the sample-level random variables. For example, the value \\(wage_i = 25000\\) is the observed wage of individual \\(i\\) after the data is observed.\n\nAlternative Sampling Assumptions\nThe random sampling assumption is one potential way to characterize the dependence structure across the observed data points. It is popular because (i) it is often reasonable when working with cross-sectional datasets2, and (ii) it is the backbone of several statistical theorems and methods.3"
  },
  {
    "objectID": "blog/random-sampling.html#random-samples",
    "href": "blog/random-sampling.html#random-samples",
    "title": "Random Sampling Framework",
    "section": "Random Samples",
    "text": "Random Samples\nThe random sampling assumption is one way to characterize the probability distribution of the random vectors \\(\\boldsymbol X_1, \\ldots, \\boldsymbol X_n\\). The definition below helps formalize the idea.\n\nDefinition. The random vectors \\(\\{\\boldsymbol X_1, \\ldots , \\boldsymbol X_n\\}\\) are called a random sample from the distribution \\(F\\) if they are mutually independent and have the same probability distribution \\(F\\). Equivalently, we say that the random vectors \\(\\{\\boldsymbol X_1, \\ldots , \\boldsymbol X_n\\}\\) are independent and identically distributed (iid) with distribution \\(F\\).\n\nThe random sampling framework asserts that our observed dataset is a random sample from some common, but unknown distribution \\(F\\). The distribution \\(F\\) is often called the data generating process or the population.2\n\nAlternative Sampling Assumptions\nThe random sampling assumption is one potential way to characterize the dependence structure across the observed data points. It is popular because (i) it is often reasonable when working with cross-sectional datasets[^4], and (ii) it is the backbone of several statistical theorems and methods.3"
  },
  {
    "objectID": "blog/random-sampling.html#random-sampling-framework",
    "href": "blog/random-sampling.html#random-sampling-framework",
    "title": "Random Sampling and Model-Based Inference",
    "section": "Random Sampling Framework",
    "text": "Random Sampling Framework\nThe discussion going forward will primarily focus on cross-sectional datasets. These consist of several observations of a collection of variables for a given point in time and can be denoted as\n\\[\n\\{x_{i1}, x_{i2} \\ldots, x_{iK}\\}_{i=1}^n,\n\\]\nwhere \\(x_{ik}\\) is the value of the \\(k\\)-th variable for the \\(i\\)-th unit. A typical source of cross-sectional data in economics is through surveys like the Current Population Survey (CPS) or the American Community Survey (ACS).\n\nMathematical Formalization of the Dataset\nSince observational data is random, it is natural to we view the observed data vectors \\[\n\\boldsymbol{x}_i = (x_{i1}, \\ldots, x_{iK})' \\in \\mathbb{R}^K \\quad \\text{for } i = 1, \\ldots, n,\n\\] as a realization of the random vector \\[\n\\boldsymbol X_i = (X_{i1}, \\ldots, X_{iK})' \\in \\mathbb{R}^K \\quad \\text{for } i = 1, \\ldots, n,\n\\]\nwith some associated probability distribution. For example, we can think of the CPS dataset as consisting of random vectors\n\\[\n\\boldsymbol X_i = (sex_i, age_i, educ_i, wage_i)' \\quad \\text{for } i = 1, \\ldots, n.\n\\] For some specific individual \\(i\\) in the CPS dataset, we then observe the vector of realized values1\n\\[\n\\boldsymbol x_i = (1, 25, 16, 25000)'.\n\\] Intuitively, the distinction between the random vector \\(\\boldsymbol X_i\\) and the realization \\(\\boldsymbol x_i\\) is that the former represents the \\(i\\)-th observation before viewing the data (unknown and random) and the latter represents the \\(i\\)-th observation after viewing the data (specific known value).\n\n\nRandom Sampling\nWe have now represented the dataset as a collection of random vectors \\(\\boldsymbol X_1, \\ldots, \\boldsymbol X_n\\), each with some probability distribution \\(F_1 \\ldots, F_n\\). However, the connection between the underlying DGP and the observed data remains unclear. Specifically, we need a simplifying assumption that will allow us to connect the distributions of each \\(\\boldsymbol X_i\\) to the underlying DGP in a straightforward manner. The simplest framework is that of random sampling. Here, we assume that the random vectors \\(\\boldsymbol X_1, \\ldots , \\boldsymbol X_n\\) are independent and identically distributed (iid) with some common but unknown distribution \\(F\\) on \\(\\mathbb{R}^K\\). Under this assumption, the data generating process \\(F\\) is precisely the distribution that governs each individual random vector \\(\\boldsymbol X_i\\).\n\nAlternative Assumptions\nThe random sampling assumption is one potential way to characterize the dependence structure across the observed data points. It is popular because (i) it is often reasonable when working with cross-sectional datasets2, and (ii) it is the backbone of several statistical theorems and methods.3 However, it does not necessarily have to hold. For example, we often work with data where the units are connected via some underlying factor (location, industry, etc.). In such cases, the assumption of independence across individual units is violated. An alternative approach in such cases is to instead assume mutual independence across clusters of units. Another example of a violation to the independence assumption is time-series data, where the individual unit is indexed by time. Here, consecutive observations are usually correlated and independence is instead formulated in terms of stationarity and other concepts outside the scope of this post.\n\n\n\nNotation and Example\nThe discussion so far has purely been conceptual, and so it is useful to consider a concrete example. Before doing so, however, let’s clarify some notation used in the random sampling framework. We denote the population-level random variables as \\(X_1, \\ldots, X_K\\). The data generating process \\(F\\) is the joint distribution of these random variables. For example, \\(wage\\) denotes the generic random variable for wage in the entire US population and we are interested in making inferences about its distribution. We denote the sample-level random variables as \\(X_{i1}, \\ldots, X_{iK}\\). These represent the random variables associated with specific units in the sample. For example, \\(wage_i\\) denotes the random variable representing the wage of individual \\(i\\) in the sample. Finally, we denote the realized observations as \\(X_{i1}=x_{i1}, \\ldots, X_{iK}=x_{iK}\\). These are the actual values we observe. For example, \\(wage_i = 25,000\\) means that the observed wage of individual \\(i\\) is \\(25,000\\).\nNow suppose our data generating process consists of one random variable with an exponential distribution with scale parameter \\(\\beta = 1\\): \\[\nX \\sim \\text{Exp}(1) \\quad \\text{with density } f(x) = e^{-x} \\text{ for } x \\geq 0.\n\\]\nIf we assume our dataset is a random sample of size 30 from the above DGP, then \\[\nX_{i} \\sim \\text{Exp}(1) \\, \\, \\text{ for } i = 1, \\ldots, 30 \\quad  \\text{and} \\quad  X_i \\perp X_j  \\, \\, \\text{ for } i \\neq j.\n\\]\nThe figure below plots the empirical kernel density of such a random sample (black line) and the true density of the exponential distribution (red line). In practice, we do not know the true DGP and need to guess its characteristics using the observed data. Randomness in the finite observed data makes this a non-trivial task — as illustrated by the discrepancy between empirical and true densities in the figure below.4\n\n\nCode\n# Simulate IID sample of 30 obs from exp(1)\nset.seed(123)\nn &lt;- 30\nx &lt;- rexp(n, rate = 1)\n\n# Empirical Density \ndens &lt;- density(x)\n\n# True Exponential Density\nxs &lt;- seq(0, max(x), length.out = 200)\nys &lt;- dexp(xs, rate = 1)\n\n# Plot\nplot(dens, main = \"\", xlab = \"Observed Data\", ylab = \"Density\", xlim = c(0, 5), ylim = c(0, max(c(dens$y, ys))))\ncurve(dexp(x, rate = 1), from = min(x), add = TRUE, col = \"red\", lwd = 2)"
  },
  {
    "objectID": "blog/random-sampling.html#mathematical-formalization-of-observational-data",
    "href": "blog/random-sampling.html#mathematical-formalization-of-observational-data",
    "title": "Random Sampling Framework",
    "section": "Mathematical Formalization of Observational Data",
    "text": "Mathematical Formalization of Observational Data\nTo keep things concrete, the rest of this post will focus on cross-sectional datasets. These consist of several observations of a collection of variables for a given point in time and can be denoted as\n\\[\n\\{x_{i1}, x_{i2} \\ldots, x_{iK}\\}_{i=1}^n,\n\\]\nwhere \\(x_{ik}\\) is the value of the \\(k\\)-th variable for the \\(i\\)-th unit. A typical source of cross-sectional data in economics is through surveys like the Current Population Survey (CPS) or the American Community Survey (ACS).\nLet us now mathematically formalize the connection between the observed dataset and the underlying DGP \\(F\\). Specifically, we view each observational vector \\[\n\\boldsymbol{x}_i = (x_{i1}, \\ldots, x_{iK})' \\in \\mathbb{R}^K \\quad \\text{for } i = 1, \\ldots, n,\n\\] as a realization of the random vector \\[\n\\boldsymbol X_i = (X_{i1}, \\ldots, X_{iK})' \\in \\mathbb{R}^K \\quad \\text{for } i = 1, \\ldots, n,\n\\]\nwith some associated probability distribution. For example, we can think of the CPS dataset as consisting of random vectors\n\\[\n\\boldsymbol X_i = (sex_i, age_i, educ_i, wage_i)' \\quad \\text{for } i = 1, \\ldots, n.\n\\] For some specific individual \\(i\\) in the CPS dataset, we then observe the vector of realized values1\n\\[\n\\boldsymbol x_i = (1, 25, 16, 25000)'.\n\\] Intuitively, the distinction between the random vector \\(\\boldsymbol X_i\\) and the realization \\(\\boldsymbol x_i\\) is that the former represents the \\(i\\)-th observation before viewing the data (unknown and random) and the latter represents the \\(i\\)-th observation after viewing the data (specific known value)."
  },
  {
    "objectID": "blog/random-sampling.html#radnom-sampling",
    "href": "blog/random-sampling.html#radnom-sampling",
    "title": "Random Sampling Framework",
    "section": "Radnom Sampling",
    "text": "Radnom Sampling\nThe random sampling assumption is one way to characterize the probability distribution of the random vectors \\(\\boldsymbol X_1, \\ldots, \\boldsymbol X_n\\). Specifically, it asserts that the random vectors \\(\\boldsymbol X_1, \\ldots , \\boldsymbol X_n\\) are independent and identically distributed (iid) with some common but unknown distribution \\(F(X_1, \\ldots, X_K)\\). The distribution \\(F\\) is often called the data generating process (DGP) because if we knew the distribution, then we could reproduce the dataset by drawing \\(n\\) independent samples from \\(F\\). We also refer to \\(F\\) as the population. This captures the intuition that the dataset is a random subset of some infinitely large population.\nIt’s easy to get confused about the notation and terminology here, so let’s clarify.\nIn this setting, making statements about the underlying process that generates the dataset is now equivalent to learning about the distribution \\(F\\) using the sample. This idea is the basis of statistical inference and is briefly explored in the next section.\n\nAlternative Sampling Assumptions\nThe random sampling assumption is one potential way to characterize the dependence structure across the observed data points. It is popular because (i) it is often reasonable when working with cross-sectional datasets[^4], and (ii) it is the backbone of several statistical theorems and methods.2"
  },
  {
    "objectID": "blog/random-sampling.html#what-is-statistical-inference",
    "href": "blog/random-sampling.html#what-is-statistical-inference",
    "title": "Random Sampling and Model-Based Inference",
    "section": "",
    "text": "Observational data is inherently random: if we were to measure the same variables repeatedly, we would almost certainly get different values each time. A classical explanation for the source of this randomness is the model-based (or sampling-based) perspective. In this abstraction, we assume there exists an underlying superpopulation or data generating process (DGP): a fixed but unknown probability distribution \\(F\\) that can, in principle, generate infinitely many observations. Randomness then arises because we only observe finitely many realizations from \\(F\\) — never the entire distribution. Within this framework, we define statistical inference as the process of using the observed random data to estimate features of \\(F\\) and quantify the uncertainty in those estimates.\nNotice that the exposition above implicitly assumes that there is a single underlying distribution \\(F\\) from which all observed data are drawn. To make this mathematically precise, we need to introduce the random sampling assumption. This provides the framework to connect the observed data to the DGP, and thereby lays the foundation for statistical inference."
  },
  {
    "objectID": "blog/random-sampling.html#random-sampling-framework-for-cross-sectional-data",
    "href": "blog/random-sampling.html#random-sampling-framework-for-cross-sectional-data",
    "title": "Random Sampling and Model-Based Inference",
    "section": "Random Sampling Framework for Cross-Sectional Data",
    "text": "Random Sampling Framework for Cross-Sectional Data\n\nMathematical Formalization of the Dataset\nSince observational data is random, it is natural to we view the observed data vectors \\[\n\\boldsymbol{x}_i = (x_{i1}, \\ldots, x_{iK})' \\in \\mathbb{R}^K \\quad \\text{for } i = 1, \\ldots, n,\n\\] as a realization of the random vector \\[\n\\boldsymbol X_i = (X_{i1}, \\ldots, X_{iK})' \\in \\mathbb{R}^K \\quad \\text{for } i = 1, \\ldots, n,\n\\]\nwith some associated probability distribution. For example, we can think of the CPS dataset as consisting of random vectors\n\\[\n\\boldsymbol X_i = (sex_i, age_i, educ_i, wage_i)' \\quad \\text{for } i = 1, \\ldots, n.\n\\] For some specific individual \\(i\\) in the CPS dataset, we then observe the vector of realized values1\n\\[\n\\boldsymbol x_i = (1, 25, 16, 25000)'.\n\\] Intuitively, the distinction between the random vector \\(\\boldsymbol X_i\\) and the realization \\(\\boldsymbol x_i\\) is that the former represents the \\(i\\)-th observation before viewing the data (unknown and random) and the latter represents the \\(i\\)-th observation after viewing the data (specific known value).\n\n\nRandom Sampling\nWe have now represented the dataset as a collection of random vectors \\(\\boldsymbol X_1, \\ldots, \\boldsymbol X_n\\), each with some probability distribution \\(F_1 \\ldots, F_n\\). As it stands right now, the connection between the underlying DGP and the observed data is still unclear. Specifically, we need a simplifying assumption that will allow us to connect the distributions of each \\(\\boldsymbol X_i\\) to the underlying DGP in a straightforward manner.\nThe simplest framework is that of random sampling. Here, we assume that the random vectors \\(\\boldsymbol X_1, \\ldots , \\boldsymbol X_n\\) are independent and identically distributed (iid) with some common but unknown distribution \\(F\\) on \\(\\mathbb{R}^K\\). Under this assumption, the data generating process \\(F\\) is precisely the distribution that governs each individual random vector \\(\\boldsymbol X_i\\).\n\n\nAlternative Sampling Assumptions\nThe random sampling assumption is one potential way to characterize the dependence structure across the observed data points. It is popular because (i) it is often reasonable when working with cross-sectional datasets2, and (ii) it is the backbone of several statistical theorems and methods.3"
  },
  {
    "objectID": "blog/random-sampling.html#notation",
    "href": "blog/random-sampling.html#notation",
    "title": "Random Sampling and Model-Based Inference",
    "section": "Notation",
    "text": "Notation\nIt’s useful to explicitly clarify the notation being used above. The random variables \\(X_1, \\ldots, X_K\\) are mathematical objects used to represent the generic or population-level variable associated with the DGP. For example, the random variable \\(wage\\) represents the generic variable for wage in the population. The random vector \\(\\boldsymbol X_i = (X_{i1}, \\ldots, X_{iK})\\) are mathematical objects used to represent the sample-level variable associated with specific units. For example, the random variable \\(wage_i\\) represents the wage of individual \\(i\\) before the data is observed. Lastly, the realizations \\(\\boldsymbol{x}_i = (x_{i1}, \\ldots, x_{iK})\\) are the actual observed values of the sample-level random variables. For example, the value \\(wage_i = 25000\\) is the observed wage of individual \\(i\\) after the data is observed."
  },
  {
    "objectID": "blog/random-sampling.html#point-estimation",
    "href": "blog/random-sampling.html#point-estimation",
    "title": "Random Sampling and Model-Based Inference",
    "section": "Point Estimation",
    "text": "Point Estimation\nPoint estimation is the first step of statistical inference, and involves constructing a “good guess” for a feature of the unknown data generating process. To be more precise, this feature is called an estimand \\(\\theta\\) and is defined as a function of the data generating process \\(F\\):\n\\[\n\\theta = \\theta(F).\n\\]\nAn example of an estimand is the the population mean of a random variable \\(X\\): \\[\n\\mu = \\mathbb{E}_F[X].\n\\]\nSince the DGP is unknown, the best we can do is use the observed data to guess the value of the estimand. An estimator \\(\\hat \\theta\\) is a function of the sample that is intended to provide a guess of the estimand:\n\\[\n\\hat{\\theta} = \\hat{\\theta}(X_1, \\ldots, X_n).\n\\]\nIn statistics, there are several estimation methods that provide systematic ways (i.e. rules) to construct estimators. One common method is the analog principle (or plug-in principle). The idea is to construct the estimator by replacing the population quantities in the estimand with their sample analogs.5 Thus, the analog estimator for the population mean is the sample mean, defined as\n\\[\n\\hat{\\mu} = \\frac{1}{n} \\sum_{i=1}^n X_i.\n\\]\n\nEstimator Properties\nHow do we know if an estimator is any good? And when we have two competing estimators, how do we decide which one to use? To answer these questions, statisticians study desirable properties that an estimator should ideally satisfy. A full treatment of estimator properties is typically the focus of a mathematical statistics course, but it is still valuable to briefly highlight some fundamental properties here."
  },
  {
    "objectID": "blog/random-sampling.html#sampling-distribution-and-the-utility-of-statistical-models",
    "href": "blog/random-sampling.html#sampling-distribution-and-the-utility-of-statistical-models",
    "title": "Model-Based Inference Under Random Sampling",
    "section": "Sampling Distribution and the Utility of Statistical Models",
    "text": "Sampling Distribution and the Utility of Statistical Models\nHow do we define a good guess? How do we choose between two different estimators?\nwe make certain simplifying assumptions about the general structure of \\(F\\) and then\nWe will be able to quantify the uncertainty of the guess if we make stronger starting assumptions about the data generating process."
  },
  {
    "objectID": "blog/random-sampling.html#sampling-distribution-and-the-necessity-for-statistical-models",
    "href": "blog/random-sampling.html#sampling-distribution-and-the-necessity-for-statistical-models",
    "title": "Model-Based Inference Under Random Sampling",
    "section": "Sampling Distribution and the Necessity for Statistical Models",
    "text": "Sampling Distribution and the Necessity for Statistical Models\nHow do we define a good guess? How do we choose between two different estimators?\nwe make certain simplifying assumptions about the general structure of \\(F\\) and then\nWe will be able to quantify the uncertainty of the guess if we make stronger starting assumptions about the data generating process."
  },
  {
    "objectID": "blog/random-sampling.html#necessity-for-statistical-models",
    "href": "blog/random-sampling.html#necessity-for-statistical-models",
    "title": "Random Sampling and Model-Based Inference",
    "section": "Necessity for Statistical Models",
    "text": "Necessity for Statistical Models\n\nSampling Distribution\nwe make certain simplifying assumptions about the general structure of \\(F\\) and then\nWe will be able to quantify the uncertainty of the guess if we make stronger starting assumptions about the data generating process.\n\n\nInference Is Within Statistical Models\nThe discussion in this sectiono is summarized by the figure below.\n\n\n\n\n\nflowchart LR\n  classDef box fill:#f8f9fa,stroke:#444,stroke-width:1px,rx:10,ry:10;\n\n  DGP[\"Data Generating Process\"]:::box\n  Data[\"Observed Data\"]:::box\n  Model[\"Statistical Model\"]:::box\n\n  %% Main flows\n  DGP -- \"Randomness\" --&gt; Data\n  DGP -. \"Assumptions\" .-&gt; Model\n  Model -- \"Probability\" --&gt; Data\n  Data -- \"Inference\" --&gt; Model\n  Model -. \"Approximate Reality\" .-&gt; DGP"
  },
  {
    "objectID": "blog/random-sampling.html#the-necessity-for-statistical-models",
    "href": "blog/random-sampling.html#the-necessity-for-statistical-models",
    "title": "Random Sampling and Model-Based Inference",
    "section": "The Necessity for Statistical Models",
    "text": "The Necessity for Statistical Models\n\nSampling Distribution\nwe make certain simplifying assumptions about the general structure of \\(F\\) and then\nWe will be able to quantify the uncertainty of the guess if we make stronger starting assumptions about the data generating process.\n\n\nInference Is Within Statistical Models\nThe discussion in this sectiono is summarized by the figure below.\n\n\n\n\n\nflowchart LR\n  classDef box fill:#f8f9fa,stroke:#444,stroke-width:1px,rx:10,ry:10;\n\n  DGP[\"Data Generating Process\"]:::box\n  Data[\"Observed Data\"]:::box\n  Model[\"Statistical Model\"]:::box\n\n  %% Main flows\n  DGP -- \"Randomness\" --&gt; Data\n  DGP -. \"Assumptions\" .-&gt; Model\n  Model -- \"Probability\" --&gt; Data\n  Data -- \"Inference\" --&gt; Model\n  Model -. \"Approximate Reality\" .-&gt; DGP"
  }
]