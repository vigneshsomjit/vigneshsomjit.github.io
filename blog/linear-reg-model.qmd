---
title: "The Linear Regression Model and its OLS Estimation"
description: "A detailed discussion of the linear regression model and the ordinary least squares (OLS) estimation method for its parameters."
date: today
categories: [Econometrics/Statistics]
draft: false
---

## Random Sampling Framework 

We are interested in datasets of the form $\{y_{i}, x_{i1}, \ldots, x_{iK}\}_{i=1}^n$ where $y_{i}$ is the **outcome** and $x_{i1}, \ldots, x_{iK}$ are the **predictors**. To mathematically formalize how this data was generated, we view the observations as realizations of random variables that are drawn from some joint distribution $F$, also called the **data generating process (DGP)**. The random sampling assumption is one possible characterization of these draws. 

::: {.assumption-box}

**Assumption 1. Random Sampling**

The observations $\{y_i, x_{i1}, \ldots, x_{iK}\}^n_{i=1}$ are realizations of the random variables $\{Y_i, X_{i1}, \ldots , X_{iK}\}_{i=1}^n$, which are **independent and identically distributed (i.i.d)** draws from the joint distribution $F(Y,X_1, \ldots, X_K)$. 
::: 

It's easy to get confused about the notation and terminology here, so let's clarify. The random variables $Y, X_1, \ldots, X_K$ are theoretical objects that are used when talking about the data generating process *in general*. The random variables $Y_i, X_{i1}, \ldots, X_{iK}$ are the theoretical objects that correspond to each observation in the sample. Intuitively, we can think of these as the data *before* it was collected. The realizations $y_i, x_{i1}, \ldots, x_{iK}$ are the actual data (numbers) we observe *after* data collection. This abstraction allows us to use the tools of probability theory and mathematical statistics to infer from our dataset.[^1] 

## Defining the Linear Regression Model 

Recall that a statistical model is simply a set of assumptions about some DGP. In the case of the linear regression model, we are interested in the joint distribution $F(Y, X_1, \ldots, X_K)$. Under the random sampling assumption, however, each observation $Y_i, X_{i1}, \ldots X_{iK}$ is an i.i.d draw from $F$. This means that we can just as well frame our assumptions in terms of the random variables $\{Y_i, X_{i1}, \ldots, X_{iK}\}_{i=1}^n$. As we will see, this formulation is especially useful when estimating the model parameters. 


:::{.assumption-box #assump-linreg}

**Assumption 2. The Linear Regression Model**

**(A)** ***(Linearity)*** The outcome $Y_i$ is a linear combination of the predictors $X_{i1}, \ldots, X_{iK}$, plus some random **error** $e_i$ that captures measurement error and idiosyncratic fluctuation in $Y_i$:
$$
Y_i = \beta_1X_{i1} + \ldots + \beta_kX_{iK} + e_i \quad \forall \, i = 1, \ldots, n.
$${#eq-linearity}

**(B)** ***(No Multicollinearity)*** None of the predictor variables are an exact linear combination of each other.

**(C)** ***(Strict Exogeneity)*** The error $e_i$ has a conditional mean of zero given the predictors of all observations 
$$
\mathbb{E}[e_i \mid \boldsymbol{X}_1 \ldots , \boldsymbol{X}_n] = 0,
$${#eq-strict-exogeneity}
where $\boldsymbol{X}_i = (X_{i1}, \ldots, X_{iK})'$.
:::

For notational compactness, it is useful to recast the model in matrix notation. First note that we can stack @eq-linearity as a system of $n$ equations 

$$
\begin{aligned}
Y_1 &= \beta_1 X_{11} + \ldots + \beta_k X_{1K} + e_1 = \boldsymbol{X_1'}\boldsymbol{\beta} + e_1\\\ 
Y_2 &= \beta_1 X_{21}+ \ldots + \beta_k X_{2K} + e_2 = \boldsymbol{X_2'}\boldsymbol{\beta} + e_2 \\
&\;\;\vdots \\
Y_n &= \beta_1X_{n1} + \ldots + \beta_k X_{nK} + e_n= \boldsymbol{X_n'}\boldsymbol{\beta} + e_n,
\end{aligned}
$$
where
$$
\boldsymbol{X_i} = \begin{pmatrix} X_{i1} \\ \vdots \\ X_{iK} \end{pmatrix} \in \mathbb{R}^{K \times 1} \quad  \text{and} \quad \boldsymbol{\beta}= \begin{pmatrix} \beta_1 \\ \vdots \\ \beta_K \end{pmatrix} \in \mathbb{R}^{K \times 1}.
$$

Now, we can collapse this system of equations into a single matrix equation:
$$
\boldsymbol{Y} = \mathbf{X}\boldsymbol{\beta} + \boldsymbol{e},
$${#eq-linreg-matrix}
where 
$$
\boldsymbol{Y} = \begin{pmatrix} Y_1 \\ \vdots \\ Y_n \end{pmatrix} \in \mathbb{R}^{n \times 1}, \quad \boldsymbol{e} = \begin{pmatrix} e_1 \\ \vdots \\ e_n \end{pmatrix} \in \mathbb{R}^{n \times 1}, \quad  \mathbf{X} = \begin{pmatrix} \boldsymbol{X_1'} \\ \vdots \\ \boldsymbol{X_n'} \end{pmatrix} = \begin{pmatrix} X_{11} & \ldots & X_{1K} \\ \vdots & \ddots & \vdots \\ X_{n1} & \ldots & X_{nK} \end{pmatrix} \in \mathbb{R}^{n \times K}.
$$

The quantity $\mathbf{X}$[^2] is called the **design matrix**.[^3]  


### Interpreting the Assumptions

It is good practice to carefully think through what is being assumed in any given model. The linearity assumption in @eq-linearity and @eq-linreg-matrix states that (i) the functional form of the relationship between the outcome and predictors is linear in the parameters, and (ii) the error $e$ is **additive**. 

The assumption of no multicollinearity ensures that the design matrix $\mathbf{X}$ has **full column rank**.[^4] This in turn means the square matrix $\mathbf{X}'\mathbf{X}$ is invertible, which is a crucial property when deriving the ordinary least squares (OLS) estimator for the parameters.

The strict exogeneity assumption states that the conditional expectation of the error given the predictors of *all observations* is zero. This is a crucial assumption of the model, and has several implications:

1. The unconditional mean of the error $e_i$ is zero:
$$
\mathbb{E}[e_i] = \mathbb{E}[\mathbb{E}[e_i \mid \mathbf{X}]] = 0 \quad \forall \, i = 1, \ldots, n,
$${#eq-uncond-mean-error}
where the first equality follows from the [law of iterated expectations](https://en.wikipedia.org/wiki/Law_of_total_expectation).

2. The predictors of *all observations* are **orthogonal** to the error $e_i$: 
$$
\begin{align}
\mathbb{E}[X_{jk}e_i] 
&\overset{(a)}{=} \mathbb{E}\big[\mathbb{E}[X_{jk}e_i \mid X_{jk}]\big] \\
&\overset{(b)}{=} \mathbb{E}\big[X_{jk} \mathbb{E}[e_i \mid X_{jk}]\big]  \\
&\overset{(c)}{=} 0 \quad \quad \quad \quad \quad \quad \quad \quad \forall \, i,j = 1, \ldots, n; k = 1, \ldots, K,
\end{align}
$${#eq-orthogonality}
where $(a)$ uses the law of iterated expectations, $(b)$ uses the linearity of expectations, and $(c)$ follows from the fact that
$$
\mathbb{E}[e_i \mid X_{jk} ]= \mathbb{E}\big[\mathbb{E}[e_i \mid \mathbf{X} ] \mid X_{jk}\big] =0
$$
by the [generalized law of iterated expectations](https://stats.stackexchange.com/questions/95947/a-generalization-of-the-law-of-iterated-expectations). 

3. The error term $e_i$ is uncorrelated with the predictors of *all observations*:
$$
\text{Cov}(e_i, X_{jk}) 
\overset{(a)}{=} \mathbb{E}[X_{jk}e_i] - \mathbb{E}[X_{jk}]\mathbb{E}[e_i]
\overset{(b)}{=} \mathbb{E}[e_iX_{jk}] 
\overset{(c)}{=} 0 
\quad \forall \, i,j,k,
$${#eq-uncorrelatedness}
where $(a)$ is the definition, $(b)$ uses @eq-uncond-mean-error, and $(c)$ uses @eq-orthogonality. Intuitively, this means the error term and predictors do not contain any information about one another.

4. The conditional expectation of $\boldsymbol{Y}$ given $\mathbf{X}$, called the **regression** of $\boldsymbol{Y}$ on $\mathbf{X}$, is a linear function of the realized values:
$$
\mathbb{E}[\boldsymbol{Y} \mid \mathbf{X}] = \mathbb{E}[\mathbf{X}\beta + \boldsymbol{e} \mid \mathbf{X}]=\mathbb{E}[{\mathbf{X}\beta \mid  \mathbf{X}}]=\mathbf{X}\beta. 
$$

### Implications of Random Sampling

Random sampling has two key implications for the linear regression model. First, it implies that the errors are independent across observations. To see this, first note that under random sampling

$$
(Y_i, \boldsymbol{X}_i) \perp (Y_j, \boldsymbol{X}_j) \quad \forall \, i \neq j.
$$
Since independence is preserved under functional transformations, and the error is a function of the outcome and covariates,
$$
e_i = f(Y_i, \boldsymbol{X}_i) = Y_i - \boldsymbol{X}_i\boldsymbol{\beta} \quad \forall \, i = 1, \ldots, n,
$$

it follows that the errors $e_i$ are independent across observations.

A second implication is that random sampling allows us to simplify the strict exogeneity assumption. Specifically, since random sampling implies

$$
(e_i, \boldsymbol{X}_i) \perp (\boldsymbol{X}_j) \quad \forall \, i \neq j,
$$
it follows that 

$$
\mathbb{E}[e_i \mid \boldsymbol{X}_1, \ldots, \boldsymbol{X}_n] = \mathbb{E}[e_i \mid \boldsymbol{X}_i] \quad \forall \, i = 1, \ldots, n.
$$

Thus, under random sampling, the strict exogeneity assumption is equivalent to the simpler assumption

$$
\mathbb{E}[e_i \mid \boldsymbol{X}_i] = 0 \quad \forall \, i = 1, \ldots, n.
$$

Intuitively, random sampling eliminates any cross-sectional dependence between the error term and the predictors of other observations. As a result, the linear regression model only needs to assert that each error is **mean independent** of its *own* predictors $\boldsymbol{X_i}$.  

### What is NOT Assumed

Before proceeding, it is worth clarifying what we do not assume in the linear regression model. First, we do not assume that $Y$ is a linear function of $(X_1, \ldots, X_k)$ in @eq-linearity; we only require that the parameters $\boldsymbol{\beta}$ enter the equation linearly. This means that we are free to include non-linear transformations of the predictor variables in @eq-linearity as long linearity in $\boldsymbol{\beta}$ is preserved. Second, we make no assumptions about the distribution of $\boldsymbol{X}$. Third, we do not make assumptions about the distribution or variance of the error term $e$.

## Deriving the OLS Estimator


[^1]: This [post](statistical-modeling.qmd) discuses the big picture intuition for statistical modeling and inference in more detail.
[^2]: Technically, the right hand side of the equation should use $\boldsymbol{x}'$. However, I have chosen to abuse notation here for brevity.
[^2]: There is some notational ambiguity here. Any bold, italicized letter represents a vector. Any bold, upright letter represents a matrix. So, $\boldsymbol{X}_i$ refers to the vector of random predictor variables $(X_{i1}, \ldots, X_{ik})$, while $\mathbf{X}$ refers to the design matrix. 
[^3]: If we want to introduce a **constant intercept term** in the model, we set the first regressor to equal one for all observations. In this case, the first column of the design matrix $\mathbf{X}$ is a vector of ones and the first element of $\boldsymbol{\beta}$ is the intercept term.
[^4]: Because in any given row of the design matrix, the column entries are linearly independent of one another. 
