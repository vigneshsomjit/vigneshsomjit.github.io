---
title: "An Applied Econometrics Rationale for Linear Regression"
description: "Linear regression can be motivated as a tool to approximate the conditional expectation function. I discuss this perspective and build intuition for interpreting regression parameters."
date: today
categories: [Econometrics]
---

```{r}
#| label: setup
#| include: false

library(dplyr)
library(fixest)
```

# Regression Approximates Conditional Averages  

We are interested in predicting the value of a random variable $Y_i$, called the **outcome**, given some realized value of a random vector $X_i$ of covariates. As a starting point, we let our predictions be a function of $X_i$ and want our predictions to minimize the mean squared error (MSE) objective function

$$
\mathbb{E} [Y_i - f(X_i)]^2.
$$ With the law of iterated expectation and some calculus, we can show that for every possible realized value $X_i = x$, the optimal function that solves the minimum mean squared error problem is the **conditional expectation function (CEF)**

$$
\mu(x) = \mathbb{E}[Y_i | X_i = x].
$$

In most empirical cases, however, the CEF of interest has no analytic form. Thus, it is more practical to *model* the relationship between the outcome and covariates using a simpler function. One option is to find a *linear approximation* of the CEF, which takes the form

$$
\ell(x) = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \ldots + \beta_k x_k = x^T\beta.
$$ 

Note that the word "linear" here refers to the simplifying assumption that the function is a linear combination of the covariates. Since we want our predictions to minimize the MSE function, the best linear predictor is found by setting the coefficients as

$$
\beta^\star = \underset{\beta}{\arg\min} \, \mathbb{E} [Y_i - (X_i^T \beta)]^2 = \mathbb{E}[X_i X_i^T]^{-1}\mathbb{E}[X_iY_i].
$$
Thus, the **linear regression model** that describes the relationship between $Y_i$ and $X_i$ is given by 
$$
Y_i = \underbrace{X_i^T \beta^\star}_{\text{best linear predictor}} + \underbrace{\epsilon_i}_{\text{residual}},
$$
where $\epsilon_i$ captures the difference between the realized outcome and the best linear predictor.

To summarize, the conditional expectation function is the optimal predictor of an outcome given covariates, in the sense that it minimizes the mean squared prediction error. Since the CEF is typically a complex function, linear regression approximates it by selecting the linear combination of covariates that minimizes the mean squared error. The resulting linear predictor provides a simple model of the relationship between the outcome and the covariates.

# Approximate and Exact Conditional Averages  



```{r}
#| echo: false
#The conditional expectation function is a function of the covariates $X_i$. In most empirical applications, the covariates of interest result in a complicated, non-analytic CEF. We have seen how linear regression can be used to *approximate* the CEF in such cases. However, it is also possible to choose a set of covariates such that the CEF is a linear function. One possible way to do this is by constructing a dummy covariate for every unique group in the dataset. 
# However, it can be shown that the linear regression model corresponding to a *linear* CEF fully recovers the underlying function. An example is the **saturated regression model**, which includes a complete set of parameters such that every unique group defined by the realized covariate values receives its own distinct predicted conditional average. By construction, the conditional expectation function 

```


```{r}
set.seed(123)

# Sample size 
n <- 1000 

# Covariates
sex <- rbinom(n, 1, 0.5)
race <- rbinom(n, 1, 0.3)                   

# Non-linear data generating process; no interactions
outcome1 <- 5 + 2^sex + 3^race 

# Non-linear data generating process; interactions 
outcome2 <- 5 + 2^sex + 3^race + 1.5^(sex*race)

# Consolidate in a dataset 
df <- data.frame(
  outcome1 = outcome1,
  outcome2 = outcome2,
  sex = sex,
  race = race
)
```

