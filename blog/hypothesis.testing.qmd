---
title: "Hypothesis Testing for the Normal Sampling Model"
description: "I review key concepts in hypothesis testing using the running example of the normal sampling model."
date: today
categories: [Mathematical Statistics]
---

## Recapitulation 

[Recall](statistical-inference.qmd) the basic setup of statistical inference under the random sampling framework. Here, we view the observed data $\boldsymbol x = (x_1, \ldots, x_n)'$ as a realization of the random variables $\boldsymbol X = (X_1, \ldots, X_n)'$ independently drawn from a common, unknown distribution $F$. In other words, the data is a realization of a **random sample** from the **population** $F$. The goal of statistical inference is to use the random sample to make statements about $F$, while quantifying the uncertainty in those statements. 

Typically, to conduct statistical inference, we need to make some starting assumptions about the structure of $F$. This is called a **statistical model** for $F$.  In this post, we will work with the **normal sampling model**, where we assume that $F$ is the normal distribution with unknown mean $\mu$ and unknown variance $\sigma^2$.

## Hypotheses

Hypothesis testing is a fundamental tool to conduct statistical inference. At its core is the **hypothesis**: a statement about a scalar parameter of interest $\theta$ determined by the population $F$. In a non-parametric model — where $F$ cannot be fully characterized by a finite set of model parameters — $\theta$ is some function of $F$, like $\mathbb{E}[X]$ or $\operatorname{Var}(X)$. In a parametric model — where $F$ is assumed to belong to a family of distribution characterized by a finite number of parameters — $\theta$ is typically one of the model parameters.[^1] For example, in the normal sampling model, our hypotheses are usually about $\mu$ or $\sigma^2$. 

Hypothesis testing is formulated in terms of two complementary hypotheses. The **null hypothesis** $H_0$ is the hypothesis to be tested, while the **alternative hypothesis** $H_1$ is the complement of the null hypothesis. These hypotheses can be defined by how they restrict the **parameter space** of $\theta$, denoted $\Theta$. Specifically, the null is defined by the restriction $\theta = \theta_0$ for some hypothesized value $\theta_0$, while the alternative is defined by the set $\{\theta \in \Theta: \theta \neq \theta_0\}$.[^2] 

In this post, we will focus on the problem of testing hypotheses about the population mean $\mu$ under the normal sampling model. Specifically, our hypotheses are 
$$
H_0: \mu = \mu_0 \quad \text{and} \quad H_1: \mu \neq \mu_0,
$$
where $\mu_0$ is some hypothesized value of $\mu$. The goal of hypothesis testing is to test the validity of $H_0$ over $H_1$ using the random sample. In the next few sections, we will develop the machinery required to do exactly this.

## Test Statistic and Critical Region 

The outcome of a hypothesis test is a **decision**: either accept the null or reject the null in favor of the alternative. Thus, we want a **decision rule** that maps the **sample space** — the set of all possible realizations of the random sample — into one of these two actions. One way to formulate this rule is as follows. First, we construct a function of the random sample called the **test statistic**

$$
T: \boldsymbol X \rightarrow \mathbb{R}.
$$
The role of the test statistic is to take any random sample from the population and compress it into a single number that reflects the information most relevant to the hypothesis. Since it is a function of random variables, the test statistic is itself a random variable. Second, we define a **critical region** $C$ to be some subset of the range of $T$. The decision rule can then be framed in terms of the observed realization of the test statistic, $t$: (i) reject $H_0$ if $t \in C$, and (ii) accept $H_0$ if $t \notin C$. 

It's worth pausing here to emphasize two important features of this setup. First, the effectiveness of a hypothesis test depends on the choice of both the test statistic and critical region. If $T$ does not capture the right information, or if $C$ is poorly specified, the resulting decision rule will not be very insightful. Second, making incorrect decisions is inevitable in hypothesis testing because of the randomness inherent in the sample. Even with a well-chosen statistic and critical region, it is entirely possible to draw a sample with a mean far from the true population mean $\mu$ but close to the hypothesized mean $\mu_0$, or conversely, a sample from $\mu_0$ even when $\mu = \mu_0$. In both situations, we make an incorrect decision purely due to chance. 

Thus, in carrying out hypothesis tests, the focus is not on eliminating mistakes entirely, but rather on quantifying and controlling the *probability* of error. The following section formalizes this idea. 

## Classical Approach to Hypothesis Testing

### Error Probabilities and Power Function

There are two types of incorrect decisions we could make in hypothesis testing. Rejecting the null when it is actually true is called a **Type I error**. Accepting the null when it is actually false is called a **Type II error**. 

To build the vocabulary and notation required to talk about the probability of making these two errors, we need to introduce the **power function** of a hypothesis test. This is the probability of rejecting the null $H_0$ under some population distribution $F$, and is denoted 
$$
\pi(F) = \mathbb{P}[\text{Reject } H_0 \mid F] = \mathbb{P}[T \in C \mid F].
$$

The power function formalizes the source of randomness in hypothesis tests. Our data is random because they are realizations of a random sample drawn from the distribution $F$. Since the test statistic is computed from the data, its randomness is also induced by $F$. Consequently, the decision to accept or reject the null is likewise determined by $F$. The power function summarizes this chain by expressing the probability of rejection directly as a function of the underlying distribution $F$. 

The probability of making a Type I error is the **size** of the hypothesis test, and is simply the power function evaluated under the distribution implied by the null, $F_0$:
$$
\mathbb{P}[\text{Reject } H_0 \mid F_0] = \pi(F_0).
$$

The **power** of the hypothesis is the *complement* of the probability of making a Type II error, and is given by the power function evaluated under the distribution implied by the alternative, $F_1$:
$$
1 - \mathbb{P}[\text{Accept } H_0 \mid F_1] = \mathbb{P}[\text{Reject } H_0 \mid F_1] = \pi(F_1). 
$$

### Null Sampling Distribution 

## Example: Two-Sided Test for Normal Samplign Model

### t-Statistic 

Let $\hat\theta$ be an estimator of $\theta$, some parameter of interest determined by the population $F$. The **t-ratio** or **t-statistic** is defined as 
$$
t = \frac{\hat\theta(\boldsymbol x) - \theta_0}{\widehat{SE}(\hat\theta)},
$$
where $\theta_0$ is a hypothesized value of $\theta$ and $\widehat{SE}(\hat\theta)$ is an estimator of the standard error of $\hat\theta$. In words, this statistic measures how many estimated standard errors the estimate $\hat\theta (\boldsymbol x)$ is away from the hypothesized value $\theta_0$. 

Recall that in the normal sampling model $X_i \overset{iid}{\sim} \mathcal{N}(\mu, \sigma^2)$, the sample mean $\bar X$ is an estimator of the population mean $\mu$, and has a standard error of $\sigma / \sqrt{n}$. Since $\sigma$ is unknown, we estimate it using the sample standard deviation $s$. Thus, the **t-statistic** for the normal sampling model is given by
$$
t = \frac{\bar X - \mu_0}{s / \sqrt{n}}.
$$



[^1]: It is worth emphasizing that while the parameter of interest is often the model parameters, the two are not always the same. 
[^2]: Technically, the null hypothesis can be a set of values as well. However, in this post, we focus on **point (null) hypotheses**.
